{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173c9fbd",
   "metadata": {},
   "source": [
    "# 2.1 Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f64bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Library Imports\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "from skimage import img_as_float\n",
    "from skimage.io import imread_collection,imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "import decimal\n",
    "decimal.getcontext().prec = 100\n",
    "\n",
    "\n",
    "# # Data Loading and Preprocessing\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "data = load('pneumoniamnist.npz')\n",
    "lst = data.files\n",
    "xtrain = data[lst[0]]\n",
    "xval = data[lst[1]]\n",
    "xtest = data[lst[2]]\n",
    "ytrain = data[lst[3]]\n",
    "yval = data[lst[4]]\n",
    "ytest = data[lst[5]]\n",
    "\n",
    "xtrain = np.reshape(xtrain,(-1,28*28))\n",
    "xval = np.reshape(xval,(-1,28*28))\n",
    "xtest = np.reshape(xtest,(-1,28*28))\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "class OneHotEncoder:\n",
    "    def __init__(self,classes=2,type='none'):\n",
    "        self.classes = classes\n",
    "        self.type = type\n",
    "        pass\n",
    "\n",
    "    def transform(self,X:np.ndarray):\n",
    "        self.classes = int(np.max(X)+1)\n",
    "        if self.type=='none':\n",
    "            Y = -1*np.ones((len(X),self.classes))\n",
    "        else:\n",
    "            Y = np.zeros((len(X),self.classes))\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            Y[i,X[i]] = 1\n",
    "        return Y\n",
    "\n",
    "    def inverse_transform(self,X:np.ndarray):\n",
    "        Y = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            Y[i] = 0\n",
    "            mx = X[i,0]\n",
    "            for j in range(self.classes):\n",
    "                if X[i,j] > mx:\n",
    "                    Y[i] = j\n",
    "                    mx = X[i,j] \n",
    "        return Y\n",
    "\n",
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "    def fit(self,X:np.ndarray):\n",
    "        self.mean = np.mean(X)\n",
    "        self.std = np.std(X)\n",
    "  \n",
    "    def transform(self,X:np.ndarray):\n",
    "        return (X-self.mean)/self.std\n",
    "\n",
    "    def inverse_transform(self,X:np.ndarray):\n",
    "        return X*self.std+self.mean\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "x_scalar = StandardScaler()\n",
    "x_scalar.fit(xtrain)\n",
    "xtrain = x_scalar.transform(xtrain)\n",
    "xval = x_scalar.transform(xval)\n",
    "xtest = x_scalar.transform(xtest)\n",
    "\n",
    "#Binary Classification\n",
    "encoder = OneHotEncoder(2,type='zeros') \n",
    "\n",
    "ytrain = encoder.transform(ytrain)\n",
    "\n",
    "\n",
    "# #  Performance Metric Related Functions\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def MeanSquaredError(ydata,ypredict):\n",
    "    return np.sqrt(np.mean(np.square(ydata-ypredict)))\n",
    "\n",
    "def ReturnConfMatrix(ypredict,ytest,N=2):\n",
    "    Conf = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            a = np.where(ypredict==i,1,0)\n",
    "            b = np.where(ytest==j,1,0)\n",
    "            c = a*b\n",
    "            Conf[i,j] = np.sum(c)\n",
    "    return Conf\n",
    "\n",
    "def EvaluateAccuracy(ypredict,ytest,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    I = np.identity(N)\n",
    "    Diag = Conf*I\n",
    "    return np.sum(Diag)/max(np.sum(Conf),0.001)\n",
    "\n",
    "def EvaluatePrecision(ypredict,ytest,i,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    return max(Conf[i,i],0.001)/max(np.sum(Conf[i,:]),0.001)\n",
    "\n",
    "def EvaluateRecall(ypredict,ytest,i,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    return max(Conf[i,i],0.001)/max(np.sum(Conf[:,i]),0.001)\n",
    "\n",
    "def EvaluateF1score(ypredict,ytest,i,N=2):\n",
    "    p = EvaluatePrecision(ypredict,ytest,i,N)\n",
    "    r = EvaluateRecall(ypredict,ytest,i,N)\n",
    "    return 2*r*p/(r+p)\n",
    "\n",
    "def EvaluateFalsePositiveRate(ypredict,ytest,i,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    t1=max(np.sum(Conf[i,:]),0.001)\n",
    "    t2=t1-Conf[i,i]\n",
    "    t3=max(t2,0.001)\n",
    "    return t3/max(np.sum(Conf[i,:]),0.001)\n",
    "\n",
    "def EvaluateAUC(ypredict,ytest,i,N=2):\n",
    "    return EvaluatePrecision(ypredict,ytest,i,N)/EvaluateFalsePositiveRate(ypredict,ytest,i,N)\n",
    "\n",
    "def EvaluateCategoricalCrossEntropy(ypredict,ytest):\n",
    "    return np.abs(np.sum(np.log(ypredict)@ytest.transpose()))\n",
    "\n",
    "def tptnfpfn(ypredict,ytest,N=2):  # Function to calculate True/False Positives/Negatives\n",
    "    Conf = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            a = np.where(ypredict==i,1,0)\n",
    "            b = np.where(ytest==j,1,0)\n",
    "            c = a*b\n",
    "            Conf[i,j] = np.sum(c)\n",
    "    tp=np.zeros(N)\n",
    "    tn=np.zeros(N)\n",
    "    fp=np.zeros(N)\n",
    "    fn=np.zeros(N)\n",
    "    for i in range(N):\n",
    "        tp[i]=Conf[i,i]\n",
    "    ConfSum=np.sum(Conf)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            fp[i]+=Conf[j,i]\n",
    "            fn[i]+=Conf[i,j]\n",
    "        temp1=fp[i]\n",
    "        temp2=fn[i]\n",
    "        fp[i]-=Conf[i,i]\n",
    "        fn[i]-=Conf[i,i]\n",
    "        tn[i]=ConfSum-temp1-temp2+Conf[i,i]\n",
    "\n",
    "    return tp,tn,fp,fn \n",
    "\n",
    "def plot(errors,iter,title=''):\n",
    "    plt.title(title)\n",
    "    plt.plot(range(iter),errors)\n",
    "    plt.xticks(np.linspace(start=0,stop=iter,num=11))\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('MeanSquaredError')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "def print_performance_metrics(y_predict, y_test, N):\n",
    "    num = len(y_predict)\n",
    "    y_predict=np.reshape(y_predict,(num,1))\n",
    "    y_test=np.reshape(y_test,(num,1))\n",
    "    \n",
    "    print('Accuracy',EvaluateAccuracy(y_predict,y_test,N))\n",
    "    \n",
    "    for i in range(N):\n",
    "        f1 = EvaluatePrecision(y_predict,y_test,i,N)\n",
    "        print(f'Precision : {i} = {f1:.3f}')\n",
    "\n",
    "    for i in range(N):\n",
    "        f1 = EvaluateRecall(y_predict,y_test,i,N)\n",
    "        print(f'Recall : {i} = {f1:.3f}')\n",
    "\n",
    "    for i in range(N):\n",
    "        f1 = EvaluateF1score(y_predict,y_test,i,N)\n",
    "        print(f'F1 Score : {i} = {f1:.3f}')\n",
    "\n",
    "    for i in range(N):\n",
    "        f1 = EvaluateAUC(y_predict,y_test,i,N)\n",
    "        print(f'AUC : {i} = {f1:.3f}')\n",
    "\n",
    "    print('Confusion Matrix',ReturnConfMatrix(y_predict,y_test,N))\n",
    "\n",
    "    tp,tn,fp,fn = tptnfpfn(y_predict,y_test,N)\n",
    "    print('True Positives',tp)\n",
    "    print('True Negatives',tn)\n",
    "    print('False Positives',fp)\n",
    "    print('False Negatives',fn)\n",
    "\n",
    "\n",
    "# # MLE\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "class GaussianMaxLikelihoodEstimate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,X:np.ndarray):\n",
    "        self.mean = np.mean(X,axis=0)\n",
    "        self.cov = np.cov(X.transpose())\n",
    "        self.cov = self.cov + 0.01*np.identity(self.cov.shape[0])\n",
    "        self.det = np.linalg.det(self.cov)\n",
    "        self.inv = np.linalg.inv(self.cov)\n",
    "        pass\n",
    "\n",
    "    def predict(self,x):\n",
    "        x = x-self.mean\n",
    "        return np.exp(-1*(x@(self.inv)@x)/2)\n",
    "\n",
    "class GaussianBayesClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X:np.ndarray,Y:np.ndarray,validation_data=False):\n",
    "        self.classes = 2\n",
    "        self.Gmles = []\n",
    "        for j in range(self.classes):\n",
    "            data = []\n",
    "            for x,y in zip(X,Y):\n",
    "                if y[j]==1:\n",
    "                    data.append(x)\n",
    "            data = np.array(data)\n",
    "            gmle = GaussianMaxLikelihoodEstimate()\n",
    "            gmle.fit(data)\n",
    "            self.Gmles.append(gmle)\n",
    "    \n",
    "        if type(validation_data)!=bool:\n",
    "            return EvaluateAccuracy(self.batchPrediction(validation_data[0]),validation_data[1],self.classes)\n",
    "  \n",
    "    def predict(self,x):\n",
    "        curr=-1\n",
    "        mx=-1\n",
    "        mx = 0\n",
    "        for i,gmle in enumerate(self.Gmles):\n",
    "            res = gmle.predict(x)\n",
    "            if res > curr:\n",
    "                mx = i\n",
    "                curr = res\n",
    "        return mx\n",
    "\n",
    "\n",
    "    def batchPrediction(self,X:np.ndarray):\n",
    "        ypredict = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            ypredict[i] = self.predict(X[i,:])\n",
    "        return ypredict\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "gm = GaussianBayesClassifier()\n",
    "gm.fit(xtrain,ytrain,validation_data=(xval,yval))\n",
    "\n",
    "ypredict_gm = gm.batchPrediction(xtest)\n",
    "\n",
    "print_performance_metrics(ypredict_gm, ytest, 2)\n",
    "\n",
    "\n",
    "# # Logistic Regression\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "class LogisticRegressor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def preprocess(self,x):\n",
    "        d = len(x)\n",
    "        x_new = np.ones(d+1)\n",
    "        k=0\n",
    "        for i in x:\n",
    "            x_new[k] = i\n",
    "            k+=1\n",
    "        return x_new\n",
    "  \n",
    "    def batchPreprocessing(self,X):\n",
    "        d = X.shape[1]\n",
    "        X_new = np.zeros((X.shape[0],d+1))\n",
    "        for i in range(X.shape[0]):\n",
    "            X_new[i] = self.preprocess(X[i])\n",
    "        return X_new\n",
    "    \n",
    "    def fit(self,X:np.ndarray,Y:np.ndarray,iter=20,alpha=1,beta=0.9,gamma=0.01,validation_data=False,regularization='none'):\n",
    "        '''\n",
    "        X : Input batch\n",
    "        Y : Output batch\n",
    "        iter : Iterations\n",
    "        alpha : step-size\n",
    "        beta : step-size decay\n",
    "        gamma : regularization parameter\n",
    "        regularization : {'none','L1','L2','Elastic'}\n",
    "        '''\n",
    "        X = self.batchPreprocessing(X)\n",
    "        if validation_data!=False:\n",
    "            xval = self.batchPreprocessing(validation_data[0])\n",
    "\n",
    "        self.W = np.zeros((Y.shape[1],X.shape[1]))\n",
    "        Y = np.reshape(Y,(len(Y),-1))\n",
    "      \n",
    "        errors = []\n",
    "        for i in range(iter):\n",
    "            for j in range(Y.shape[1]):\n",
    "                if(regularization=='none'):\n",
    "                    self.W[j,:] = self.W[j,:] - alpha*((self.batchPrediction(X)*(1-self.batchPrediction(X)))[:,j]).transpose()@X/Y.shape[0]\n",
    "                elif(regularization=='L2'):\n",
    "                    self.W[j,:] = self.W[j,:] - alpha*(((self.batchPrediction(X)*(1-self.batchPrediction(X)))[:,j]).transpose()@X/Y.shape[0] - gamma*self.W[j,:])\n",
    "                elif(regularization=='L1'):\n",
    "                    self.W[j,:] = self.W[j,:] - alpha*(((self.batchPrediction(X)*(1-self.batchPrediction(X)))[:,j]).transpose()@X/Y.shape[0] - gamma*np.sign(self.W[j,:]))\n",
    "                elif(regularization=='Elastic'):\n",
    "                    self.W[j,:] = self.W[j,:] - alpha*(((self.batchPrediction(X)*(1-self.batchPrediction(X)))[:,j]).transpose()@X/Y.shape[0] - gamma*np.sign(self.W[j,:]))\n",
    "        alpha = beta*alpha\n",
    "\n",
    "        if type(validation_data)!=bool:\n",
    "            errors.append(EvaluateCategoricalCrossEntropy(self.batchPrediction(xval),validation_data[1]))\n",
    "\n",
    "        return errors\n",
    "\n",
    "\n",
    "    def predict(self,x:np.ndarray):\n",
    "        if x.shape[0]!=self.W.shape[1]:\n",
    "            x = self.preprocess(x)\n",
    "        x = np.reshape(x,(len(x),-1))\n",
    "        y = self.W@x\n",
    "        for i in range(y.shape[0]):\n",
    "            y[i] = 1/(1+np.exp(y[i]))\n",
    "        y = np.reshape(y,(y.shape[0],))\n",
    "        return y\n",
    "\n",
    "    def batchPrediction(self,X:np.ndarray):\n",
    "        ypredict = []\n",
    "        for i in range(X.shape[0]):\n",
    "            ypredict.append(self.predict(X[i,:]))\n",
    "        ypredict = np.array(ypredict)\n",
    "        return ypredict\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "lm = LogisticRegressor()\n",
    "\n",
    "iter = 50\n",
    "errors = lm.fit(xtrain,ytrain,alpha=0.001,beta=1,iter=iter,validation_data=(xval,encoder.transform(yval)))\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "ypredict_lm = lm.batchPrediction(xtest)\n",
    "encoder = OneHotEncoder()\n",
    "ypredict_lm = encoder.inverse_transform(ypredict_lm)\n",
    "\n",
    "print_performance_metrics(ypredict_lm, ytest, 2)\n",
    "\n",
    "\n",
    "# # K Nearest Neighbour\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "class KNNclassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def EUC_DIST(self,v1,v2): \n",
    "        v1,v2 = np.array(v1),np.array(v2)\n",
    "        distance = np.sum((v1-v2)**2)\n",
    "        return np.sqrt(distance)\n",
    "    \n",
    "    def Predict(self,k,Xtrain,Ytrain,Xtest_instance): \n",
    "        distances = [] \n",
    "        for i in range(len(Xtrain)):\n",
    "            dist = self.EUC_DIST(Xtrain[i], Xtest_instance)\n",
    "            distances.append((Ytrain[i],dist)) \n",
    "        distances.sort(key=lambda x: x[1]) \n",
    "        neighbors = []\n",
    "        for i in range(k):\n",
    "            neighbors.append(distances[i][0])\n",
    "        classes = {}\n",
    "        for i in range(len(neighbors)):\n",
    "            response = neighbors[i][1]\n",
    "            if response in classes.keys():\n",
    "                classes[int(response)] += 1\n",
    "            else:\n",
    "                classes[int(response)] = 1\n",
    "        sorted_classes = sorted(classes.items() , key = lambda x: x[1],reverse = True )\n",
    "        return sorted_classes[0][0]\n",
    "\n",
    "    def batchPrediction(self,Knn,Xtrain,Ytrain,Xtest):\n",
    "        ypredict=[]\n",
    "        for i in range(Xtest.shape[0]):\n",
    "            ypredict.append(self.Predict(Knn,Xtrain,Ytrain,Xtest[i,:]))\n",
    "        ypredict = np.array(ypredict)\n",
    "        return ypredict\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "knn = KNNclassifier()\n",
    "\n",
    "ypredict_k1 = knn.batchPrediction(1,xtrain,ytrain,xtest)\n",
    "print_performance_metrics(ypredict_k1, ytest, 2)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "ypredict_k2 = knn.batchPrediction(2,xtrain,ytrain,xtest)\n",
    "print_performance_metrics(ypredict_k2, ytest, 2)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "ypredict_k5 = knn.batchPrediction(5,xtrain,ytrain,xtest)\n",
    "print_performance_metrics(ypredict_k5, ytest, 2)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "ypredict_k9 = knn.batchPrediction(9,xtrain,ytrain,xtest)\n",
    "print_performance_metrics(ypredict_k9, ytest, 2)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "ypredict_k11 = knn.batchPrediction(11,xtrain,ytrain,xtest)\n",
    "print_performance_metrics(ypredict_k11, ytest, 2)\n",
    "\n",
    "\n",
    "# # Naive Bayes\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "class GaussianNaiveMLE:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "    def fit(self,X:np.ndarray):\n",
    "        self.mean = np.mean(X,axis=0)\n",
    "        self.cov = np.cov(X.transpose())    \n",
    "        self.cov = self.cov*np.identity(self.cov.shape[0])\n",
    "        self.cov = self.cov + 0.01*np.identity(self.cov.shape[0])\n",
    "        self.det = np.linalg.det(self.cov)\n",
    "        self.inv = np.linalg.inv(self.cov)\n",
    "        pass\n",
    "  \n",
    "    def predict(self,x):\n",
    "        x = x-self.mean\n",
    "        return np.exp(-1*(x@(self.inv)@x)/2)\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "    def fit(self,X:np.ndarray,Y:np.ndarray,validation_data=False):\n",
    "        x,y = (np.shape(Y))\n",
    "        self.classes = y\n",
    "        self.Gmles = []\n",
    "        for j in range(self.classes):\n",
    "            data = []\n",
    "        for x,y in zip(X,Y):\n",
    "            if y[j]==1:\n",
    "                data.append(x)\n",
    "        data = np.array(data)\n",
    "        gmle = GaussianNaiveMLE()\n",
    "        gmle.fit(data)\n",
    "        self.Gmles.append(gmle)\n",
    "    \n",
    "        if type(validation_data)!=bool:\n",
    "            return EvaluateAccuracy(self.batchPrediction(validation_data[0]),validation_data[1],self.classes)\n",
    "  \n",
    "    def predict(self,x):\n",
    "        curr=-1\n",
    "        mx=-1\n",
    "        mx = 0\n",
    "        for i,gmle in enumerate(self.Gmles):\n",
    "            res = gmle.predict(x)\n",
    "            if res > curr:\n",
    "                mx = i\n",
    "                curr = res\n",
    "        return mx\n",
    "\n",
    "\n",
    "    def batchPrediction(self,X:np.ndarray):\n",
    "        ypredict = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            ypredict[i] = self.predict(X[i,:])\n",
    "        return ypredict\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "nbc = NaiveBayesClassifier()\n",
    "nbc.fit(xtrain,ytrain,validation_data=(xval,yval))\n",
    "ypredict_nbc = nbc.batchPrediction(xtest)\n",
    "\n",
    "print_performance_metrics(ypredict_nbc, ytest, 2)\n",
    "\n",
    "\n",
    "# # Parzen Window\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "class ParzenWindow():\n",
    "    \"\"\"\n",
    "    Parzen Window\n",
    "    \"\"\"\n",
    "    def __init__(self, X, window_function=\"gaussian\"):\n",
    "        self.X = X\n",
    "    def func_val_gaussian(self, x):\n",
    "        val = 0.0\n",
    "        for pts in self.X:\n",
    "            val += np.exp(-0.5 * np.dot(x-pts, (x-pts).T)) / len(self.X)*(np.sqrt(2 * np.pi))**pts.shape[0]\n",
    "        return val\n",
    "    def posterior(self, x):\n",
    "        _posterior = self.func_val_gaussian(x)\n",
    "        return _posterior\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "def Parzen_pred(xtrain, ytrain, xtest):\n",
    "    X_1 = []\n",
    "    X_0 = []\n",
    "    for i in range(len(ytrain)):\n",
    "        if ytrain[i][0]==1:\n",
    "            X_1.append(xtrain[i])\n",
    "        else:\n",
    "            X_0.append(xtrain[i])\n",
    "    X_0 = np.array(X_0)\n",
    "    X_1 = np.array(X_1)\n",
    "    post_1 = ParzenWindow(X_1)\n",
    "    post_0 = ParzenWindow(X_0)\n",
    "    \n",
    "    y_pred = np.zeros((624,1))\n",
    "    for j in range (624):\n",
    "        if post_1.posterior(xtest[j])>post_0.posterior(xtest[j]):\n",
    "            y_pred[j] = 1\n",
    "        else:\n",
    "            y_pred[j] = 0 \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1159a",
   "metadata": {},
   "source": [
    "# 2.2 Multi-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b33e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Library Imports\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "from skimage import img_as_float\n",
    "from skimage.io import imread_collection,imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "import decimal\n",
    "decimal.getcontext().prec = 100\n",
    "\n",
    "\n",
    "# # Data Loading and Preprocessing\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "data = load('bloodmnist.npz')\n",
    "lst = data.files\n",
    "xtrain = data[lst[0]]\n",
    "xval = data[lst[1]]\n",
    "xtest = data[lst[2]]\n",
    "ytrain = data[lst[3]]\n",
    "yval = data[lst[4]]\n",
    "ytest = data[lst[5]]\n",
    "\n",
    "xtrain = np.reshape(xtrain,(-1,28*28))\n",
    "xval = np.reshape(xval,(-1,28*28))\n",
    "xtest = np.reshape(xtest,(-1,28*28))\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "class OneHotEncoder:\n",
    "    def __init__(self,classes=2,type='none'):\n",
    "        self.classes = classes\n",
    "        self.type = type\n",
    "        pass\n",
    "\n",
    "    def transform(self,X:np.ndarray):\n",
    "        self.classes = int(np.max(X)+1)\n",
    "        if self.type=='none':\n",
    "            Y = -1*np.ones((len(X),self.classes))\n",
    "        else:\n",
    "            Y = np.zeros((len(X),self.classes))\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            Y[i,X[i]] = 1\n",
    "        return Y\n",
    "\n",
    "    def inverse_transform(self,X:np.ndarray):\n",
    "        Y = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            Y[i] = 0\n",
    "            mx = X[i,0]\n",
    "            for j in range(self.classes):\n",
    "                if X[i,j] > mx:\n",
    "                    Y[i] = j\n",
    "                    mx = X[i,j] \n",
    "        return Y\n",
    "\n",
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "    def fit(self,X:np.ndarray):\n",
    "        self.mean = np.mean(X)\n",
    "        self.std = np.std(X)\n",
    "  \n",
    "    def transform(self,X:np.ndarray):\n",
    "        return (X-self.mean)/self.std\n",
    "\n",
    "    def inverse_transform(self,X:np.ndarray):\n",
    "        return X*self.std+self.mean\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "x_scalar = StandardScaler()\n",
    "x_scalar.fit(xtrain)\n",
    "xtrain = x_scalar.transform(xtrain)\n",
    "xval = x_scalar.transform(xval)\n",
    "xtest = x_scalar.transform(xtest)\n",
    "\n",
    "#MultiClass Classification\n",
    "encoder = OneHotEncoder(8,type='zeros') \n",
    "\n",
    "ytrain = encoder.transform(ytrain)\n",
    "\n",
    "\n",
    "# #  Performance Metric Related Functions\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def MeanSquaredError(ydata,ypredict):\n",
    "    return np.sqrt(np.mean(np.square(ydata-ypredict)))\n",
    "\n",
    "def ReturnConfMatrix(ypredict,ytest,N=2):\n",
    "    Conf = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            a = np.where(ypredict==i,1,0)\n",
    "            b = np.where(ytest==j,1,0)\n",
    "            c = a*b\n",
    "            Conf[i,j] = np.sum(c)\n",
    "    return Conf\n",
    "\n",
    "def EvaluateAccuracy(ypredict,ytest,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    I = np.identity(N)\n",
    "    Diag = Conf*I\n",
    "    return np.sum(Diag)/max(np.sum(Conf),0.001)\n",
    "\n",
    "def EvaluatePrecision(ypredict,ytest,i,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    return max(Conf[i,i],0.001)/max(np.sum(Conf[i,:]),0.001)\n",
    "\n",
    "def EvaluateRecall(ypredict,ytest,i,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    return max(Conf[i,i],0.001)/max(np.sum(Conf[:,i]),0.001)\n",
    "\n",
    "def EvaluateF1score(ypredict,ytest,i,N=2):\n",
    "    p = EvaluatePrecision(ypredict,ytest,i,N)\n",
    "    r = EvaluateRecall(ypredict,ytest,i,N)\n",
    "    return 2*r*p/(r+p)\n",
    "\n",
    "def EvaluateFalsePositiveRate(ypredict,ytest,i,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    t1=max(np.sum(Conf[i,:]),0.001)\n",
    "    t2=t1-Conf[i,i]\n",
    "    t3=max(t2,0.001)\n",
    "    return t3/max(np.sum(Conf[i,:]),0.001)\n",
    "\n",
    "def EvaluateAUC(ypredict,ytest,i,N=2):\n",
    "    return EvaluatePrecision(ypredict,ytest,i,N)/EvaluateFalsePositiveRate(ypredict,ytest,i,N)\n",
    "\n",
    "def EvaluateCategoricalCrossEntropy(ypredict,ytest):\n",
    "    return np.abs(np.sum(np.log(ypredict)@ytest.transpose()))\n",
    "\n",
    "def tptnfpfn(ypredict,ytest,N=2):  # Function to calculate True/False Positives/Negatives\n",
    "    Conf = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            a = np.where(ypredict==i,1,0)\n",
    "            b = np.where(ytest==j,1,0)\n",
    "            c = a*b\n",
    "            Conf[i,j] = np.sum(c)\n",
    "    tp=np.zeros(N)\n",
    "    tn=np.zeros(N)\n",
    "    fp=np.zeros(N)\n",
    "    fn=np.zeros(N)\n",
    "    for i in range(N):\n",
    "        tp[i]=Conf[i,i]\n",
    "    ConfSum=np.sum(Conf)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            fp[i]+=Conf[j,i]\n",
    "            fn[i]+=Conf[i,j]\n",
    "        temp1=fp[i]\n",
    "        temp2=fn[i]\n",
    "        fp[i]-=Conf[i,i]\n",
    "        fn[i]-=Conf[i,i]\n",
    "        tn[i]=ConfSum-temp1-temp2+Conf[i,i]\n",
    "\n",
    "    return tp,tn,fp,fn \n",
    "\n",
    "def plot(errors,iter,title=''):\n",
    "    plt.title(title)\n",
    "    plt.plot(range(iter),errors)\n",
    "    plt.xticks(np.linspace(start=0,stop=iter,num=11))\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('MeanSquaredError')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "def print_performance_metrics(y_predict, y_test, N):\n",
    "    num = len(y_predict)\n",
    "    y_predict=np.reshape(y_predict,(num,1))\n",
    "    y_test=np.reshape(y_test,(num,1))\n",
    "    \n",
    "    print('Accuracy',EvaluateAccuracy(y_predict,y_test,N))\n",
    "    \n",
    "    for i in range(N):\n",
    "        f1 = EvaluatePrecision(y_predict,y_test,i,N)\n",
    "        print(f'Precision : {i} = {f1:.3f}')\n",
    "\n",
    "    for i in range(N):\n",
    "        f1 = EvaluateRecall(y_predict,y_test,i,N)\n",
    "        print(f'Recall : {i} = {f1:.3f}')\n",
    "\n",
    "    for i in range(N):\n",
    "        f1 = EvaluateF1score(y_predict,y_test,i,N)\n",
    "        print(f'F1 Score : {i} = {f1:.3f}')\n",
    "\n",
    "    for i in range(N):\n",
    "        f1 = EvaluateAUC(y_predict,y_test,i,N)\n",
    "        print(f'AUC : {i} = {f1:.3f}')\n",
    "\n",
    "    print('Confusion Matrix',ReturnConfMatrix(y_predict,y_test,N))\n",
    "\n",
    "    tp,tn,fp,fn = tptnfpfn(y_predict,y_test,N)\n",
    "    print('True Positives',tp)\n",
    "    print('True Negatives',tn)\n",
    "    print('False Positives',fp)\n",
    "    print('False Negatives',fn)\n",
    "\n",
    "\n",
    "# # MLE\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "class GaussianMaxLikelihoodEstimate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,X:np.ndarray):\n",
    "        self.mean = np.mean(X,axis=0)\n",
    "        self.cov = np.cov(X.transpose())\n",
    "        self.cov = self.cov + 0.01*np.identity(self.cov.shape[0])\n",
    "        self.det = np.linalg.det(self.cov)\n",
    "        self.inv = np.linalg.inv(self.cov)\n",
    "        pass\n",
    "\n",
    "    def predict(self,x):\n",
    "        x = x-self.mean\n",
    "        return np.exp(-1*(x@(self.inv)@x)/2)\n",
    "\n",
    "class GaussianBayesClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X:np.ndarray,Y:np.ndarray,validation_data=False):\n",
    "        self.classes = 8\n",
    "        self.Gmles = []\n",
    "        for j in range(self.classes):\n",
    "            data = []\n",
    "            for x,y in zip(X,Y):\n",
    "                if y[j]==1:\n",
    "                    data.append(x)\n",
    "            data = np.array(data)\n",
    "            gmle = GaussianMaxLikelihoodEstimate()\n",
    "            gmle.fit(data)\n",
    "            self.Gmles.append(gmle)\n",
    "    \n",
    "        if type(validation_data)!=bool:\n",
    "            return EvaluateAccuracy(self.batchPrediction(validation_data[0]),validation_data[1],self.classes)\n",
    "  \n",
    "    def predict(self,x):\n",
    "        curr=-1\n",
    "        mx=-1\n",
    "        mx = 0\n",
    "        for i,gmle in enumerate(self.Gmles):\n",
    "            res = gmle.predict(x)\n",
    "            if res > curr:\n",
    "                mx = i\n",
    "                curr = res\n",
    "        return mx\n",
    "\n",
    "\n",
    "    def batchPrediction(self,X:np.ndarray):\n",
    "        ypredict = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            ypredict[i] = self.predict(X[i,:])\n",
    "        return ypredict\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "gm = GaussianBayesClassifier()\n",
    "gm.fit(xtrain,ytrain,validation_data=(xval,yval))\n",
    "\n",
    "ypredict_gm = gm.batchPrediction(xtest)\n",
    "\n",
    "print_performance_metrics(ypredict_gm, ytest, 8)\n",
    "\n",
    "\n",
    "# # Logistic Regression\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "class LogisticRegressor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def preprocess(self,x):\n",
    "        d = len(x)\n",
    "        x_new = np.ones(d+1)\n",
    "        k=0\n",
    "        for i in x:\n",
    "            x_new[k] = i\n",
    "            k+=1\n",
    "        return x_new\n",
    "  \n",
    "    def batchPreprocessing(self,X):\n",
    "        d = X.shape[1]\n",
    "        X_new = np.zeros((X.shape[0],d+1))\n",
    "        for i in range(X.shape[0]):\n",
    "            X_new[i] = self.preprocess(X[i])\n",
    "        return X_new\n",
    "    \n",
    "    def fit(self,X:np.ndarray,Y:np.ndarray,iter=20,alpha=1,beta=0.9,gamma=0.01,validation_data=False,regularization='none'):\n",
    "        '''\n",
    "        X : Input batch\n",
    "        Y : Output batch\n",
    "        iter : Iterations\n",
    "        alpha : step-size\n",
    "        beta : step-size decay\n",
    "        gamma : regularization parameter\n",
    "        regularization : {'none','L1','L2','Elastic'}\n",
    "        '''\n",
    "        X = self.batchPreprocessing(X)\n",
    "        if validation_data!=False:\n",
    "            xval = self.batchPreprocessing(validation_data[0])\n",
    "\n",
    "        self.W = np.zeros((Y.shape[1],X.shape[1]))\n",
    "        Y = np.reshape(Y,(len(Y),-1))\n",
    "      \n",
    "        errors = []\n",
    "        for i in range(iter):\n",
    "            for j in range(Y.shape[1]):\n",
    "                if(regularization=='none'):\n",
    "                    self.W[j,:] = self.W[j,:] - alpha*((self.batchPrediction(X)*(1-self.batchPrediction(X)))[:,j]).transpose()@X/Y.shape[0]\n",
    "                elif(regularization=='L2'):\n",
    "                    self.W[j,:] = self.W[j,:] - alpha*(((self.batchPrediction(X)*(1-self.batchPrediction(X)))[:,j]).transpose()@X/Y.shape[0] - gamma*self.W[j,:])\n",
    "                elif(regularization=='L1'):\n",
    "                    self.W[j,:] = self.W[j,:] - alpha*(((self.batchPrediction(X)*(1-self.batchPrediction(X)))[:,j]).transpose()@X/Y.shape[0] - gamma*np.sign(self.W[j,:]))\n",
    "                elif(regularization=='Elastic'):\n",
    "                    self.W[j,:] = self.W[j,:] - alpha*(((self.batchPrediction(X)*(1-self.batchPrediction(X)))[:,j]).transpose()@X/Y.shape[0] - gamma*np.sign(self.W[j,:]))\n",
    "        alpha = beta*alpha\n",
    "\n",
    "        if type(validation_data)!=bool:\n",
    "            errors.append(EvaluateCategoricalCrossEntropy(self.batchPrediction(xval),validation_data[1]))\n",
    "\n",
    "        return errors\n",
    "\n",
    "\n",
    "    def predict(self,x:np.ndarray):\n",
    "        if x.shape[0]!=self.W.shape[1]:\n",
    "            x = self.preprocess(x)\n",
    "        x = np.reshape(x,(len(x),-1))\n",
    "        y = self.W@x\n",
    "        for i in range(y.shape[0]):\n",
    "            y[i] = 1/(1+np.exp(y[i]))\n",
    "        y = np.reshape(y,(y.shape[0],))\n",
    "        return y\n",
    "\n",
    "    def batchPrediction(self,X:np.ndarray):\n",
    "        ypredict = []\n",
    "        for i in range(X.shape[0]):\n",
    "            ypredict.append(self.predict(X[i,:]))\n",
    "        ypredict = np.array(ypredict)\n",
    "        return ypredict\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "lm = LogisticRegressor()\n",
    "\n",
    "iter = 50\n",
    "errors = lm.fit(xtrain,ytrain,alpha=0.001,beta=1,iter=iter,validation_data=(xval,encoder.transform(yval)))\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "ypredict_lm = lm.batchPrediction(xtest)\n",
    "encoder = OneHotEncoder()\n",
    "ypredict_lm = encoder.inverse_transform(ypredict_lm)\n",
    "\n",
    "print_performance_metrics(ypredict_lm, ytest, 8)\n",
    "\n",
    "\n",
    "# # K Nearest Neighbour\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "class KNNclassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def EUC_DIST(self,v1,v2): \n",
    "        v1,v2 = np.array(v1),np.array(v2)\n",
    "        distance = np.sum((v1-v2)**2)\n",
    "        return np.sqrt(distance)\n",
    "    \n",
    "    def Predict(self,k,Xtrain,Ytrain,Xtest_instance): \n",
    "        distances = [] \n",
    "        for i in range(len(Xtrain)):\n",
    "            dist = self.EUC_DIST(Xtrain[i], Xtest_instance)\n",
    "            distances.append((Ytrain[i],dist)) \n",
    "        distances.sort(key=lambda x: x[1]) \n",
    "        neighbors = []\n",
    "        for i in range(k):\n",
    "            neighbors.append(distances[i][0])\n",
    "        classes = {}\n",
    "        for i in range(len(neighbors)):\n",
    "            response = neighbors[i][1]\n",
    "            if response in classes.keys():\n",
    "                classes[int(response)] += 1\n",
    "            else:\n",
    "                classes[int(response)] = 1\n",
    "        sorted_classes = sorted(classes.items() , key = lambda x: x[1],reverse = True )\n",
    "        return sorted_classes[0][0]\n",
    "\n",
    "    def batchPrediction(self,Knn,Xtrain,Ytrain,Xtest):\n",
    "        ypredict=[]\n",
    "        for i in range(Xtest.shape[0]):\n",
    "            ypredict.append(self.Predict(Knn,Xtrain,Ytrain,Xtest[i,:]))\n",
    "        ypredict = np.array(ypredict)\n",
    "        return ypredict\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "knn = KNNclassifier()\n",
    "\n",
    "ypredict_k1 = knn.batchPrediction(1,xtrain,ytrain,xtest)\n",
    "print_performance_metrics(ypredict_k1, ytest, 8)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "ypredict_k2 = knn.batchPrediction(2,xtrain,ytrain,xtest)\n",
    "print_performance_metrics(ypredict_k2, ytest, 8)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "ypredict_k5 = knn.batchPrediction(5,xtrain,ytrain,xtest)\n",
    "print_performance_metrics(ypredict_k5, ytest, 8)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "ypredict_k9 = knn.batchPrediction(9,xtrain,ytrain,xtest)\n",
    "print_performance_metrics(ypredict_k9, ytest, 8)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "ypredict_k11 = knn.batchPrediction(11,xtrain,ytrain,xtest)\n",
    "print_performance_metrics(ypredict_k11, ytest, 8)\n",
    "\n",
    "\n",
    "# # Naive Bayes\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "class GaussianNaiveMLE:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "    def fit(self,X:np.ndarray):\n",
    "        self.mean = np.mean(X,axis=0)\n",
    "        self.cov = np.cov(X.transpose())    \n",
    "        self.cov = self.cov*np.identity(self.cov.shape[0])\n",
    "        self.cov = self.cov + 0.01*np.identity(self.cov.shape[0])\n",
    "        self.det = np.linalg.det(self.cov)\n",
    "        self.inv = np.linalg.inv(self.cov)\n",
    "        pass\n",
    "  \n",
    "    def predict(self,x):\n",
    "        x = x-self.mean\n",
    "        return np.exp(-1*(x@(self.inv)@x)/2)\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "    def fit(self,X:np.ndarray,Y:np.ndarray,validation_data=False):\n",
    "        x,y = (np.shape(Y))\n",
    "        self.classes = y\n",
    "        self.Gmles = []\n",
    "        for j in range(self.classes):\n",
    "            data = []\n",
    "        for x,y in zip(X,Y):\n",
    "            if y[j]==1:\n",
    "                data.append(x)\n",
    "        data = np.array(data)\n",
    "        gmle = GaussianNaiveMLE()\n",
    "        gmle.fit(data)\n",
    "        self.Gmles.append(gmle)\n",
    "    \n",
    "        if type(validation_data)!=bool:\n",
    "            return EvaluateAccuracy(self.batchPrediction(validation_data[0]),validation_data[1],self.classes)\n",
    "  \n",
    "    def predict(self,x):\n",
    "        curr=-1\n",
    "        mx=-1\n",
    "        mx = 0\n",
    "        for i,gmle in enumerate(self.Gmles):\n",
    "            res = gmle.predict(x)\n",
    "            if res > curr:\n",
    "                mx = i\n",
    "                curr = res\n",
    "        return mx\n",
    "\n",
    "\n",
    "    def batchPrediction(self,X:np.ndarray):\n",
    "        ypredict = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            ypredict[i] = self.predict(X[i,:])\n",
    "        return ypredict\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "nbc = NaiveBayesClassifier()\n",
    "nbc.fit(xtrain,ytrain,validation_data=(xval,yval))\n",
    "ypredict_nbc = nbc.batchPrediction(xtest)\n",
    "\n",
    "print_performance_metrics(ypredict_nbc, ytest, 8)\n",
    "\n",
    "\n",
    "# # Parzen Window\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "class ParzenWindow():\n",
    "    \"\"\"\n",
    "    Parzen Window\n",
    "    \"\"\"\n",
    "    def __init__(self, X, window_function=\"gaussian\"):\n",
    "        self.X = X\n",
    "    def func_val_gaussian(self, x):\n",
    "        val = 0.0\n",
    "        for pts in self.X:\n",
    "            val += np.exp(-0.5 * np.dot(x-pts, (x-pts).T)) / len(self.X)*(np.sqrt(2 * np.pi))**pts.shape[0]\n",
    "        return val\n",
    "    def posterior(self, x):\n",
    "        _posterior = self.func_val_gaussian(x)\n",
    "        return _posterior\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "def Parzen_pred(xtrain, ytrain, xtest):\n",
    "    X_1 = []\n",
    "    X_0 = []\n",
    "    for i in range(len(ytrain)):\n",
    "        if ytrain[i][0]==1:\n",
    "            X_1.append(xtrain[i])\n",
    "        else:\n",
    "            X_0.append(xtrain[i])\n",
    "    X_0 = np.array(X_0)\n",
    "    X_1 = np.array(X_1)\n",
    "    post_1 = ParzenWindow(X_1)\n",
    "    post_0 = ParzenWindow(X_0)\n",
    "    \n",
    "    y_pred = np.zeros((624,1))\n",
    "    for j in range (624):\n",
    "        if post_1.posterior(xtest[j])>post_0.posterior(xtest[j]):\n",
    "            y_pred[j] = 1\n",
    "        else:\n",
    "            y_pred[j] = 0 \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "y_predict_pw = Parzen_pred(xtrain, ytrain, xtest)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "print_performance_metrics(y_predict_pw, ytest, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db3c68",
   "metadata": {},
   "source": [
    "# 2.3 Bounding Box Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#library imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "path_img = Path('Q3_data/images')\n",
    "path_ann = Path('Q3_data/annotations')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def filelist(root, file_type):\n",
    "    return [os.path.join(directory_path, f) for directory_path, directory_name, files in os.walk(root) for f in files if f.endswith(file_type)]\n",
    "\n",
    "def Img_df (path_ann):\n",
    "    annotations = filelist(path_ann, '.xml')\n",
    "    ann_list = []\n",
    "    for path_ann in annotations:\n",
    "        root = ET.parse(path_ann).getroot()\n",
    "        ann = {}\n",
    "        ann['filename'] = Path(str(path_img) + '/'+ root.find(\"./filename\").text)\n",
    "        ann['xmin'] = int(root.find(\"./object/bndbox/xmin\").text)\n",
    "        ann['ymin'] = int(root.find(\"./object/bndbox/ymin\").text)\n",
    "        ann['xmax'] = int(root.find(\"./object/bndbox/xmax\").text)\n",
    "        ann['ymax'] = int(root.find(\"./object/bndbox/ymax\").text)\n",
    "        ann_list.append(ann)\n",
    "    return pd.DataFrame(ann_list)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "df_train = Img_df(path_ann)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#Reading an image\n",
    "def read_image(path):\n",
    "    img = cv2.imread(str(path))\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def create_mask(bb, x):\n",
    "    \"\"\"Creates a mask for the bounding box of same shape as image\"\"\"\n",
    "    rows,cols,*_ = x.shape\n",
    "    Y = np.zeros((rows, cols))\n",
    "    bb = bb.astype(int)\n",
    "    Y[bb[0]:bb[2], bb[1]:bb[3]] = 1.\n",
    "    return Y\n",
    "\n",
    "def mask_to_bb(Y):\n",
    "    \"\"\"Convert mask Y to a bounding box, assumes 0 as background nonzero object\"\"\"\n",
    "    cols, rows = np.nonzero(Y)\n",
    "    if len(cols)==0: \n",
    "        return np.zeros(4, dtype=np.float32)\n",
    "    top_row = np.min(rows)\n",
    "    left_col = np.min(cols)\n",
    "    bottom_row = np.max(rows)\n",
    "    right_col = np.max(cols)\n",
    "    return np.array([left_col, top_row, right_col, bottom_row], dtype=np.float32)\n",
    "\n",
    "def create_bb_array(x):\n",
    "    \"\"\"Generates bounding box array from a train_df row\"\"\"\n",
    "    return np.array([x[2],x[1],x[4],x[3]])\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def resize_image_bb(read_path,write_path,bb,sz):\n",
    "    \"\"\"Resize an image and its bounding box and write image to new path\"\"\"\n",
    "    im = read_image(read_path)\n",
    "    im_resized = cv2.resize(im, (int(1.49*sz), sz))\n",
    "    Y_resized = cv2.resize(create_mask(bb, im), (int(1.49*sz), sz))\n",
    "    new_path = str(write_path/read_path.parts[-1])\n",
    "    cv2.imwrite(new_path, cv2.cvtColor(im_resized, cv2.COLOR_RGB2BGR))\n",
    "    return new_path, mask_to_bb(Y_resized)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "#Populating Training DF with new paths and bounding boxes\n",
    "new_paths = []\n",
    "new_bbs = []\n",
    "train_path_resized = Path('Q3_data/images_resized')\n",
    "for index, row in df_train.iterrows():\n",
    "    new_path,new_bb = resize_image_bb(row['filename'], train_path_resized, create_bb_array(row.values),30)\n",
    "    new_paths.append(new_path)\n",
    "    new_bbs.append(new_bb)\n",
    "df_train['new_path'] = new_paths\n",
    "df_train['new_bb'] = new_bbs\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "X = []\n",
    "for i in range(0, 877):\n",
    "    im = read_image(df_train.values[i][5])\n",
    "    im_v = im.flatten()\n",
    "    X.append(im_v)\n",
    "X = np.array(X)\n",
    "X = np.append(X, np.ones((877,1)), axis = 1)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "Y = df_train['new_bb'].to_numpy().copy()\n",
    "Z =[]\n",
    "for i in range(0,877):\n",
    "    Z.append(Y[i])\n",
    "Z= np.array(Z)\n",
    "Y = Z\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def MSE(X, Y, w_opt):\n",
    "    Y_ = np.matmul(X, w_opt)\n",
    "    err = 0\n",
    "    for i in range(0,877):\n",
    "        err += np.linalg.norm(Y_-Y)\n",
    "    mse = err/877\n",
    "    return mse\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def MAE(X, Y, w_opt):\n",
    "    Y_ = np.matmul(X, w_opt)\n",
    "    err = 0\n",
    "    for i in range(0,877):\n",
    "        err += np.sum(np.absolute(Y_-Y))\n",
    "    mae = err /(877*4)\n",
    "    return mae\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "def MIOU(X, Y, w_opt):\n",
    "    Y_ = np.matmul(X, w_opt)\n",
    "    iou = 0\n",
    "    for i in range(0,877):\n",
    "        boxA = Y_[i]\n",
    "        boxB = Y[i]\n",
    "        xA = max(boxA[0], boxB[0])\n",
    "        yA = max(boxA[1], boxB[1])\n",
    "        xB = min(boxA[2], boxB[2])\n",
    "        yB = min(boxA[3], boxB[3])\n",
    "        # compute the area of intersection rectangle\n",
    "        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "        boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "        boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "        iou += interArea / float(boxAArea + boxBArea - interArea)\t\n",
    "    miou = iou/877\n",
    "    return miou\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "L = []\n",
    "MSErr = []\n",
    "MAErr = []\n",
    "MeanIOU = []\n",
    "W=[]\n",
    "# Ridge Regression \n",
    "for lam in range(0, 10):\n",
    "    w_opt= np.matmul(np.linalg.pinv(np.matmul(X.T,X) - lam * np.eye(3961)),np.matmul(X.T,Y))\n",
    "    W.append(w_opt)\n",
    "    mse = MSE(X, Y, w_opt)\n",
    "    mae = MAE(X, Y, w_opt)\n",
    "    miou = MIOU(X, Y, w_opt)\n",
    "    L.append(lam)\n",
    "    MSErr.append(mse)\n",
    "    MAErr.append(mae)\n",
    "    MeanIOU.append(miou)\n",
    "    print('lambda:',lam, 'MSE:', mse,'MAE:', mae, 'MIOU:', miou)\n",
    "\n",
    "plt.plot(MSErr)\n",
    "plt.plot(MAErr)\n",
    " \n",
    "plt.legend([\"MSE\", \"MAE\"])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(MeanIOU)\n",
    "plt.legend([\"Mean IOU\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "def create_corner_rect(bb, color='red'):\n",
    "    bb = np.array(bb, dtype=np.float32)\n",
    "    return plt.Rectangle((bb[1], bb[0]), bb[3]-bb[1], bb[2]-bb[0], color=color,\n",
    "                         fill=False, lw=3)\n",
    "\n",
    "def show_corner_bb(im, bb):\n",
    "    plt.imshow(im)\n",
    "    plt.gca().add_patch(create_corner_rect(bb))\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "im = cv2.imread(str(df_train.values[100][5]))\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "show_corner_bb(im, df_train.values[100][6])\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "w = np.array(W[0])\n",
    "Y_ = np.matmul(X, w)\n",
    "show_corner_bb(im, Y_[100])\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "im = cv2.imread(str(df_train.values[98][5]))\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "show_corner_bb(im, df_train.values[98][6])\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "w = np.array(W[0])\n",
    "Y_ = np.matmul(X, w)\n",
    "show_corner_bb(im, Y_[98])\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "im = cv2.imread(str(df_train.values[110][5]))\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "show_corner_bb(im, df_train.values[110][6])\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "w = np.array(W[0])\n",
    "Y_ = np.matmul(X, w)\n",
    "show_corner_bb(im, Y_[110])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3398f9",
   "metadata": {},
   "source": [
    "# 2.4 Frame classification on audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from sklearn.mixture._base import  BaseMixture, _check_shape\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.extmath import row_norms\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "dir_path = \"/home/ece/Piyush/Coursework/PRNN/Assignment 1/archive/data/TRAIN/DR1/\"\n",
    "\n",
    "sample_rate = 16000\n",
    "\n",
    "\n",
    "\n",
    "def _check_weights(weights, n_components):\n",
    "    weights = check_array(weights, dtype=[np.float64, np.float32], ensure_2d=False)\n",
    "    _check_shape(weights, (n_components,), \"weights\")\n",
    "\n",
    "    # check range\n",
    "    if any(np.less(weights, 0.0)) or any(np.greater(weights, 1.0)):\n",
    "        raise ValueError(\n",
    "            \"The parameter 'weights' should be in the range \"\n",
    "            \"[0, 1], but got max value %.5f, min value %.5f\"\n",
    "            % (np.min(weights), np.max(weights))\n",
    "        )\n",
    "\n",
    "    # check normalization\n",
    "    if not np.allclose(np.abs(1.0 - np.sum(weights)), 0.0):\n",
    "        raise ValueError(\n",
    "            \"The parameter 'weights' should be normalized, but got sum(weights) = %.5f\"\n",
    "            % np.sum(weights)\n",
    "        )\n",
    "    return weights\n",
    "\n",
    "\n",
    "def _check_means(means, n_components, n_features):\n",
    "\n",
    "    means = check_array(means, dtype=[np.float64, np.float32], ensure_2d=False)\n",
    "    _check_shape(means, (n_components, n_features), \"means\")\n",
    "    return means\n",
    "\n",
    "\n",
    "def _check_precision_positivity(precision, covariance_type):\n",
    "    if np.any(np.less_equal(precision, 0.0)):\n",
    "        raise ValueError(\"'%s precision' should be positive\" % covariance_type)\n",
    "\n",
    "\n",
    "def _check_precision_matrix(precision, covariance_type):\n",
    "    if not (\n",
    "        np.allclose(precision, precision.T) and np.all(linalg.eigvalsh(precision) > 0.0)\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"'%s precision' should be symmetric, positive-definite\" % covariance_type\n",
    "        )\n",
    "\n",
    "\n",
    "def _check_precisions_full(precisions, covariance_type):\n",
    "    for prec in precisions:\n",
    "        _check_precision_matrix(prec, covariance_type)\n",
    "\n",
    "\n",
    "def _check_precisions(precisions, covariance_type, n_components, n_features):\n",
    "    precisions = check_array(\n",
    "        precisions,\n",
    "        dtype=[np.float64, np.float32],\n",
    "        ensure_2d=False,\n",
    "        allow_nd=covariance_type == \"full\",\n",
    "    )\n",
    "\n",
    "    precisions_shape = {\n",
    "        \"full\": (n_components, n_features, n_features),\n",
    "        \"tied\": (n_features, n_features),\n",
    "        \"diag\": (n_components, n_features),\n",
    "        \"spherical\": (n_components,),\n",
    "    }\n",
    "    _check_shape(\n",
    "        precisions, precisions_shape[covariance_type], \"%s precision\" % covariance_type\n",
    "    )\n",
    "\n",
    "    _check_precisions = {\n",
    "        \"full\": _check_precisions_full,\n",
    "        \"tied\": _check_precision_matrix,\n",
    "        \"diag\": _check_precision_positivity,\n",
    "        \"spherical\": _check_precision_positivity,\n",
    "    }\n",
    "    _check_precisions[covariance_type](precisions, covariance_type)\n",
    "    return precisions\n",
    "\n",
    "\n",
    "def _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar):\n",
    "    n_components, n_features = means.shape\n",
    "    covariances = np.empty((n_components, n_features, n_features))\n",
    "    for k in range(n_components):\n",
    "        diff = X - means[k]\n",
    "        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]\n",
    "        covariances[k].flat[:: n_features + 1] += reg_covar\n",
    "    return covariances\n",
    "\n",
    "\n",
    "def _estimate_gaussian_covariances_tied(resp, X, nk, means, reg_covar):\n",
    "\n",
    "    avg_X2 = np.dot(X.T, X)\n",
    "    avg_means2 = np.dot(nk * means.T, means)\n",
    "    covariance = avg_X2 - avg_means2\n",
    "    covariance /= nk.sum()\n",
    "    covariance.flat[:: len(covariance) + 1] += reg_covar\n",
    "    return covariance\n",
    "\n",
    "\n",
    "def _estimate_gaussian_covariances_diag(resp, X, nk, means, reg_covar):\n",
    "    avg_X2 = np.dot(resp.T, X * X) / nk[:, np.newaxis]\n",
    "    avg_means2 = means ** 2\n",
    "    avg_X_means = means * np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "    return avg_X2 - 2 * avg_X_means + avg_means2 + reg_covar\n",
    "\n",
    "\n",
    "def _estimate_gaussian_covariances_spherical(resp, X, nk, means, reg_covar):\n",
    "    return _estimate_gaussian_covariances_diag(resp, X, nk, means, reg_covar).mean(1)\n",
    "\n",
    "\n",
    "def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type):\n",
    "    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    means = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "    covariances = {\n",
    "        \"full\": _estimate_gaussian_covariances_full,\n",
    "        \"tied\": _estimate_gaussian_covariances_tied,\n",
    "        \"diag\": _estimate_gaussian_covariances_diag,\n",
    "        \"spherical\": _estimate_gaussian_covariances_spherical,\n",
    "    }[covariance_type](resp, X, nk, means, reg_covar)\n",
    "    return nk, means, covariances\n",
    "\n",
    "\n",
    "def _compute_precision_cholesky(covariances, covariance_type):\n",
    "    estimate_precision_error_message = (\n",
    "        \"Fitting the mixture model failed because some components have \"\n",
    "        \"ill-defined empirical covariance (for instance caused by singleton \"\n",
    "        \"or collapsed samples). Try to decrease the number of components, \"\n",
    "        \"or increase reg_covar.\"\n",
    "    )\n",
    "\n",
    "    if covariance_type == \"full\":\n",
    "        n_components, n_features, _ = covariances.shape\n",
    "        precisions_chol = np.empty((n_components, n_features, n_features))\n",
    "        for k, covariance in enumerate(covariances):\n",
    "            try:\n",
    "                cov_chol = linalg.cholesky(covariance, lower=True)\n",
    "            except linalg.LinAlgError:\n",
    "                raise ValueError(estimate_precision_error_message)\n",
    "            precisions_chol[k] = linalg.solve_triangular(\n",
    "                cov_chol, np.eye(n_features), lower=True\n",
    "            ).T\n",
    "    elif covariance_type == \"tied\":\n",
    "        _, n_features = covariances.shape\n",
    "        try:\n",
    "            cov_chol = linalg.cholesky(covariances, lower=True)\n",
    "        except linalg.LinAlgError:\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol = linalg.solve_triangular(\n",
    "            cov_chol, np.eye(n_features), lower=True\n",
    "        ).T\n",
    "    else:\n",
    "        if np.any(np.less_equal(covariances, 0.0)):\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol = 1.0 / np.sqrt(covariances)\n",
    "    return precisions_chol\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Gaussian mixture probability estimators\n",
    "def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):\n",
    "    if covariance_type == \"full\":\n",
    "        n_components, _, _ = matrix_chol.shape\n",
    "        log_det_chol = np.sum(\n",
    "            np.log(matrix_chol.reshape(n_components, -1)[:, :: n_features + 1]), 1\n",
    "        )\n",
    "\n",
    "    elif covariance_type == \"tied\":\n",
    "        log_det_chol = np.sum(np.log(np.diag(matrix_chol)))\n",
    "\n",
    "    elif covariance_type == \"diag\":\n",
    "        log_det_chol = np.sum(np.log(matrix_chol), axis=1)\n",
    "\n",
    "    else:\n",
    "        log_det_chol = n_features * (np.log(matrix_chol))\n",
    "\n",
    "    return log_det_chol\n",
    "\n",
    "\n",
    "def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components, _ = means.shape\n",
    "    log_det = _compute_log_det_cholesky(precisions_chol, covariance_type, n_features)\n",
    "\n",
    "    if covariance_type == \"full\":\n",
    "        log_prob = np.empty((n_samples, n_components))\n",
    "        for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):\n",
    "            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n",
    "            log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "\n",
    "    elif covariance_type == \"tied\":\n",
    "        log_prob = np.empty((n_samples, n_components))\n",
    "        for k, mu in enumerate(means):\n",
    "            y = np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)\n",
    "            log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "\n",
    "    elif covariance_type == \"diag\":\n",
    "        precisions = precisions_chol ** 2\n",
    "        log_prob = (\n",
    "            np.sum((means ** 2 * precisions), 1)\n",
    "            - 2.0 * np.dot(X, (means * precisions).T)\n",
    "            + np.dot(X ** 2, precisions.T)\n",
    "        )\n",
    "\n",
    "    elif covariance_type == \"spherical\":\n",
    "        precisions = precisions_chol ** 2\n",
    "        log_prob = (\n",
    "            np.sum(means ** 2, 1) * precisions\n",
    "            - 2 * np.dot(X, means.T * precisions)\n",
    "            + np.outer(row_norms(X, squared=True), precisions)\n",
    "        )\n",
    "    return -0.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n",
    "\n",
    "\n",
    "class GaussianMixture(BaseMixture):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_components=1,\n",
    "        *,\n",
    "        covariance_type=\"full\",\n",
    "        tol=1e-3,\n",
    "        reg_covar=1e-6,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        init_params=\"kmeans\",\n",
    "        weights_init=None,\n",
    "        means_init=None,\n",
    "        precisions_init=None,\n",
    "        random_state=None,\n",
    "        warm_start=False,\n",
    "        verbose=0,\n",
    "        verbose_interval=10,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_components=n_components,\n",
    "            tol=tol,\n",
    "            reg_covar=reg_covar,\n",
    "            max_iter=max_iter,\n",
    "            n_init=n_init,\n",
    "            init_params=init_params,\n",
    "            random_state=random_state,\n",
    "            warm_start=warm_start,\n",
    "            verbose=verbose,\n",
    "            verbose_interval=verbose_interval,\n",
    "        )\n",
    "\n",
    "        self.covariance_type = covariance_type\n",
    "        self.weights_init = weights_init\n",
    "        self.means_init = means_init\n",
    "        self.precisions_init = precisions_init\n",
    "\n",
    "    def _check_parameters(self, X):\n",
    "        _, n_features = X.shape\n",
    "        if self.covariance_type not in [\"spherical\", \"tied\", \"diag\", \"full\"]:\n",
    "            raise ValueError(\n",
    "                \"Invalid value for 'covariance_type': %s \"\n",
    "                \"'covariance_type' should be in \"\n",
    "                \"['spherical', 'tied', 'diag', 'full']\"\n",
    "                % self.covariance_type\n",
    "            )\n",
    "\n",
    "        if self.weights_init is not None:\n",
    "            self.weights_init = _check_weights(self.weights_init, self.n_components)\n",
    "\n",
    "        if self.means_init is not None:\n",
    "            self.means_init = _check_means(\n",
    "                self.means_init, self.n_components, n_features\n",
    "            )\n",
    "\n",
    "        if self.precisions_init is not None:\n",
    "            self.precisions_init = _check_precisions(\n",
    "                self.precisions_init,\n",
    "                self.covariance_type,\n",
    "                self.n_components,\n",
    "                n_features,\n",
    "            )\n",
    "\n",
    "    def _initialize(self, X, resp):\n",
    "        n_samples, _ = X.shape\n",
    "\n",
    "        weights, means, covariances = _estimate_gaussian_parameters(\n",
    "            X, resp, self.reg_covar, self.covariance_type\n",
    "        )\n",
    "        weights /= n_samples\n",
    "\n",
    "        self.weights_ = weights if self.weights_init is None else self.weights_init\n",
    "        self.means_ = means if self.means_init is None else self.means_init\n",
    "\n",
    "        if self.precisions_init is None:\n",
    "            self.covariances_ = covariances\n",
    "            self.precisions_cholesky_ = _compute_precision_cholesky(\n",
    "                covariances, self.covariance_type\n",
    "            )\n",
    "        elif self.covariance_type == \"full\":\n",
    "            self.precisions_cholesky_ = np.array(\n",
    "                [\n",
    "                    linalg.cholesky(prec_init, lower=True)\n",
    "                    for prec_init in self.precisions_init\n",
    "                ]\n",
    "            )\n",
    "        elif self.covariance_type == \"tied\":\n",
    "            self.precisions_cholesky_ = linalg.cholesky(\n",
    "                self.precisions_init, lower=True\n",
    "            )\n",
    "        else:\n",
    "            self.precisions_cholesky_ = np.sqrt(self.precisions_init)\n",
    "\n",
    "    def _m_step(self, X, log_resp):\n",
    "        n_samples, _ = X.shape\n",
    "        self.weights_, self.means_, self.covariances_ = _estimate_gaussian_parameters(\n",
    "            X, np.exp(log_resp), self.reg_covar, self.covariance_type\n",
    "        )\n",
    "        self.weights_ /= n_samples\n",
    "        self.precisions_cholesky_ = _compute_precision_cholesky(\n",
    "            self.covariances_, self.covariance_type\n",
    "        )\n",
    "\n",
    "    def _estimate_log_prob(self, X):\n",
    "        return _estimate_log_gaussian_prob(\n",
    "            X, self.means_, self.precisions_cholesky_, self.covariance_type\n",
    "        )\n",
    "\n",
    "    def _estimate_log_weights(self):\n",
    "        return np.log(self.weights_)\n",
    "\n",
    "    def _compute_lower_bound(self, _, log_prob_norm):\n",
    "        return log_prob_norm\n",
    "\n",
    "    def _get_parameters(self):\n",
    "        return (\n",
    "            self.weights_,\n",
    "            self.means_,\n",
    "            self.covariances_,\n",
    "            self.precisions_cholesky_,\n",
    "        )\n",
    "\n",
    "    def _set_parameters(self, params):\n",
    "        (\n",
    "            self.weights_,\n",
    "            self.means_,\n",
    "            self.covariances_,\n",
    "            self.precisions_cholesky_,\n",
    "        ) = params\n",
    "\n",
    "        # Attributes computation\n",
    "        _, n_features = self.means_.shape\n",
    "\n",
    "        if self.covariance_type == \"full\":\n",
    "            self.precisions_ = np.empty(self.precisions_cholesky_.shape)\n",
    "            for k, prec_chol in enumerate(self.precisions_cholesky_):\n",
    "                self.precisions_[k] = np.dot(prec_chol, prec_chol.T)\n",
    "\n",
    "        elif self.covariance_type == \"tied\":\n",
    "            self.precisions_ = np.dot(\n",
    "                self.precisions_cholesky_, self.precisions_cholesky_.T\n",
    "            )\n",
    "        else:\n",
    "            self.precisions_ = self.precisions_cholesky_ ** 2\n",
    "\n",
    "    def _n_parameters(self):\n",
    "        _, n_features = self.means_.shape\n",
    "        if self.covariance_type == \"full\":\n",
    "            cov_params = self.n_components * n_features * (n_features + 1) / 2.0\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            cov_params = self.n_components * n_features\n",
    "        elif self.covariance_type == \"tied\":\n",
    "            cov_params = n_features * (n_features + 1) / 2.0\n",
    "        elif self.covariance_type == \"spherical\":\n",
    "            cov_params = self.n_components\n",
    "        mean_params = n_features * self.n_components\n",
    "        return int(cov_params + mean_params + self.n_components - 1)\n",
    "\n",
    "    def bic(self, X):\n",
    "        return -2 * self.score(X) * X.shape[0] + self._n_parameters() * np.log(\n",
    "            X.shape[0]\n",
    "        )\n",
    "\n",
    "    def aic(self, X):\n",
    "        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n",
    "\n",
    "\n",
    "\n",
    "def is_invertible(a):\n",
    "    return a.shape[0] == a.shape[1] and np.linalg.matrix_rank(a) == a.shape[0]\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self,\n",
    "                 sampling_rate=16000, audio_duration=2, n_classes=41,\n",
    "                 use_mfcc=False, n_folds=10, learning_rate=0.0001, \n",
    "                 max_epochs=50, n_mfcc=20):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.audio_duration = audio_duration\n",
    "        self.n_classes = n_classes\n",
    "        self.use_mfcc = use_mfcc\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_folds = n_folds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "        self.audio_length = self.sampling_rate * self.audio_duration\n",
    "        if self.use_mfcc:\n",
    "            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n",
    "        else:\n",
    "            self.dim = (self.audio_length, 1)\n",
    "\n",
    "\n",
    "\n",
    "class GMM():\n",
    "    def __init__(self, n_components, covariance_type='diag'):\n",
    "        self.n_components = n_components\n",
    "        self.covariance_type = covariance_type\n",
    "        # self.iter = 30\n",
    "\n",
    "    def fit(self, X, n_iter=20, thresh=1e-2):\n",
    "        self.means_ = np.zeros((self.n_components, X.shape[1]))\n",
    "        self.covars_ = np.random.random((self.n_components, X.shape[1], X.shape[1]))\n",
    "        self.weights_ = np.zeros(self.n_components)\n",
    "        # self.precs_ = np.zeros((self.n_components, X.shape[1], X.shape[1]))\n",
    "        # self.converged_ = False\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            # likelihood_old = self.likelihood(X)\n",
    "            # E-Step\n",
    "            gamma = self.E_Step(X)\n",
    "            # M-Step\n",
    "            for i in range(self.n_components):\n",
    "                self.weights_[i] = np.sum(gamma[:, i]) / X.shape[0]\n",
    "                print(self.weights_[i])\n",
    "                self.means_[i] = np.sum(self.weights_[i] * X, axis=0) / np.sum(gamma[:, i])\n",
    "                x_mu = X - self.means_[i]\n",
    "                self.covars_[i] = np.sum(self.weights_[i] * np.dot(x_mu.T, x_mu), axis=0) / np.sum(gamma[:, i])\n",
    "\n",
    "            # likelihood_new = self.likelihood(X)\n",
    "            # print(\"Iteration: {} Likelihood: {}\".format(i, likelihood_new))\n",
    "\n",
    "            # if np.abs(likelihood_new - likelihood_old) < thresh:\n",
    "            #     break\n",
    "\n",
    "    def E_Step(self, X):\n",
    "        # Calculate the responsibilities\n",
    "        # Calculate the log likelihood of the data\n",
    "        # return the responsibilities and log likelihood\n",
    "        gamma = np.zeros((X.shape[0], self.n_components))\n",
    "        # print(X.shape)\n",
    "        # 1/0\n",
    "\n",
    "        for n in range(X.shape[0]):\n",
    "            n_factor = 0.0\n",
    "            for i in range(self.n_components):\n",
    "                # print(X[n].shape)\n",
    "                n_factor += self.weights_[i] * self.gaussian(X[n], self.means_[i], self.covars_[i])\n",
    "\n",
    "            for k in range(self.n_components):\n",
    "                gamma[n][k] = (self.weights_[k] * self.gaussian(X[n], self.means_[k], self.covars_[k])) / n_factor\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    def gaussian(self, x, mu, sigma):\n",
    "        # print( (x-mu).T.shape, np.linalg.pinv(sigma).shape, (x-mu).shape)\n",
    "        print( np.dot(np.dot((x - mu).T, np.linalg.inv(sigma)), (x-mu)) , np.linalg.det(sigma))\n",
    "        return np.exp(-1*0.5*np.dot(np.dot((x - mu).T, np.linalg.inv(sigma)), (x-mu))) / np.sqrt(2 * np.pi * np.linalg.det(sigma))\n",
    "\n",
    "    def likelihood(self, X):\n",
    "        # Calculate the log likelihood of the data\n",
    "        # return the log likelihood\n",
    "        llf = 0.0\n",
    "\n",
    "        for n in range(X.shape[0]):\n",
    "            n_factor = 0.0\n",
    "            for i in range(self.n_components):\n",
    "                n_factor += self.weights_[i] * self.gaussian(X[n], self.means_[i], self.covars_[i])\n",
    "\n",
    "            llf += np.log(n_factor)\n",
    "        \n",
    "        return llf\n",
    "    \n",
    "    def posterior(self, x):\n",
    "        prob = 0.0\n",
    "        for i in range(self.n_components):\n",
    "            prob += self.weights_[i] * self.gaussian(x, self.means_[i], self.covars_[i])\n",
    "\n",
    "\n",
    "class MLE():\n",
    "    \"\"\"\n",
    "    Maximum Likelihood Estimator\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        self.mu, self.sigma = self.mle(x)\n",
    "    \n",
    "    def mle(self, x):\n",
    "\n",
    "        size = x[0].shape\n",
    "        mu = np.zeros(size)\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            mu = mu + x[i]\n",
    "        mu = mu / len(x)\n",
    "\n",
    "        sigma = np.zeros((size[0], size[0]))\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            sigma = sigma + np.dot((x[i] - mu).reshape(size[0], 1), (x[i] - mu).reshape(1, size[0]))\n",
    "        sigma = sigma / len(x)\n",
    "\n",
    "        return mu, sigma\n",
    "    \n",
    "    def pdf(self, x):\n",
    "        return np.exp(-1*0.5*np.dot(np.dot((x - self.mu).T, np.linalg.inv(self.sigma)), self.mu)) / np.sqrt(2 * np.pi * np.linalg.det(self.sigma))\n",
    "\n",
    "\n",
    "class kNN():\n",
    "    \"\"\"\n",
    "    k-Nearest Neighbors\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, k=5):\n",
    "        self.k = k\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict the class of X_test\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for i in range(len(X_test)):\n",
    "            distances = []\n",
    "            for j in range(len(self.X)):\n",
    "                distances.append(np.linalg.norm(X_test[i] - self.X[j]))\n",
    "            distances = np.array(distances)\n",
    "            idx = np.argsort(distances)\n",
    "            idx = idx[:self.k]\n",
    "            \n",
    "            idx_tmp = [self.y[i] for i in idx]\n",
    "            y_pred.append(max(idx_tmp, key=idx_tmp.count))\n",
    "            # y_pred.append(self.y[idx])\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "    \"\"\"\n",
    "    Logistic Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, lr=0.01, epochs=100):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.w = np.zeros(self.X.shape[1])\n",
    "        self.b = 0\n",
    "        self.costs = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def cost(self, X, y):\n",
    "        m = len(y)\n",
    "        z = np.dot(X, self.w) + self.b\n",
    "        h = self.sigmoid(z)\n",
    "        cost = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        return cost\n",
    "    \n",
    "    def fit(self):\n",
    "        for i in range(self.epochs):\n",
    "            z = np.dot(self.X, self.w) + self.b\n",
    "            h = self.sigmoid(z)\n",
    "            self.w = self.w - self.lr * np.dot(self.X.T, (h - self.y))\n",
    "            self.b = self.b - self.lr * np.sum(h - self.y)\n",
    "            self.costs.append(self.cost(self.X, self.y))\n",
    "        return self.w, self.b\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        z = np.dot(X_test, self.w) + self.b\n",
    "        h = self.sigmoid(z)\n",
    "        return h\n",
    "\n",
    "\n",
    "class NaiveBayes():\n",
    "    \"\"\"\n",
    "    Naive Bayes\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.classes = np.unique(y)\n",
    "        self.k = len(self.classes)\n",
    "        self.prior = np.zeros(self.k)\n",
    "        self.mu = np.zeros((self.k, X.shape[1]))\n",
    "        self.sigma = np.zeros((self.k, X.shape[1], X.shape[1]))\n",
    "        self.likelihood = np.zeros((self.k, X.shape[1]))\n",
    "    \n",
    "    def fit(self):\n",
    "        for i in range(self.k):\n",
    "            idx = np.where(self.y == self.classes[i])\n",
    "            self.prior[i] = len(idx[0]) / len(self.y)\n",
    "            self.mu[i] = np.mean(self.X[idx], axis=0)\n",
    "            self.sigma[i] = np.cov(self.X[idx].T)\n",
    "            self.likelihood[i] = self.normal(self.X, self.mu[i], self.sigma[i])\n",
    "    \n",
    "    def normal(self, X, mu, sigma):\n",
    "        k, d = X.shape\n",
    "        likelihood = np.zeros(k)\n",
    "        for i in range(k):\n",
    "            # likelihood[i] = np.exp(-0.5 * np.sum((X[i] - mu)**2 / sigma)) / (np.sqrt(2 * np.pi * sigma))\n",
    "            likelihood[i] = np.exp(-1*0.5*np.dot(np.dot((x - self.mu).T, np.linalg.inv(self.sigma)), self.mu)) / np.sqrt(2 * np.pi * np.linalg.det(self.sigma))\n",
    "        return likelihood\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for i in range(len(X_test)):\n",
    "            posterior = np.zeros(self.k)\n",
    "            for j in range(self.k):\n",
    "                posterior[j] = self.prior[j] * self.likelihood[j][i]\n",
    "            y_pred.append(self.classes[np.argmax(posterior)])\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class PolynomialRegression():\n",
    "    \"\"\"\n",
    "    Polynomial Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, degree=2):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.degree = degree\n",
    "        self.w = np.zeros((degree + 1, X.shape[1]))\n",
    "        self.b = 0\n",
    "        self.costs = []\n",
    "    \n",
    "    def fit(self):\n",
    "        for i in range(self.degree + 1):\n",
    "            self.w[i] = np.dot(np.linalg.pinv(self.X), self.y)\n",
    "        self.b = np.mean(self.y - np.dot(self.X, self.w))\n",
    "        return self.w, self.b\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for i in range(len(X_test)):\n",
    "            y_pred.append(np.dot(self.w, X_test[i]) + self.b)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class FLDClassifier():\n",
    "    \"\"\"\n",
    "    Fisher Linear Discriminant\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.classes = np.unique(y)\n",
    "        self.k = len(self.classes)\n",
    "        self.prior = np.zeros(self.k)\n",
    "        self.mean = np.zeros((self.k, X.shape[1]))\n",
    "        self.cov = np.zeros((self.k, X.shape[1], X.shape[1]))\n",
    "        self.w = np.zeros((self.k, X.shape[1]))\n",
    "    \n",
    "    def fit(self):\n",
    "        for i in range(self.k):\n",
    "            idx = np.where(self.y == self.classes[i])\n",
    "            self.prior[i] = len(idx[0]) / len(self.y)\n",
    "            self.mean[i] = np.mean(self.X[idx], axis=0)\n",
    "            self.cov[i] = np.cov(self.X[idx].T)\n",
    "        for i in range(self.k):\n",
    "            self.w[i] = np.dot(np.linalg.pinv(self.cov[i]), (self.mean[i] - self.mean.mean(axis=0)))\n",
    "        return self.w\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for i in range(len(X_test)):\n",
    "            posterior = np.zeros(self.k)\n",
    "            for j in range(self.k):\n",
    "                posterior[j] = self.prior[j] * np.exp(-0.5 * np.dot(np.dot((X_test[i] - self.mean[j]), np.linalg.pinv(self.cov[j])), (X_test[i] - self.mean[j]).T))\n",
    "            y_pred.append(self.classes[np.argmax(posterior)])\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class ParzenWindow():\n",
    "    \"\"\"\n",
    "    Parzen Window\n",
    "    \"\"\"\n",
    "    def __init__(self, X, window_function=\"gaussian\"):\n",
    "        self.X = X\n",
    "        # self.y = y\n",
    "    \n",
    "    def func_val_gaussian(self, x):\n",
    "        val = 0.0\n",
    "        for pts in self.X:\n",
    "            val += np.exp(-0.5 * np.dot(x-pts, (x-pts).T)) / len(self.X)*(np.sqrt(2 * np.pi))**pts.shape[0]\n",
    "        return val\n",
    "    \n",
    "    def posterior(self, x):\n",
    "        _posterior = self.func_val_gaussian(x)\n",
    "        return _posterior\n",
    "\n",
    "\n",
    "def parse_wrd_timestamps(wrd_path):\n",
    "    speaker_id = wrd_path.split('/')[-2]\n",
    "    sentence_id = wrd_path.split('/')[-1].replace('.WRD', '')\n",
    "    wrd_file = open(wrd_path)\n",
    "    content = wrd_file.read()\n",
    "    content = content.split('\\n')\n",
    "    content = [tuple(foo.split(' ') + [speaker_id, sentence_id]) for foo in content if foo != ''][1:-1]\n",
    "    wrd_file.close()\n",
    "    return content\n",
    "\n",
    "\n",
    "def read_audio(wave_path, verbose=False):\n",
    "    \"\"\"\n",
    "    Read Audio File\n",
    "    \"\"\"\n",
    "    rate, data = wavfile.read(wave_path)\n",
    "    assert rate == sample_rate\n",
    "    return data\n",
    "\n",
    "def extract_mfcc(x, sample_rate=44100, n_mfcc=40):\n",
    "    \"\"\"\n",
    "    Extract MFCC from audio\n",
    "    \"\"\"\n",
    "    mfcc = librosa.feature.mfcc(x, sr = sample_rate, n_mfcc=40)\n",
    "    return mfcc\n",
    "\n",
    "def align_data(data, words):\n",
    "    aligned = []\n",
    "    for tup in words:\n",
    "        # print(type(data), data.shape)\n",
    "        # print(tup[0], type(tup[0]))\n",
    "        start = int(tup[0])\n",
    "        end = int(tup[1])\n",
    "        word = tup[2]\n",
    "        aligned.append((data[start:end], word))\n",
    "    assert len(aligned) == len(words)\n",
    "    return aligned\n",
    "\n",
    "\n",
    "def parse_word_waves(time_aligned_words, audio_data):\n",
    "    return [align_data(data, words) for data, words in zip(audio_data, time_aligned_words)]\n",
    "\n",
    "\n",
    "def gaussian_prob(x, mu, sigma):\n",
    "    x_mu = x-mu\n",
    "    prob = np.exp(-0.5 * np.dot(np.dot(x_mu, np.linalg.inv(sigma)), x_mu.T)) #/ (np.sqrt(2 * np.pi * np.linalg.det(sigma))**x.shape[0])\n",
    "    # print(prob, np.dot(np.dot(x_mu, np.linalg.inv(sigma)), x_mu.T), np.linalg.det(sigma))\n",
    "    # 1/0\n",
    "    return prob\n",
    "\n",
    "def gmm_prob(X, Mu, Sigma, Weights):\n",
    "    n = Mu.shape[0]\n",
    "    # print(n)\n",
    "\n",
    "    probs = 0.0\n",
    "    for i in range(n):\n",
    "        probs += Weights[i] * gaussian_prob(X, Mu[i], Sigma[i])\n",
    "        # print(probs)\n",
    "    return probs\n",
    "\n",
    "\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"hi\")\n",
    "\n",
    "    training_dir = \"FDAW0\"\n",
    "    testing_dir = \"MKLW0\"\n",
    "\n",
    "\n",
    "\n",
    "    wav_train = glob.glob(dir_path+training_dir+\"/*.WAV.wav\")\n",
    "    wav_train = [ read_audio(wfile) for wfile in wav_train ]\n",
    "\n",
    "    wav_test = glob.glob(dir_path+testing_dir+\"/*.WAV.wav\")\n",
    "    wav_test = [ read_audio(wfile) for wfile in wav_test ]\n",
    "\n",
    "    wrd_train = [ foo.replace('.WAV.wav', '.PHN') for foo in glob.glob(dir_path+training_dir+\"/*.WAV.wav\")]\n",
    "    wrd_test = [ foo.replace('.WAV.wav', '.PHN') for foo in glob.glob(dir_path+testing_dir+\"/*.WAV.wav\")]\n",
    "\n",
    "\n",
    "    wrd_tuple_train = [parse_wrd_timestamps(wrd) for wrd in wrd_train]\n",
    "    wrd_tuple_test = [parse_wrd_timestamps(wrd) for wrd in wrd_test]\n",
    "\n",
    "    train_tuple = parse_word_waves(wrd_tuple_train, wav_train)\n",
    "    test_tuple = parse_word_waves(wrd_tuple_test, wav_test)\n",
    "\n",
    "    # mn_len = min( min(bar[0].shape[0] for bar in foo for foo in train_tuple), min(bar[0].shape[0] for bar in foo for foo in test_tuple))\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    mx_train, mn_train = 0, 1e10\n",
    "    mx_test, mn_test = 0, 1e10\n",
    "\n",
    "    for foo in train_tuple:\n",
    "        for bar in foo:\n",
    "            mx_train = max(mx_train, bar[0].shape[0])\n",
    "            mn_train = min(mn_train, bar[0].shape[0])\n",
    "    \n",
    "    for foo in test_tuple:\n",
    "        for bar in foo:\n",
    "            mx_test = max(mx_test, bar[0].shape[0])\n",
    "            mn_test = min(mn_test, bar[0].shape[0])\n",
    "    # print(mx_train, mn_train, \"\\n\", mx_test, mn_test)\n",
    "    mn_len = min(mn_test, mn_train)\n",
    "\n",
    "    for foo in train_tuple:\n",
    "        for bar in foo:\n",
    "            X_train.append(bar[0][:mn_len])\n",
    "            res = [ele for ele in vowels if(ele in bar[1])]\n",
    "            if bool(res):\n",
    "                y_train.append(1)\n",
    "            else:\n",
    "                y_train.append(0)\n",
    "                # print(0)\n",
    "    \n",
    "    for foo in test_tuple:\n",
    "        for bar in foo:\n",
    "            X_test.append(bar[0][:mn_len])\n",
    "            res = [ele for ele in vowels if(ele in bar[1])]\n",
    "            if bool(res):\n",
    "                y_test.append(1)\n",
    "            else:\n",
    "                y_test.append(0)\n",
    "\n",
    "    idx0 = [idx for idx, element in enumerate(y_train) if element==0]\n",
    "    idx1 = [idx for idx, element in enumerate(y_train) if element==1]\n",
    "\n",
    "    X_train0 = list(np.array(X_train)[idx0])\n",
    "    X_train1 = list(np.array(X_train)[idx1])\n",
    "\n",
    "    # print(len(X_train), len(X_train0), len(X_train1))\n",
    "\n",
    "    \"\"\"\n",
    "    Maximum Likelihood Estimation based Bayes Classifier\n",
    "    \"\"\"\n",
    "    print(\"\\n\",\"#\"*15, \"Maximum Likelihood Estimation based Bayes Classifier\", \"#\"*15, \"\\n\")\n",
    "\n",
    "    mle0 = MLE(X_train0)\n",
    "    mle1 = MLE(X_train1)\n",
    "\n",
    "    acc = 0\n",
    "    y_pred = []\n",
    "    \n",
    "\n",
    "    for x, y in zip(X_test, y_test):\n",
    "        posterior0 = mle0.pdf(x)\n",
    "        # print(posterior0)\n",
    "        posterior1 = mle1.pdf(x)\n",
    "        if posterior0 >= posterior1 and y == 0:\n",
    "            acc += 1\n",
    "            # y_pred.append(0)\n",
    "        if posterior0 <= posterior1 and y == 1:\n",
    "            acc += 1\n",
    "            # y_pred.append(1)\n",
    "        if posterior0 >= posterior1:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "\n",
    "    metrics = perf_measure(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy: \", acc/len(X_test))\n",
    "    print(\"True Positives: {} False Positives: {}\".format(metrics[0], metrics[1]))\n",
    "    print(\"True Negatives: {} False Negatives: {}\".format(metrics[2], metrics[3]))\n",
    "    print(\"Precision: \", metrics[0]/(metrics[0]+metrics[1]))\n",
    "    print(\"Recall: \", metrics[0]/(metrics[0]+metrics[2]))\n",
    "    # print()\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    "    \n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Actuals', fontsize=18)\n",
    "    plt.title('Confusion Matrix', fontsize=18)\n",
    "    plt.show()\n",
    "    fig.savefig('cm_mle.png')\n",
    "\n",
    "    \"\"\"\n",
    "    k-Nearest Neighbors Classifier\n",
    "    \"\"\"\n",
    "    print(\"\\n\",\"#\"*15, \"k-Nearest Neighbors Classifier\", \"#\"*15, \"\\n\")\n",
    "\n",
    "    for k in range(1,7):\n",
    "        knn = kNN(X_train, y_train, k)\n",
    "        \n",
    "        y_pred = knn.predict(X_test)\n",
    "\n",
    "        acc = 0\n",
    "        for predicted, actual in zip(y_pred, y_test):\n",
    "            if predicted == actual:\n",
    "                acc += 1\n",
    "        metrics = perf_measure(y_test, y_pred)\n",
    "\n",
    "        print(\"k = {} Accuracy: {}\".format(k, acc/len(y_test)))\n",
    "        print(\"True Positives: {} False Positives: {}\".format(metrics[0], metrics[1]))\n",
    "        print(\"True Negatives: {} False Negatives: {}\".format(metrics[2], metrics[3]))\n",
    "        print(\"Precision: \", metrics[0]/(metrics[0]+metrics[1]))\n",
    "        print(\"Recall: \", metrics[0]/(metrics[0]+metrics[2]), \"\\n\")\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "        ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    "        \n",
    "        plt.xlabel('Predictions', fontsize=18)\n",
    "        plt.ylabel('Actuals', fontsize=18)\n",
    "        plt.title('Confusion Matrix', fontsize=18)\n",
    "        plt.show()\n",
    "        fig.savefig('cm_knn_{}.png'.format(k))\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Parzen based Bayes Classifier\n",
    "    \"\"\"\n",
    "    print(\"\\n\",\"#\"*15, \"Parzen Window based Bayes Classifier\", \"#\"*15, \"\\n\")\n",
    "\n",
    "    parzen0 = ParzenWindow(X_train0)\n",
    "    parzen1 = ParzenWindow(X_train1)\n",
    "\n",
    "    prior0 = y_train.count(0) / len(y_train)\n",
    "    prior1 = y_train.count(1) / len(y_train)\n",
    "\n",
    "    acc = 0\n",
    "    y_pred = []\n",
    "\n",
    "    for x, y in zip(X_test, y_test):\n",
    "        posterior0 = parzen0.posterior(x)\n",
    "        posterior1 = parzen1.posterior(x)\n",
    "\n",
    "        if posterior0*prior0 >= posterior1*prior1:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "\n",
    "        if posterior0*prior0 >= posterior1*prior1 and y == 0:\n",
    "            acc += 1\n",
    "        if posterior0*prior0 <= posterior1*prior1 and y == 1:\n",
    "            acc += 1\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    metrics = perf_measure(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy: \", (metrics[0]+metrics[2])/(metrics[0]+metrics[2]+metrics[1]+metrics[3]))\n",
    "    print(\"True Positives: {} False Positives: {}\".format(metrics[0], metrics[1]))\n",
    "    print(\"True Negatives: {} False Negatives: {}\".format(metrics[2], metrics[3]))\n",
    "    print(\"Recall: \", metrics[0]/(metrics[0]+metrics[2]), \"\\n\")\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    "    \n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Actuals', fontsize=18)\n",
    "    plt.title('Confusion Matrix', fontsize=18)\n",
    "    plt.show()\n",
    "    fig.savefig('cm_parzen.png')\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Fischer Linear Discriminant Classifier\n",
    "    \"\"\"\n",
    "    print(\"\\n\",\"#\"*15, \"Fischer Linear Discriminant Classifier\", \"#\"*15, \"\\n\")\n",
    "    fldc = FLDClassifier(np.array(X_train), np.array(y_train))\n",
    "    fldc.fit()\n",
    "    y_pred = fldc.predict(np.array(X_test))\n",
    "\n",
    "    acc = 0\n",
    "    for predicted, actual in zip(y_pred, y_test):\n",
    "        if predicted == actual:\n",
    "            acc += 1\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    metrics = perf_measure(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy: \", acc/len(X_test))\n",
    "    print(\"True Positives: {} False Positives: {}\".format(metrics[0], metrics[1]))\n",
    "    print(\"True Negatives: {} False Negatives: {}\".format(metrics[2], metrics[3]))\n",
    "    print(\"Precision: \", metrics[0]/(metrics[0]+metrics[1]))\n",
    "    print(\"Recall: \", metrics[0]/(metrics[0]+metrics[2]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    "    \n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Actuals', fontsize=18)\n",
    "    plt.title('Confusion Matrix', fontsize=18)\n",
    "    plt.show()\n",
    "    fig.savefig('cm_flda.png')\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Gaussian Mixture Model based Bayes Classifier\n",
    "    \"\"\"\n",
    "    print(\"\\n\",\"#\"*15, \"Gaussian Mixture Model based Bayes Classifier\", \"#\"*15, \"\\n\")\n",
    "\n",
    "\n",
    "    gmm0 = GaussianMixture(n_components=2).fit(np.array(X_train0))\n",
    "    gmm1 = GaussianMixture(n_components=2).fit(np.array(X_train1))\n",
    "\n",
    "    mu0, sigma0, weights0 = gmm0.means_, gmm0.covariances_, gmm0.weights_\n",
    "    mu1, sigma1, weights1 = gmm1.means_, gmm1.covariances_, gmm1.weights_\n",
    "\n",
    "\n",
    "    acc = 0\n",
    "    y_pred = []\n",
    "\n",
    "    for x, y in zip(X_test, y_test):\n",
    "        posterior0 = gmm_prob(x, mu0, sigma0, weights0)\n",
    "        # print(posterior0)\n",
    "        posterior1 = gmm_prob(x, mu1, sigma1, weights1)\n",
    "\n",
    "        # y_pred = []\n",
    "        # print(posterior0.shape, posterior1.shape)\n",
    "        if posterior0 >= posterior1:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "\n",
    "        if posterior0 >= posterior1 and y == 0:\n",
    "            acc += 1\n",
    "        if posterior0 <= posterior1 and y == 1:\n",
    "            acc += 1\n",
    "        \n",
    "    metrics = perf_measure(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy: \", acc/len(X_test))\n",
    "    print(\"True Positives: {} False Positives: {}\".format(metrics[0], metrics[1]))\n",
    "    print(\"True Negatives: {} False Negatives: {}\".format(metrics[2], metrics[3]))\n",
    "    print(\"Recall: \", metrics[0]/(metrics[0]+metrics[2]))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    "    \n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Actuals', fontsize=18)\n",
    "    plt.title('Confusion Matrix', fontsize=18)\n",
    "    plt.show()\n",
    "    fig.savefig('cm_gmm.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314f9c6",
   "metadata": {},
   "source": [
    "# 2.5 Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c57494",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'IMagenet/tiny-imagenet-200/wnids.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31742/2238968277.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_id_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"train data shape: \"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31742/2238968277.py\u001b[0m in \u001b[0;36mget_id_dictionary\u001b[0;34m()\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_id_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0mid_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'wnids.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0mid_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mid_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'IMagenet/tiny-imagenet-200/wnids.txt'"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture._base import  BaseMixture, _check_shape\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.extmath import row_norms\n",
    "from scipy import linalg\n",
    "\n",
    "\n",
    "def _check_weights(weights, n_components):\n",
    "    weights = check_array(weights, dtype=[np.float64, np.float32], ensure_2d=False)\n",
    "    _check_shape(weights, (n_components,), \"weights\")\n",
    "\n",
    "    # check range\n",
    "    if any(np.less(weights, 0.0)) or any(np.greater(weights, 1.0)):\n",
    "        raise ValueError(\n",
    "            \"The parameter 'weights' should be in the range \"\n",
    "            \"[0, 1], but got max value %.5f, min value %.5f\"\n",
    "            % (np.min(weights), np.max(weights))\n",
    "        )\n",
    "\n",
    "    # check normalization\n",
    "    if not np.allclose(np.abs(1.0 - np.sum(weights)), 0.0):\n",
    "        raise ValueError(\n",
    "            \"The parameter 'weights' should be normalized, but got sum(weights) = %.5f\"\n",
    "            % np.sum(weights)\n",
    "        )\n",
    "    return weights\n",
    "\n",
    "\n",
    "def _check_means(means, n_components, n_features):\n",
    "\n",
    "    means = check_array(means, dtype=[np.float64, np.float32], ensure_2d=False)\n",
    "    _check_shape(means, (n_components, n_features), \"means\")\n",
    "    return means\n",
    "\n",
    "\n",
    "def _check_precision_positivity(precision, covariance_type):\n",
    "    if np.any(np.less_equal(precision, 0.0)):\n",
    "        raise ValueError(\"'%s precision' should be positive\" % covariance_type)\n",
    "\n",
    "\n",
    "def _check_precision_matrix(precision, covariance_type):\n",
    "    if not (\n",
    "        np.allclose(precision, precision.T) and np.all(linalg.eigvalsh(precision) > 0.0)\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"'%s precision' should be symmetric, positive-definite\" % covariance_type\n",
    "        )\n",
    "\n",
    "\n",
    "def _check_precisions_full(precisions, covariance_type):\n",
    "    for prec in precisions:\n",
    "        _check_precision_matrix(prec, covariance_type)\n",
    "\n",
    "\n",
    "def _check_precisions(precisions, covariance_type, n_components, n_features):\n",
    "    precisions = check_array(\n",
    "        precisions,\n",
    "        dtype=[np.float64, np.float32],\n",
    "        ensure_2d=False,\n",
    "        allow_nd=covariance_type == \"full\",\n",
    "    )\n",
    "\n",
    "    precisions_shape = {\n",
    "        \"full\": (n_components, n_features, n_features),\n",
    "        \"tied\": (n_features, n_features),\n",
    "        \"diag\": (n_components, n_features),\n",
    "        \"spherical\": (n_components,),\n",
    "    }\n",
    "    _check_shape(\n",
    "        precisions, precisions_shape[covariance_type], \"%s precision\" % covariance_type\n",
    "    )\n",
    "\n",
    "    _check_precisions = {\n",
    "        \"full\": _check_precisions_full,\n",
    "        \"tied\": _check_precision_matrix,\n",
    "        \"diag\": _check_precision_positivity,\n",
    "        \"spherical\": _check_precision_positivity,\n",
    "    }\n",
    "    _check_precisions[covariance_type](precisions, covariance_type)\n",
    "    return precisions\n",
    "\n",
    "\n",
    "def _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar):\n",
    "    n_components, n_features = means.shape\n",
    "    covariances = np.empty((n_components, n_features, n_features))\n",
    "    for k in range(n_components):\n",
    "        diff = X - means[k]\n",
    "        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]\n",
    "        covariances[k].flat[:: n_features + 1] += reg_covar\n",
    "    return covariances\n",
    "\n",
    "\n",
    "def _estimate_gaussian_covariances_tied(resp, X, nk, means, reg_covar):\n",
    "\n",
    "    avg_X2 = np.dot(X.T, X)\n",
    "    avg_means2 = np.dot(nk * means.T, means)\n",
    "    covariance = avg_X2 - avg_means2\n",
    "    covariance /= nk.sum()\n",
    "    covariance.flat[:: len(covariance) + 1] += reg_covar\n",
    "    return covariance\n",
    "\n",
    "\n",
    "def _estimate_gaussian_covariances_diag(resp, X, nk, means, reg_covar):\n",
    "    avg_X2 = np.dot(resp.T, X * X) / nk[:, np.newaxis]\n",
    "    avg_means2 = means ** 2\n",
    "    avg_X_means = means * np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "    return avg_X2 - 2 * avg_X_means + avg_means2 + reg_covar\n",
    "\n",
    "\n",
    "def _estimate_gaussian_covariances_spherical(resp, X, nk, means, reg_covar):\n",
    "    return _estimate_gaussian_covariances_diag(resp, X, nk, means, reg_covar).mean(1)\n",
    "\n",
    "\n",
    "def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type):\n",
    "    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    means = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "    covariances = {\n",
    "        \"full\": _estimate_gaussian_covariances_full,\n",
    "        \"tied\": _estimate_gaussian_covariances_tied,\n",
    "        \"diag\": _estimate_gaussian_covariances_diag,\n",
    "        \"spherical\": _estimate_gaussian_covariances_spherical,\n",
    "    }[covariance_type](resp, X, nk, means, reg_covar)\n",
    "    return nk, means, covariances\n",
    "\n",
    "\n",
    "def _compute_precision_cholesky(covariances, covariance_type):\n",
    "    estimate_precision_error_message = (\n",
    "        \"Fitting the mixture model failed because some components have \"\n",
    "        \"ill-defined empirical covariance (for instance caused by singleton \"\n",
    "        \"or collapsed samples). Try to decrease the number of components, \"\n",
    "        \"or increase reg_covar.\"\n",
    "    )\n",
    "\n",
    "    if covariance_type == \"full\":\n",
    "        n_components, n_features, _ = covariances.shape\n",
    "        precisions_chol = np.empty((n_components, n_features, n_features))\n",
    "        for k, covariance in enumerate(covariances):\n",
    "            try:\n",
    "                cov_chol = linalg.cholesky(covariance, lower=True)\n",
    "            except linalg.LinAlgError:\n",
    "                raise ValueError(estimate_precision_error_message)\n",
    "            precisions_chol[k] = linalg.solve_triangular(\n",
    "                cov_chol, np.eye(n_features), lower=True\n",
    "            ).T\n",
    "    elif covariance_type == \"tied\":\n",
    "        _, n_features = covariances.shape\n",
    "        try:\n",
    "            cov_chol = linalg.cholesky(covariances, lower=True)\n",
    "        except linalg.LinAlgError:\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol = linalg.solve_triangular(\n",
    "            cov_chol, np.eye(n_features), lower=True\n",
    "        ).T\n",
    "    else:\n",
    "        if np.any(np.less_equal(covariances, 0.0)):\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol = 1.0 / np.sqrt(covariances)\n",
    "    return precisions_chol\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Gaussian mixture probability estimators\n",
    "def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):\n",
    "    if covariance_type == \"full\":\n",
    "        n_components, _, _ = matrix_chol.shape\n",
    "        log_det_chol = np.sum(\n",
    "            np.log(matrix_chol.reshape(n_components, -1)[:, :: n_features + 1]), 1\n",
    "        )\n",
    "\n",
    "    elif covariance_type == \"tied\":\n",
    "        log_det_chol = np.sum(np.log(np.diag(matrix_chol)))\n",
    "\n",
    "    elif covariance_type == \"diag\":\n",
    "        log_det_chol = np.sum(np.log(matrix_chol), axis=1)\n",
    "\n",
    "    else:\n",
    "        log_det_chol = n_features * (np.log(matrix_chol))\n",
    "\n",
    "    return log_det_chol\n",
    "\n",
    "\n",
    "def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components, _ = means.shape\n",
    "    log_det = _compute_log_det_cholesky(precisions_chol, covariance_type, n_features)\n",
    "\n",
    "    if covariance_type == \"full\":\n",
    "        log_prob = np.empty((n_samples, n_components))\n",
    "        for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):\n",
    "            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n",
    "            log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "\n",
    "    elif covariance_type == \"tied\":\n",
    "        log_prob = np.empty((n_samples, n_components))\n",
    "        for k, mu in enumerate(means):\n",
    "            y = np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)\n",
    "            log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "\n",
    "    elif covariance_type == \"diag\":\n",
    "        precisions = precisions_chol ** 2\n",
    "        log_prob = (\n",
    "            np.sum((means ** 2 * precisions), 1)\n",
    "            - 2.0 * np.dot(X, (means * precisions).T)\n",
    "            + np.dot(X ** 2, precisions.T)\n",
    "        )\n",
    "\n",
    "    elif covariance_type == \"spherical\":\n",
    "        precisions = precisions_chol ** 2\n",
    "        log_prob = (\n",
    "            np.sum(means ** 2, 1) * precisions\n",
    "            - 2 * np.dot(X, means.T * precisions)\n",
    "            + np.outer(row_norms(X, squared=True), precisions)\n",
    "        )\n",
    "    return -0.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n",
    "\n",
    "\n",
    "class GaussianMixture(BaseMixture):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_components=1,\n",
    "        *,\n",
    "        covariance_type=\"full\",\n",
    "        tol=1e-3,\n",
    "        reg_covar=1e-6,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        init_params=\"kmeans\",\n",
    "        weights_init=None,\n",
    "        means_init=None,\n",
    "        precisions_init=None,\n",
    "        random_state=None,\n",
    "        warm_start=False,\n",
    "        verbose=0,\n",
    "        verbose_interval=10,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_components=n_components,\n",
    "            tol=tol,\n",
    "            reg_covar=reg_covar,\n",
    "            max_iter=max_iter,\n",
    "            n_init=n_init,\n",
    "            init_params=init_params,\n",
    "            random_state=random_state,\n",
    "            warm_start=warm_start,\n",
    "            verbose=verbose,\n",
    "            verbose_interval=verbose_interval,\n",
    "        )\n",
    "\n",
    "        self.covariance_type = covariance_type\n",
    "        self.weights_init = weights_init\n",
    "        self.means_init = means_init\n",
    "        self.precisions_init = precisions_init\n",
    "\n",
    "    def _check_parameters(self, X):\n",
    "        _, n_features = X.shape\n",
    "        if self.covariance_type not in [\"spherical\", \"tied\", \"diag\", \"full\"]:\n",
    "            raise ValueError(\n",
    "                \"Invalid value for 'covariance_type': %s \"\n",
    "                \"'covariance_type' should be in \"\n",
    "                \"['spherical', 'tied', 'diag', 'full']\"\n",
    "                % self.covariance_type\n",
    "            )\n",
    "\n",
    "        if self.weights_init is not None:\n",
    "            self.weights_init = _check_weights(self.weights_init, self.n_components)\n",
    "\n",
    "        if self.means_init is not None:\n",
    "            self.means_init = _check_means(\n",
    "                self.means_init, self.n_components, n_features\n",
    "            )\n",
    "\n",
    "        if self.precisions_init is not None:\n",
    "            self.precisions_init = _check_precisions(\n",
    "                self.precisions_init,\n",
    "                self.covariance_type,\n",
    "                self.n_components,\n",
    "                n_features,\n",
    "            )\n",
    "\n",
    "    def _initialize(self, X, resp):\n",
    "        n_samples, _ = X.shape\n",
    "\n",
    "        weights, means, covariances = _estimate_gaussian_parameters(\n",
    "            X, resp, self.reg_covar, self.covariance_type\n",
    "        )\n",
    "        weights /= n_samples\n",
    "\n",
    "        self.weights_ = weights if self.weights_init is None else self.weights_init\n",
    "        self.means_ = means if self.means_init is None else self.means_init\n",
    "\n",
    "        if self.precisions_init is None:\n",
    "            self.covariances_ = covariances\n",
    "            self.precisions_cholesky_ = _compute_precision_cholesky(\n",
    "                covariances, self.covariance_type\n",
    "            )\n",
    "        elif self.covariance_type == \"full\":\n",
    "            self.precisions_cholesky_ = np.array(\n",
    "                [\n",
    "                    linalg.cholesky(prec_init, lower=True)\n",
    "                    for prec_init in self.precisions_init\n",
    "                ]\n",
    "            )\n",
    "        elif self.covariance_type == \"tied\":\n",
    "            self.precisions_cholesky_ = linalg.cholesky(\n",
    "                self.precisions_init, lower=True\n",
    "            )\n",
    "        else:\n",
    "            self.precisions_cholesky_ = np.sqrt(self.precisions_init)\n",
    "\n",
    "    def _m_step(self, X, log_resp):\n",
    "        n_samples, _ = X.shape\n",
    "        self.weights_, self.means_, self.covariances_ = _estimate_gaussian_parameters(\n",
    "            X, np.exp(log_resp), self.reg_covar, self.covariance_type\n",
    "        )\n",
    "        self.weights_ /= n_samples\n",
    "        self.precisions_cholesky_ = _compute_precision_cholesky(\n",
    "            self.covariances_, self.covariance_type\n",
    "        )\n",
    "\n",
    "    def _estimate_log_prob(self, X):\n",
    "        return _estimate_log_gaussian_prob(\n",
    "            X, self.means_, self.precisions_cholesky_, self.covariance_type\n",
    "        )\n",
    "\n",
    "    def _estimate_log_weights(self):\n",
    "        return np.log(self.weights_)\n",
    "\n",
    "    def _compute_lower_bound(self, _, log_prob_norm):\n",
    "        return log_prob_norm\n",
    "\n",
    "    def _get_parameters(self):\n",
    "        return (\n",
    "            self.weights_,\n",
    "            self.means_,\n",
    "            self.covariances_,\n",
    "            self.precisions_cholesky_,\n",
    "        )\n",
    "\n",
    "    def _set_parameters(self, params):\n",
    "        (\n",
    "            self.weights_,\n",
    "            self.means_,\n",
    "            self.covariances_,\n",
    "            self.precisions_cholesky_,\n",
    "        ) = params\n",
    "\n",
    "        # Attributes computation\n",
    "        _, n_features = self.means_.shape\n",
    "\n",
    "        if self.covariance_type == \"full\":\n",
    "            self.precisions_ = np.empty(self.precisions_cholesky_.shape)\n",
    "            for k, prec_chol in enumerate(self.precisions_cholesky_):\n",
    "                self.precisions_[k] = np.dot(prec_chol, prec_chol.T)\n",
    "\n",
    "        elif self.covariance_type == \"tied\":\n",
    "            self.precisions_ = np.dot(\n",
    "                self.precisions_cholesky_, self.precisions_cholesky_.T\n",
    "            )\n",
    "        else:\n",
    "            self.precisions_ = self.precisions_cholesky_ ** 2\n",
    "\n",
    "    def _n_parameters(self):\n",
    "        _, n_features = self.means_.shape\n",
    "        if self.covariance_type == \"full\":\n",
    "            cov_params = self.n_components * n_features * (n_features + 1) / 2.0\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            cov_params = self.n_components * n_features\n",
    "        elif self.covariance_type == \"tied\":\n",
    "            cov_params = n_features * (n_features + 1) / 2.0\n",
    "        elif self.covariance_type == \"spherical\":\n",
    "            cov_params = self.n_components\n",
    "        mean_params = n_features * self.n_components\n",
    "        return int(cov_params + mean_params + self.n_components - 1)\n",
    "\n",
    "    def bic(self, X):\n",
    "        return -2 * self.score(X) * X.shape[0] + self._n_parameters() * np.log(\n",
    "            X.shape[0]\n",
    "        )\n",
    "\n",
    "    def aic(self, X):\n",
    "        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "import time\n",
    "import scipy.ndimage as nd\n",
    "import numpy as np\n",
    "import imageio as im\n",
    "from matplotlib.pyplot import imread as im2\n",
    "import cv2\n",
    "\n",
    "path = 'IMagenet/tiny-imagenet-200/'\n",
    "\n",
    "def get_id_dictionary():\n",
    "    id_dict = {}\n",
    "    for i, line in enumerate(open( path + 'wnids.txt', 'r')):\n",
    "        id_dict[line.replace('\\n', '')] = i\n",
    "    return id_dict\n",
    "  \n",
    "def get_class_to_id_dict():\n",
    "    id_dict = get_id_dictionary()\n",
    "    all_classes = {}\n",
    "    result = {}\n",
    "    for i, line in enumerate(open( path + 'words.txt', 'r')):\n",
    "        n_id, word = line.split('\\t')[:2]\n",
    "        all_classes[n_id] = word\n",
    "    for key, value in id_dict.items():\n",
    "        result[value] = (key, all_classes[key])      \n",
    "    return result\n",
    "\n",
    "def get_data(id_dict):\n",
    "    print('starting loading data')\n",
    "    train_data, test_data = [], []\n",
    "    train_labels, test_labels = [], []\n",
    "    t = time.time()\n",
    "    for key, value in id_dict.items():\n",
    "        train_data += [cv2.imread( path + 'train/{}/images/{}_{}.JPEG'.format(key, key, str(i))) for i in range(500)]\n",
    "        train_labels_ = np.array([[0]*200]*500)\n",
    "        train_labels_[:, value] = 1\n",
    "        train_labels += train_labels_.tolist()\n",
    "\n",
    "    for line in open( path + 'val/val_annotations.txt'):\n",
    "        img_name, class_id = line.split('\\t')[:2]\n",
    "        test_data.append(cv2.imread( path + 'val/images/{}'.format(img_name)))\n",
    "        test_labels_ = np.array([[0]*200])\n",
    "        test_labels_[0, id_dict[class_id]] = 1\n",
    "        test_labels += test_labels_.tolist()\n",
    "        \n",
    "    print('finished loading data, in {} seconds'.format(time.time() - t))\n",
    "    return np.array(train_data), np.array(train_labels), np.array(test_data), np.array(test_labels)\n",
    "  \n",
    "train_data, train_labels, test_data, test_labels = get_data(get_id_dictionary())\n",
    "\n",
    "print( \"train data shape: \",  train_data.shape )\n",
    "print( \"train label shape: \", train_labels.shape )\n",
    "print( \"test data shape: \",   test_data.shape )\n",
    "print( \"test_labels.shape: \", test_labels.shape )\n",
    "\n",
    "def rez_down(x):\n",
    "    gen_img=[]\n",
    "    for i in range(len(x)):\n",
    "        gen_img.append(np.resize(x[i],(8,8,3)))\n",
    "    return np.array(gen_img)\n",
    "\n",
    "def rez_up(x):\n",
    "    gen_img=[]\n",
    "    for i in range(len(x)):\n",
    "        gen_img.append(np.resize(x[i],(75,75,3)))\n",
    "    return np.array(gen_img)\n",
    "\n",
    "\n",
    "def flatter(x):\n",
    "    train_final=[]\n",
    "    for j in range(len(train)):\n",
    "        train_final.append(train[j].flatten())\n",
    "    return np.array(train_final)\n",
    "\n",
    "\n",
    "def sampler(N):\n",
    "    generated_data=[]\n",
    "    generated_labels=[]\n",
    "    for i in range(N):\n",
    "        j=random.randint(0,len(gmm.weights_)-1)\n",
    "        generated_labels.append(j+1)\n",
    "        generated_data.append(np.random.multivariate_normal(gmm.means_[j],gmm.covariances_[j]))\n",
    "    return (np.array(generated_data),np.array(generated_labels))\n",
    "\n",
    "\n",
    "gen_data,gen_class_labels=sampler(1000)\n",
    "\n",
    "\n",
    "data=rez_up(gen_data)\n",
    "\n",
    "\n",
    "\n",
    "# example of calculating the frechet inception distance\n",
    "import numpy\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy.random import random\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def calculate_fid(model,images1, images2):\n",
    "\t# calculate activations\n",
    "\tact1 = model.predict(images1)\n",
    "\tact2 = model.predict(images2)\n",
    "\t# calculate mean and covariance statistics\n",
    "\tmu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "\tmu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "\t# calculate sum squared difference between means\n",
    "\tssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
    "\t# calculate sqrt of product between cov\n",
    "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
    "\t# check and correct imaginary numbers from sqrt\n",
    "\tif iscomplexobj(covmean):\n",
    "\t\tcovmean = covmean.real\n",
    "\t# calculate score\n",
    "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\treturn fid\n",
    "\n",
    "score=calculate_fid(model,data,test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91592fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
