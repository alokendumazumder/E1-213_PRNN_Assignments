{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820ab3a2",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc9e7b",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaac9de",
   "metadata": {},
   "source": [
    "#### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df25db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load \n",
    "import numpy as np\n",
    "data = load('/home/alok/pneumoniamnist.npz') #put penmonia instead of bloodmnist, rest remians same\n",
    "lst = data.files \n",
    "Xtrain = data[lst[0]] \n",
    "Xtest = data[lst[1]] \n",
    "Xval = data[lst[2]] \n",
    "ytrain = data[lst[3]] \n",
    "ytest= data[lst[4]] \n",
    "yval = data[lst[5]] \n",
    "print(Xtrain.shape) \n",
    "print(Xtest.shape) \n",
    "print(Xval.shape) \n",
    "print(ytrain.shape) \n",
    "print(ytest.shape) \n",
    "print(yval.shape)\n",
    "\n",
    "X=[]\n",
    "for i in range(len(Xtrain)):\n",
    "    X.append(Xtrain[i].flatten())\n",
    "X=np.array(X)\n",
    "X=X/255\n",
    "\n",
    "Xt=[]\n",
    "for i in range(len(Xtest)):\n",
    "    Xt.append(Xtest[i].flatten())\n",
    "Xt=np.array(Xt)\n",
    "Xt=Xt/255\n",
    "\n",
    "y=[]\n",
    "for i in range(len(ytrain)):\n",
    "    y.append(ytrain[i][0])\n",
    "y=np.array(y)\n",
    "\n",
    "yt=[]\n",
    "for i in range(len(ytest)):\n",
    "    yt.append(ytest[i][0])\n",
    "yt=np.array(yt)\n",
    "\n",
    "\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_ = np.logspace(-9, 3, 13)\n",
    "degree_=[3,4,5,6,7,8,9]\n",
    "param_grid = dict(degree=degree_, C=C_range,kernel=['poly'])\n",
    "#param_grid = {'C': np.logspace(-2, 10, 13),'gamma': np.logspace(-9,3,13),'kernel': ['rbf']}\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X,y)\n",
    "\n",
    "print(\n",
    "    \"The best parameters are %s with a score of %0.2f\"\n",
    "    % (grid.best_params_, grid.best_score_)\n",
    ")\n",
    "\n",
    "scores = grid.cv_results_[\"mean_test_score\"].reshape(len(C_range), len(degree_))\n",
    "\n",
    "print(\"Accuracy on test data is\"+ \" \" + str(np.mean(grid.predict(Xt)==yt)*100)+\" \"+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc8eaa",
   "metadata": {},
   "source": [
    "#### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1140ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from numpy import load \n",
    "import numpy as np\n",
    "data = load('/home/alok/pneumoniamnist.npz') #put penmonia instead of bloodmnist, rest remians same\n",
    "lst = data.files \n",
    "Xtrain = data[lst[0]] \n",
    "Xtest = data[lst[1]] \n",
    "Xval = data[lst[2]] \n",
    "ytrain = data[lst[3]] \n",
    "ytest= data[lst[4]] \n",
    "yval = data[lst[5]] \n",
    "print(Xtrain.shape) \n",
    "print(Xtest.shape) \n",
    "print(Xval.shape) \n",
    "print(ytrain.shape) \n",
    "print(ytest.shape) \n",
    "print(yval.shape)\n",
    "\n",
    "X=[]\n",
    "for i in range(len(Xtrain)):\n",
    "    X.append(Xtrain[i].flatten())\n",
    "X=np.array(X)\n",
    "X=X/255\n",
    "\n",
    "Xt=[]\n",
    "for i in range(len(Xtest)):\n",
    "    Xt.append(Xtest[i].flatten())\n",
    "Xt=np.array(Xt)\n",
    "Xt=Xt/255\n",
    "\n",
    "y=[]\n",
    "for i in range(len(ytrain)):\n",
    "    y.append(ytrain[i][0])\n",
    "y=np.array(y)\n",
    "\n",
    "\n",
    "yt=[]\n",
    "for i in range(len(ytest)):\n",
    "    yt.append(ytest[i][0])\n",
    "yt=np.array(yt)\n",
    "\n",
    "\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\n",
    "    \"The best parameters are %s with a score of %0.2f\"\n",
    "    % (grid.best_params_, grid.best_score_)\n",
    ")\n",
    "\n",
    "scores = grid.cv_results_[\"mean_test_score\"].reshape(len(C_range), len(gamma_range))\n",
    "\n",
    "\n",
    "print(\"Accuracy on test data is\"+ \" \" + str(np.mean(grid.predict(Xt)==yt)*100)+\" \"+ '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4dd4e",
   "metadata": {},
   "source": [
    "#### Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from numpy import load \n",
    "import numpy as np\n",
    "data = load('/home/alok/pneumoniamnist.npz') #put penmonia instead of bloodmnist, rest remians same\n",
    "lst = data.files \n",
    "Xtrain = data[lst[0]] \n",
    "Xtest = data[lst[1]] \n",
    "Xval = data[lst[2]] \n",
    "ytrain = data[lst[3]] \n",
    "ytest= data[lst[4]] \n",
    "yval = data[lst[5]] \n",
    "print(Xtrain.shape) \n",
    "print(Xtest.shape) \n",
    "print(Xval.shape) \n",
    "print(ytrain.shape) \n",
    "print(ytest.shape) \n",
    "print(yval.shape)\n",
    "\n",
    "\n",
    "X=[]\n",
    "for i in range(len(Xtrain)):\n",
    "    X.append(Xtrain[i].flatten())\n",
    "X=np.array(X)\n",
    "X=X/255\n",
    "\n",
    "Xt=[]\n",
    "for i in range(len(Xtest)):\n",
    "    Xt.append(Xtest[i].flatten())\n",
    "Xt=np.array(Xt)\n",
    "Xt=Xt/255\n",
    "\n",
    "yt=[]\n",
    "for i in range(len(ytest)):\n",
    "    yt.append(ytest[i][0])\n",
    "yt=np.array(yt)\n",
    "\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_ = np.logspace(-9, 3, 13)\n",
    "#degree_=[3,4,5,6,7,8,9]\n",
    "param_grid = dict(gamma=gamma_, C=C_range,kernel=['poly'])\n",
    "#param_grid = {'C': np.logspace(-2, 10, 13),'gamma': np.logspace(-9,3,13),'kernel': ['rbf']}\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X,y)\n",
    "\n",
    "print(\n",
    "    \"The best parameters are %s with a score of %0.2f\"\n",
    "    % (grid.best_params_, grid.best_score_)\n",
    ")\n",
    "\n",
    "\n",
    "scores = grid.cv_results_[\"mean_test_score\"].reshape(len(C_range), len(gamma_))\n",
    "\n",
    "print(\"Accuracy on test data is\"+ \" \" + str(np.mean(grid.predict(Xt)==yt)*100)+\" \"+ '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee343f",
   "metadata": {},
   "source": [
    "## Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a04873",
   "metadata": {},
   "source": [
    "#### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fddfba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from numpy import load \n",
    "import numpy as np\n",
    "data = load('/home/alok/bloodmnist.npz') #put penmonia instead of bloodmnist, rest remians same\n",
    "lst = data.files \n",
    "Xtrain = data[lst[0]] \n",
    "ytrain = data[lst[1]] \n",
    "Xval = data[lst[2]] \n",
    "yval = data[lst[3]] \n",
    "Xtest= data[lst[4]] \n",
    "ytest = data[lst[5]] \n",
    "print(Xtrain.shape) \n",
    "print(Xtest.shape) \n",
    "print(Xval.shape) \n",
    "print(ytrain.shape) \n",
    "print(ytest.shape) \n",
    "print(yval.shape)\n",
    "\n",
    "X=[]\n",
    "for i in range(len(Xtrain)):\n",
    "    X.append(Xtrain[i].flatten())\n",
    "X=np.array(X)\n",
    "X=X/255\n",
    "\n",
    "Xt=[]\n",
    "for i in range(len(Xtest)):\n",
    "    Xt.append(Xtest[i].flatten())\n",
    "Xt=np.array(Xt)\n",
    "Xt=Xt/255\n",
    "\n",
    "y=[]\n",
    "for i in range(len(ytrain)):\n",
    "    y.append(ytrain[i][0])\n",
    "y=np.array(y)\n",
    "\n",
    "\n",
    "yt=[]\n",
    "for i in range(len(ytest)):\n",
    "    yt.append(ytest[i][0])\n",
    "yt=np.array(yt)\n",
    "\n",
    "\n",
    "C_range = np.logspace(-2, 2, 6)\n",
    "gamma_ = np.logspace(-3, 3, 7)\n",
    "degree_=[3,4,5,6,7,8,9]\n",
    "param_grid = dict(degree=degree_, C=C_range,kernel=['poly'])\n",
    "#param_grid = {'C': np.logspace(-2, 10, 13),'gamma': np.logspace(-9,3,13),'kernel': ['rbf']}\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X,y)\n",
    "\n",
    "print(\n",
    "    \"The best parameters are %s with a score of %0.2f\"\n",
    "    % (grid.best_params_, grid.best_score_)\n",
    ")\n",
    "\n",
    "\n",
    "scores = grid.cv_results_[\"mean_test_score\"].reshape(len(C_range), len(degree_))\n",
    "\n",
    "print(\"Accuracy on test data is\"+ \" \" + str(np.mean(grid.predict(Xt)==yt)*100)+\" \"+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ce33f",
   "metadata": {},
   "source": [
    "#### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8dfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from numpy import load \n",
    "import numpy as np\n",
    "data = load('/home/alok/bloodmnist.npz') #put penmonia instead of bloodmnist, rest remians same\n",
    "lst = data.files \n",
    "Xtrain = data[lst[0]] \n",
    "ytrain = data[lst[1]] \n",
    "Xval = data[lst[2]] \n",
    "yval = data[lst[3]] \n",
    "Xtest= data[lst[4]] \n",
    "ytest = data[lst[5]] \n",
    "print(Xtrain.shape) \n",
    "print(Xtest.shape) \n",
    "print(Xval.shape) \n",
    "print(ytrain.shape) \n",
    "print(ytest.shape) \n",
    "print(yval.shape)\n",
    "\n",
    "X=[]\n",
    "for i in range(len(Xtrain)):\n",
    "    X.append(Xtrain[i].flatten())\n",
    "X=np.array(X)\n",
    "X=X/255\n",
    "\n",
    "y=[]\n",
    "for i in range(len(ytrain)):\n",
    "    y.append(ytrain[i][0])\n",
    "y=np.array(y)\n",
    "\n",
    "\n",
    "Xt=[]\n",
    "for i in range(len(Xtest)):\n",
    "    Xt.append(Xtest[i].flatten())\n",
    "Xt=np.array(Xt)\n",
    "Xt=Xt/255\n",
    "\n",
    "yt=[]\n",
    "for i in range(len(ytest)):\n",
    "    yt.append(ytest[i][0])\n",
    "yt=np.array(yt)\n",
    "\n",
    "\n",
    "C_range = np.logspace(-2, 2, 6)\n",
    "gamma_range = np.logspace(-3, 3, 7)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\n",
    "    \"The best parameters are %s with a score of %0.2f\"\n",
    "    % (grid.best_params_, grid.best_score_)\n",
    ")\n",
    "\n",
    "scores = grid.cv_results_[\"mean_test_score\"].reshape(len(C_range), len(gamma_range))\n",
    "\n",
    "\n",
    "print(\"Accuracy on test data is\"+ \" \" + str(np.mean(grid.predict(Xt)==yt)*100)+\" \"+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c00b42",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac70c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Multi-Class Classification\n",
    "\n",
    "# %% [markdown]\n",
    "# # Library Imports and Data Handling\n",
    "\n",
    "# %%\n",
    "from numpy import load \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "data = load('bloodmnist.npz') #put penmonia instead of bloodmnist, rest remians same\n",
    "lst = data.files \n",
    "Xtrain = data[lst[0]] \n",
    "Xtest = data[lst[1]] \n",
    "Xval = data[lst[2]] \n",
    "ytrain = data[lst[3]] \n",
    "ytest = data[lst[4]] \n",
    "yval = data[lst[5]]\n",
    "########\n",
    "Xtrain = data[lst[0]]\n",
    "ytrain = data[lst[1]]\n",
    "Xval = data[lst[2]]\n",
    "yval = data[lst[3]]\n",
    "Xtest = data[lst[4]]\n",
    "ytest = data[lst[5]]\n",
    "\n",
    "print(Xtrain.shape) \n",
    "print(Xtest.shape) \n",
    "print(Xval.shape) \n",
    "print(ytrain.shape) \n",
    "print(ytest.shape) \n",
    "print(yval.shape)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "y=[]\n",
    "for i in range(len(ytrain)):\n",
    "    y.append(ytrain[i][0])\n",
    "y_train=np.array(y)\n",
    "print(y_train.shape)\n",
    "y_train\n",
    "\n",
    "# %%\n",
    "y_=[]\n",
    "for i in range(len(ytest)):\n",
    "    y_.append(ytest[i][0])\n",
    "y_test=np.array(y_)\n",
    "print(y_test.shape)\n",
    "y_test\n",
    "\n",
    "# %%\n",
    "x=[]\n",
    "for i in range(len(Xtrain)):\n",
    "    x.append(Xtrain[i].flatten())\n",
    "x_train=np.array(x)\n",
    "\n",
    "x_=[]\n",
    "for i in range(len(Xtest)):\n",
    "    x_.append(Xtest[i].flatten())\n",
    "x_test=np.array(x_)\n",
    "print(x_test.shape)\n",
    "x_test    \n",
    "\n",
    "# %% [markdown]\n",
    "# # Backpropagation Without DropOut\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "#import datasets.mnist.loader as mnist\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    " \n",
    " \n",
    "class ANN:\n",
    "    def __init__(self, layers_size):\n",
    "        self.layers_size = layers_size\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)\n",
    "        self.n = 0\n",
    "        self.costs = []\n",
    " \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    " \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    " \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
    "                self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    " \n",
    "    def forward(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    " \n",
    "        return A, store\n",
    " \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    " \n",
    "    def backward(self, X, Y, lam,store):\n",
    " \n",
    "        derivatives = {}\n",
    " \n",
    "        store[\"A0\"] = X.T\n",
    " \n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dZ = A - Y.T\n",
    " \n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n +(lam/self.n)*store['W'+str(self.L)]\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    " \n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    " \n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)+(lam/self.n)*store['W'+str(l)]\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    " \n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    " \n",
    "        return derivatives\n",
    " \n",
    "    def fit(self, X, Y, lam,learning_rate=0.01, n_iterations=2500):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        self.n = X.shape[0]\n",
    " \n",
    "        self.layers_size.insert(0, X.shape[1])\n",
    " \n",
    "        self.initialize_parameters()\n",
    "        for loop in range(n_iterations):\n",
    "            A, store = self.forward(X)\n",
    "            cost = -np.mean(Y * np.log(A.T+ 1e-8))\n",
    "            derivatives = self.backward(X, Y, lam,store)\n",
    " \n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"dW\" + str(l)]\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"db\" + str(l)]\n",
    " \n",
    "            if loop % 100 == 0:\n",
    "                print(\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y))\n",
    " \n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    " \n",
    "    def predict(self, X, Y):\n",
    "        A, cache = self.forward(X)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        return accuracy * 100\n",
    " \n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()\n",
    " \n",
    " \n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    # Normalize\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    " \n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
    " \n",
    "    test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
    " \n",
    "    return train_x, train_y, test_x, test_y\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    \n",
    " \n",
    "    train_x, train_y, test_x, test_y = pre_process_data(x_train,y_train,x_test,y_test)\n",
    " \n",
    "    print(\"train_x's shape: \" + str(train_x.shape))\n",
    "    print(\"test_x's shape: \" + str(test_x.shape))\n",
    " \n",
    "    layers_dims = [50, 8]\n",
    " \n",
    "    ann = ANN(layers_dims)\n",
    "    ann.fit(train_x, train_y, lam=0,learning_rate=0.1, n_iterations=10000)\n",
    "    print(\"Train Accuracy:\", ann.predict(train_x, train_y))\n",
    "    print(\"Test Accuracy:\", ann.predict(test_x, test_y))\n",
    "    ann.plot_cost()\n",
    "\n",
    "# %% [markdown]\n",
    "# # Backpropagation With DropOut\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "#import datasets.mnist.loader as mnist\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn .preprocessing import OneHotEncoder\n",
    " \n",
    " \n",
    "class ANN:\n",
    "    def __init__(self, layers_size,p):\n",
    "        self.layers_size = layers_size\n",
    "        #print(layers_size)\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)\n",
    "        self.n = 0\n",
    "        self.p=p\n",
    "        self.costs = []\n",
    " \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    " \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    " \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    "        print(self.layers_size)\n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
    "                self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    "            #print(\"W\" + str(l)+\" shape \")\n",
    "            #print(self.parameters[\"W\" + str(l)].shape)\n",
    "            #print(\"b\" + str(l)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l)].shape)\n",
    "            \n",
    "            \n",
    " \n",
    "    def forward(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            #print(\"W\" + str(l + 1)+\"shape is\")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].shape)\n",
    "            #print(\"A \"+\"shape is\")\n",
    "            #print(A.shape)\n",
    "            #print(\"W\" + str(l+1)+\" dot A shape \")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].dot(A).shape)\n",
    "            #print(\"b\" + str(l+1)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l+1)].shape)\n",
    "            \n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            #print(\"Z shape \")\n",
    "            #print(Z.shape)\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    "            #print(\"Stored A,W,Z for index \"+str(l + 1))\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    "        #print(\"A shape is \")\n",
    "        #print(A.shape)\n",
    "        #print(\"Stored A,W,Z for index \"+str(self.L))\n",
    "        return A, store\n",
    "    \n",
    "    def forward2(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            #print(\"W\" + str(l + 1)+\"shape is\")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].shape)\n",
    "            #print(\"A \"+\"shape is\")\n",
    "            #print(A.shape)\n",
    "            #print(\"W\" + str(l+1)+\" dot A shape \")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].dot(A).shape)\n",
    "            #print(\"b\" + str(l+1)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l+1)].shape)\n",
    "            \n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            #print(\"Z shape \")\n",
    "            #print(Z.shape)\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]*np.ones(self.parameters[\"W\" + str(l + 1)].shape)*self.p\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    "            #print(\"Stored A,W,Z for index \"+str(l + 1))\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    "        #print(\"A shape is \")\n",
    "        #print(A.shape)\n",
    "        #print(\"Stored A,W,Z for index \"+str(self.L))\n",
    "        return A, store\n",
    " \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    "    \n",
    " \n",
    "    def backward(self, X, Y, lam,store):\n",
    " \n",
    "        derivatives = {}\n",
    " \n",
    "        store[\"A0\"] = X.T\n",
    " \n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dZ = A - Y.T\n",
    "        #print(\"self.n \",self.n)\n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n +(lam/self.n)*store['W'+str(self.L)]\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    "        \n",
    "        \n",
    "        #print(\"Shape of dZ is\",dZ.shape)\n",
    "        #print(\"Shape of dW is\",dW.shape)\n",
    "        #print(\"Shape of db is\",db.shape)\n",
    "        #print(\"Shape of dAPrev is\",dAPrev.shape)\n",
    " \n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    "        #print(\"shape of derivative of dW \"+str(self.L)+\" is \",derivatives[\"dW\" + str(self.L)].shape)\n",
    "        #print(\"shape of derivative of db \"+str(self.L)+\" is \",derivatives[\"db\" + str(self.L)].shape)\n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)+(lam/self.n)*store['W'+str(l)]\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    " \n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    "            #print(\"shape of derivative of dW \"+str(l)+\" is \",derivatives[\"dW\" + str(l)].shape)\n",
    "            #print(\"shape of derivative of db \"+str(l)+\" is \",derivatives[\"db\" + str(l)].shape)\n",
    "        \n",
    "        #print(\"derivatives shape is \")\n",
    "        #print(derivatives)\n",
    "        return derivatives\n",
    " \n",
    "    def fit(self, X, Y, lam,learning_rate=0.01, n_iterations=2500):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        self.n = X.shape[0]\n",
    " \n",
    "        self.layers_size.insert(0, X.shape[1])\n",
    " \n",
    "        self.initialize_parameters()\n",
    "        for loop in range(n_iterations):\n",
    "            A, store = self.forward(X)\n",
    "            cost = -np.mean(Y * np.log(A.T+ 1e-8))\n",
    "            #print(\"Y shape is \",Y.shape)\n",
    "            #print(\"np.log(A.T+ 1e-8) shape is \",np.log(A.T+ 1e-8).shape )\n",
    "            #print(\"(Y * np.log(A.T+ 1e-8)).shape is \",(Y * np.log(A.T+ 1e-8)).shape)\n",
    "            #print(\"cost\",cost)\n",
    "            derivatives = self.backward(X, Y, lam,store)\n",
    " \n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"dW\" + str(l)]*self.droputMatrix(self.p,derivatives[\n",
    "                    \"dW\" + str(l)])\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"db\" + str(l)]*self.droputMatrix(self.p,derivatives[\n",
    "                    \"db\" + str(l)])\n",
    " \n",
    "            if loop % 100 == 0:\n",
    "                print(\"Iteration: \", loop,\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y))\n",
    " \n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    " \n",
    "    def predict(self, X, Y):\n",
    "        A, cache = self.forward2(X)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        return accuracy * 100\n",
    " \n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()\n",
    "        \n",
    "    def droputMatrix(self,p,Mat):\n",
    "        noOfOnes=int(Mat.shape[0]*Mat.shape[1]*p)\n",
    "        noOfZeros=Mat.shape[0]*Mat.shape[1]-noOfOnes\n",
    "        ones=np.ones(noOfOnes)\n",
    "        zeroes=np.zeros(noOfZeros)\n",
    "        total=np.concatenate((ones,zeroes))\n",
    "        np.random.shuffle(total)\n",
    "        total=total.reshape((Mat.shape[0],Mat.shape[1]))\n",
    "        return total\n",
    "    \n",
    "        \n",
    "        \n",
    " \n",
    " \n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    # Normalize\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    " \n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
    " \n",
    "    test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
    " \n",
    "    return train_x, train_y, test_x, test_y\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    " \n",
    "    train_x, train_y, test_x, test_y = pre_process_data(x_train,y_train,x_test,y_test)\n",
    " \n",
    "    print(\"train_x's shape: \" + str(train_x.shape))\n",
    "    print(\"test_x's shape: \" + str(test_x.shape))\n",
    " \n",
    "    layers_dims = [50, 8]\n",
    " \n",
    "    ann = ANN(layers_dims,0.5)\n",
    "    ann.fit(train_x, train_y, lam=0,learning_rate=0.1, n_iterations=10000)\n",
    "    print(\"Train Accuracy:\", ann.predict(train_x, train_y))\n",
    "    print(\"Test Accuracy:\", ann.predict(test_x, test_y))\n",
    "    ann.plot_cost()\n",
    "\n",
    "# %% [markdown]\n",
    "# # Binary Classification\n",
    "\n",
    "# %% [markdown]\n",
    "# # Library Imports And Data Handling\n",
    "\n",
    "# %%\n",
    "from numpy import load \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "data = load('pneumoniamnist.npz') #put penmonia instead of bloodmnist, rest remians same\n",
    "lst = data.files \n",
    "Xtrain = data[lst[0]] \n",
    "Xtest = data[lst[1]] \n",
    "Xval = data[lst[2]] \n",
    "ytrain = data[lst[3]] \n",
    "ytest = data[lst[4]] \n",
    "yval = data[lst[5]] \n",
    "print(Xtrain.shape) \n",
    "print(Xtest.shape) \n",
    "print(Xval.shape) \n",
    "print(ytrain.shape) \n",
    "print(ytest.shape) \n",
    "print(yval.shape)\n",
    "\n",
    "\n",
    "# %%\n",
    "y=[]\n",
    "for i in range(len(ytrain)):\n",
    "    y.append(ytrain[i][0])\n",
    "y_train=np.array(y)\n",
    "    \n",
    "\n",
    "# %%\n",
    "y_=[]\n",
    "for i in range(len(ytest)):\n",
    "    y_.append(ytest[i][0])\n",
    "y_test=np.array(y_)\n",
    "\n",
    "# %%\n",
    "x=[]\n",
    "for i in range(len(Xtrain)):\n",
    "    x.append(Xtrain[i].flatten())\n",
    "x_train=np.array(x)\n",
    "\n",
    "x_=[]\n",
    "for i in range(len(Xtest)):\n",
    "    x_.append(Xtest[i].flatten())\n",
    "x_test=np.array(x_)\n",
    "    \n",
    "\n",
    "# %% [markdown]\n",
    "# # Backpropagation Without Dropout\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "#import datasets.mnist.loader as mnist\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    " \n",
    " \n",
    "class ANN:\n",
    "    def __init__(self, layers_size):\n",
    "        self.layers_size = layers_size\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)\n",
    "        self.n = 0\n",
    "        self.costs = []\n",
    " \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    " \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    " \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
    "                self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    " \n",
    "    def forward(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    " \n",
    "        return A, store\n",
    " \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    " \n",
    "    def backward(self, X, Y, lam,store):\n",
    " \n",
    "        derivatives = {}\n",
    " \n",
    "        store[\"A0\"] = X.T\n",
    " \n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dZ = A - Y.T\n",
    " \n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n +(lam/self.n)*store['W'+str(self.L)]\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    " \n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    " \n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)+(lam/self.n)*store['W'+str(l)]\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    " \n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    " \n",
    "        return derivatives\n",
    " \n",
    "    def fit(self, X, Y, lam,learning_rate=0.01, n_iterations=2500):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        self.n = X.shape[0]\n",
    " \n",
    "        self.layers_size.insert(0, X.shape[1])\n",
    " \n",
    "        self.initialize_parameters()\n",
    "        for loop in range(n_iterations):\n",
    "            A, store = self.forward(X)\n",
    "            cost = -np.mean(Y * np.log(A.T+ 1e-8))\n",
    "            derivatives = self.backward(X, Y, lam,store)\n",
    " \n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"dW\" + str(l)]\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"db\" + str(l)]\n",
    " \n",
    "            if loop % 100 == 0:\n",
    "                print(\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y))\n",
    " \n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    " \n",
    "    def predict(self, X, Y):\n",
    "        A, cache = self.forward(X)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        return accuracy * 100\n",
    " \n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()\n",
    " \n",
    " \n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    # Normalize\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    " \n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
    " \n",
    "    test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
    " \n",
    "    return train_x, train_y, test_x, test_y\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    \n",
    " \n",
    "    train_x, train_y, test_x, test_y = pre_process_data(x_train,y_train,x_test,y_test)\n",
    " \n",
    "    print(\"train_x's shape: \" + str(train_x.shape))\n",
    "    print(\"test_x's shape: \" + str(test_x.shape))\n",
    " \n",
    "    layers_dims = [50, 2]\n",
    " \n",
    "    ann = ANN(layers_dims)\n",
    "    ann.fit(train_x, train_y, lam=0,learning_rate=0.1, n_iterations=10000)\n",
    "    print(\"Train Accuracy:\", ann.predict(train_x, train_y))\n",
    "    print(\"Test Accuracy:\", ann.predict(test_x, test_y))\n",
    "    ann.plot_cost()\n",
    "\n",
    "# %% [markdown]\n",
    "# # Backpropagation with Dropout\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "#import datasets.mnist.loader as mnist\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn .preprocessing import OneHotEncoder\n",
    " \n",
    " \n",
    "class ANN:\n",
    "    def __init__(self, layers_size,p):\n",
    "        self.layers_size = layers_size\n",
    "        #print(layers_size)\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)\n",
    "        self.n = 0\n",
    "        self.p=p\n",
    "        self.costs = []\n",
    " \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    " \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    " \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    "        print(self.layers_size)\n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
    "                self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    "            #print(\"W\" + str(l)+\" shape \")\n",
    "            #print(self.parameters[\"W\" + str(l)].shape)\n",
    "            #print(\"b\" + str(l)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l)].shape)\n",
    "            \n",
    "            \n",
    " \n",
    "    def forward(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            #print(\"W\" + str(l + 1)+\"shape is\")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].shape)\n",
    "            #print(\"A \"+\"shape is\")\n",
    "            #print(A.shape)\n",
    "            #print(\"W\" + str(l+1)+\" dot A shape \")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].dot(A).shape)\n",
    "            #print(\"b\" + str(l+1)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l+1)].shape)\n",
    "            \n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            #print(\"Z shape \")\n",
    "            #print(Z.shape)\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    "            #print(\"Stored A,W,Z for index \"+str(l + 1))\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    "        #print(\"A shape is \")\n",
    "        #print(A.shape)\n",
    "        #print(\"Stored A,W,Z for index \"+str(self.L))\n",
    "        return A, store\n",
    "    \n",
    "    def forward2(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            #print(\"W\" + str(l + 1)+\"shape is\")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].shape)\n",
    "            #print(\"A \"+\"shape is\")\n",
    "            #print(A.shape)\n",
    "            #print(\"W\" + str(l+1)+\" dot A shape \")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].dot(A).shape)\n",
    "            #print(\"b\" + str(l+1)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l+1)].shape)\n",
    "            \n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            #print(\"Z shape \")\n",
    "            #print(Z.shape)\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]*np.ones(self.parameters[\"W\" + str(l + 1)].shape)*self.p\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    "            #print(\"Stored A,W,Z for index \"+str(l + 1))\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    "        #print(\"A shape is \")\n",
    "        #print(A.shape)\n",
    "        #print(\"Stored A,W,Z for index \"+str(self.L))\n",
    "        return A, store\n",
    " \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    "    \n",
    " \n",
    "    def backward(self, X, Y, lam,store):\n",
    " \n",
    "        derivatives = {}\n",
    " \n",
    "        store[\"A0\"] = X.T\n",
    " \n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dZ = A - Y.T\n",
    "        #print(\"self.n \",self.n)\n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n +(lam/self.n)*store['W'+str(self.L)]\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    "        \n",
    "        \n",
    "        #print(\"Shape of dZ is\",dZ.shape)\n",
    "        #print(\"Shape of dW is\",dW.shape)\n",
    "        #print(\"Shape of db is\",db.shape)\n",
    "        #print(\"Shape of dAPrev is\",dAPrev.shape)\n",
    " \n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    "        #print(\"shape of derivative of dW \"+str(self.L)+\" is \",derivatives[\"dW\" + str(self.L)].shape)\n",
    "        #print(\"shape of derivative of db \"+str(self.L)+\" is \",derivatives[\"db\" + str(self.L)].shape)\n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)+(lam/self.n)*store['W'+str(l)]\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    " \n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    "            #print(\"shape of derivative of dW \"+str(l)+\" is \",derivatives[\"dW\" + str(l)].shape)\n",
    "            #print(\"shape of derivative of db \"+str(l)+\" is \",derivatives[\"db\" + str(l)].shape)\n",
    "        \n",
    "        #print(\"derivatives shape is \")\n",
    "        #print(derivatives)\n",
    "        return derivatives\n",
    " \n",
    "    def fit(self, X, Y, lam,learning_rate=0.01, n_iterations=2500):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        self.n = X.shape[0]\n",
    " \n",
    "        self.layers_size.insert(0, X.shape[1])\n",
    " \n",
    "        self.initialize_parameters()\n",
    "        for loop in range(n_iterations):\n",
    "            A, store = self.forward(X)\n",
    "            cost = -np.mean(Y * np.log(A.T+ 1e-8))\n",
    "            #print(\"Y shape is \",Y.shape)\n",
    "            #print(\"np.log(A.T+ 1e-8) shape is \",np.log(A.T+ 1e-8).shape )\n",
    "            #print(\"(Y * np.log(A.T+ 1e-8)).shape is \",(Y * np.log(A.T+ 1e-8)).shape)\n",
    "            #print(\"cost\",cost)\n",
    "            derivatives = self.backward(X, Y, lam,store)\n",
    " \n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"dW\" + str(l)]*self.droputMatrix(self.p,derivatives[\n",
    "                    \"dW\" + str(l)])\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"db\" + str(l)]*self.droputMatrix(self.p,derivatives[\n",
    "                    \"db\" + str(l)])\n",
    " \n",
    "            if loop % 100 == 0:\n",
    "                print(\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y))\n",
    " \n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    " \n",
    "    def predict(self, X, Y):\n",
    "        A, cache = self.forward2(X)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        return accuracy * 100\n",
    " \n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()\n",
    "        \n",
    "    def droputMatrix(self,p,Mat):\n",
    "        noOfOnes=int(Mat.shape[0]*Mat.shape[1]*p)\n",
    "        noOfZeros=Mat.shape[0]*Mat.shape[1]-noOfOnes\n",
    "        ones=np.ones(noOfOnes)\n",
    "        zeroes=np.zeros(noOfZeros)\n",
    "        total=np.concatenate((ones,zeroes))\n",
    "        np.random.shuffle(total)\n",
    "        total=total.reshape((Mat.shape[0],Mat.shape[1]))\n",
    "        return total\n",
    "    \n",
    "        \n",
    "        \n",
    " \n",
    " \n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    # Normalize\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    " \n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
    " \n",
    "    test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
    " \n",
    "    return train_x.reshape(len(train_x),28*28), train_y, test_x.reshape(len(test_x),28*28), test_y\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    \n",
    " \n",
    "    train_x, train_y, test_x, test_y = pre_process_data(Xtrain,ytrain,Xtest,ytest)\n",
    "    #print(train_y)\n",
    "    #print(\"train_x's shape: \" + str(train_x.shape))\n",
    "    #print(\"test_x's shape: \" + str(test_x.shape))\n",
    "    #print(\"train_y's shape: \" + str(train_y.shape))\n",
    "    #print(\"test_y's shape: \" + str(test_y.shape))\n",
    " \n",
    "    layers_dims = [50, 25, 2]\n",
    " \n",
    "    ann = ANN(layers_dims,0.4)\n",
    "    ann.fit(train_x, train_y, lam=0,learning_rate=0.1, n_iterations=10000)\n",
    "    print(\"Train Accuracy:\", ann.predict(train_x, train_y))\n",
    "    print(\"Test Accuracy:\", ann.predict(test_x, test_y))\n",
    "    ann.plot_cost()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86593e",
   "metadata": {},
   "source": [
    "# Fischer Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a275373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as tt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "data_flag = 'pneumoniamnist'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "data_transform = tt.Compose([tt.Resize((227,227)),\n",
    "                             tt.RandomHorizontalFlip(),\n",
    "                             tt.ToTensor(),\n",
    "                             tt.Normalize(mean=[0.485],\n",
    "                                          std=[0.229] ,inplace=True)\n",
    "                                           ])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "val_dataset = DataClass(split='val', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# %%\n",
    "y = torch.tensor([])\n",
    "X = torch.tensor([])\n",
    "for inputs, targets in train_loader:\n",
    "    targets = targets.squeeze().long()\n",
    "    targets = targets.float().resize_(len(targets), 1)\n",
    "    y = torch.cat((y, targets), 0)\n",
    "    X = torch.cat((X, inputs), 0)\n",
    "y = y.numpy()\n",
    "X = X.detach().numpy()\n",
    "\n",
    "# %%\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "model = LDA()\n",
    "X = X.reshape(4708, 227*227)\n",
    "y = y.reshape(4708,)\n",
    "model.fit(X, y)\n",
    "\n",
    "# %%\n",
    "y_test = torch.tensor([])\n",
    "X_test = torch.tensor([])\n",
    "for inputs, targets in test_loader:\n",
    "    targets = targets.squeeze().long()\n",
    "    targets = targets.float().resize_(len(targets), 1)\n",
    "    y_test = torch.cat((y_test, targets), 0)\n",
    "    X_test = torch.cat((X_test, inputs), 0)\n",
    "y_test = y_test.numpy()\n",
    "X_test = X_test.detach().numpy()\n",
    "\n",
    "# %%\n",
    "def MeanSquaredError(ydata,ypredict):\n",
    "    return np.sqrt(np.mean(np.square(ydata-ypredict)))\n",
    "\n",
    "def ReturnConfMatrix(ypredict,ytest,N=2):\n",
    "    Conf = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            a = np.where(ypredict==i,1,0)\n",
    "            b = np.where(ytest==j,1,0)\n",
    "            c = a*b\n",
    "            Conf[i,j] = np.sum(c)\n",
    "    return Conf\n",
    "\n",
    "def EvaluateAccuracy(ypredict,ytest,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    I = np.identity(N)\n",
    "    Diag = Conf*I\n",
    "    return np.sum(Diag)/max(np.sum(Conf),0.001)\n",
    "\n",
    "def EvaluatePrecision(ypredict,ytest,i,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    return max(Conf[i,i],0.001)/max(np.sum(Conf[i,:]),0.001)\n",
    "\n",
    "def EvaluateRecall(ypredict,ytest,i,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    return max(Conf[i,i],0.001)/max(np.sum(Conf[:,i]),0.001)\n",
    "\n",
    "def EvaluateF1score(ypredict,ytest,i,N=2):\n",
    "    p = EvaluatePrecision(ypredict,ytest,i,N)\n",
    "    r = EvaluateRecall(ypredict,ytest,i,N)\n",
    "    return 2*r*p/(r+p)\n",
    "\n",
    "def EvaluateFalsePositiveRate(ypredict,ytest,i,N=2):\n",
    "    Conf = ReturnConfMatrix(ypredict,ytest,N)\n",
    "    t1=max(np.sum(Conf[i,:]),0.001)\n",
    "    t2=t1-Conf[i,i]\n",
    "    t3=max(t2,0.001)\n",
    "    return t3/max(np.sum(Conf[i,:]),0.001)\n",
    "\n",
    "def EvaluateAUC(ypredict,ytest,i,N=2):\n",
    "    return EvaluatePrecision(ypredict,ytest,i,N)/EvaluateFalsePositiveRate(ypredict,ytest,i,N)\n",
    "\n",
    "def EvaluateCategoricalCrossEntropy(ypredict,ytest):\n",
    "    return np.abs(np.sum(np.log(ypredict)@ytest.transpose()))\n",
    "\n",
    "def tptnfpfn(ypredict,ytest,N=2):  # Function to calculate True/False Positives/Negatives\n",
    "    Conf = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            a = np.where(ypredict==i,1,0)\n",
    "            b = np.where(ytest==j,1,0)\n",
    "            c = a*b\n",
    "            Conf[i,j] = np.sum(c)\n",
    "    tp=np.zeros(N)\n",
    "    tn=np.zeros(N)\n",
    "    fp=np.zeros(N)\n",
    "    fn=np.zeros(N)\n",
    "    for i in range(N):\n",
    "        tp[i]=Conf[i,i]\n",
    "    ConfSum=np.sum(Conf)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            fp[i]+=Conf[j,i]\n",
    "            fn[i]+=Conf[i,j]\n",
    "        temp1=fp[i]\n",
    "        temp2=fn[i]\n",
    "        fp[i]-=Conf[i,i]\n",
    "        fn[i]-=Conf[i,i]\n",
    "        tn[i]=ConfSum-temp1-temp2+Conf[i,i]\n",
    "\n",
    "    return tp,tn,fp,fn \n",
    "\n",
    "def plot(errors,iter,title=''):\n",
    "    plt.title(title)\n",
    "    plt.plot(range(iter),errors)\n",
    "    plt.xticks(np.linspace(start=0,stop=iter,num=11))\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('MeanSquaredError')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "def print_performance_metrics(y_predict, y_test, N):\n",
    "    num = len(y_predict)\n",
    "    y_predict=np.reshape(y_predict,(num,1))\n",
    "    y_test=np.reshape(y_test,(num,1))\n",
    "    \n",
    "    print('Accuracy',EvaluateAccuracy(y_predict,y_test,N))\n",
    "    \n",
    "    for i in range(N):\n",
    "        f1 = EvaluatePrecision(y_predict,y_test,i,N)\n",
    "        print(f'Precision : {i} = {f1:.3f}')\n",
    "\n",
    "    for i in range(N):\n",
    "        f1 = EvaluateRecall(y_predict,y_test,i,N)\n",
    "        print(f'Recall : {i} = {f1:.3f}')\n",
    "\n",
    "    for i in range(N):\n",
    "        f1 = EvaluateF1score(y_predict,y_test,i,N)\n",
    "        print(f'F1 Score : {i} = {f1:.3f}')\n",
    "\n",
    "    for i in range(N):\n",
    "        f1 = EvaluateAUC(y_predict,y_test,i,N)\n",
    "        print(f'AUC : {i} = {f1:.3f}')\n",
    "\n",
    "    print('Confusion Matrix',ReturnConfMatrix(y_predict,y_test,N))\n",
    "\n",
    "    tp,tn,fp,fn = tptnfpfn(y_predict,y_test,N)\n",
    "    print('True Positives',tp)\n",
    "    print('True Negatives',tn)\n",
    "    print('False Positives',fp)\n",
    "    print('False Negatives',fn)\n",
    "\n",
    "# %%\n",
    "class OneHotEncoder:\n",
    "    def __init__(self,classes=2,type='none'):\n",
    "        self.classes = classes\n",
    "        self.type = type\n",
    "        pass\n",
    "\n",
    "    def transform(self,X:np.ndarray):\n",
    "        self.classes = int(np.max(X)+1)\n",
    "        if self.type=='none':\n",
    "            Y = -1*np.ones((len(X),self.classes))\n",
    "        else:\n",
    "            Y = np.zeros((len(X),self.classes))\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            Y[i,int(X[i])] = 1\n",
    "        return Y\n",
    "\n",
    "    def inverse_transform(self,X:np.ndarray):\n",
    "        Y = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            Y[i] = 0\n",
    "            mx = X[i,0]\n",
    "            for j in range(self.classes):\n",
    "                if X[i,j] > mx:\n",
    "                    Y[i] = j\n",
    "                    mx = X[i,j] \n",
    "        return Y\n",
    "\n",
    "# %%\n",
    "X_test = X_test.reshape(624,227*227)\n",
    "y_pred = model.predict(X_test)\n",
    "print_performance_metrics(y_pred, y_test, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b868ab5",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d77592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Note: In this implementation, we assume the input is a 2d numpy array for simplicity, because that's\n",
    "how our MNIST images are stored. This works for us because we use it as the first layer in our\n",
    "network, but most CNNs have many more Conv layers. If we were building a bigger network that needed\n",
    "to use Conv3x3 multiple times, we'd have to make the input be a 3d numpy array.\n",
    "'''\n",
    "\n",
    "class Conv3x3:\n",
    "  # A Convolution layer using 3x3 filters.\n",
    "\n",
    "  def __init__(self, num_filters):\n",
    "    self.num_filters = num_filters\n",
    "\n",
    "    # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
    "    # We divide by 9 to reduce the variance of our initial values\n",
    "    self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
    "\n",
    "  def iterate_regions(self, image):\n",
    "    '''\n",
    "    Generates all possible 3x3 image regions using valid padding.\n",
    "    - image is a 2d numpy array.\n",
    "    '''\n",
    "    h, w = image.shape\n",
    "\n",
    "    for i in range(h - 2):\n",
    "      for j in range(w - 2):\n",
    "        im_region = image[i:(i + 3), j:(j + 3)]\n",
    "        yield im_region, i, j\n",
    "\n",
    "  def forward(self, input):\n",
    "    '''\n",
    "    Performs a forward pass of the conv layer using the given input.\n",
    "    Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
    "    - input is a 2d numpy array\n",
    "    '''\n",
    "    self.last_input = input\n",
    "    # print(input.shape)\n",
    "\n",
    "    h, w = input.shape\n",
    "    output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "\n",
    "    for im_region, i, j in self.iterate_regions(input):\n",
    "      output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "\n",
    "    return output\n",
    "\n",
    "  def backprop(self, d_L_d_out, learn_rate):\n",
    "    '''\n",
    "    Performs a backward pass of the conv layer.\n",
    "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "    - learn_rate is a float.\n",
    "    '''\n",
    "    d_L_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "    for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "      for f in range(self.num_filters):\n",
    "        d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
    "\n",
    "    # Update filters\n",
    "    self.filters -= learn_rate * d_L_d_filters\n",
    "\n",
    "    # We aren't returning anything here since we use Conv3x3 as the first layer in our CNN.\n",
    "    # Otherwise, we'd need to return the loss gradient for this layer's inputs, just like every\n",
    "    # other layer in our CNN.\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MaxPool2:\n",
    "  # A Max Pooling layer using a pool size of 2.\n",
    "\n",
    "  def iterate_regions(self, image):\n",
    "    '''\n",
    "    Generates non-overlapping 2x2 image regions to pool over.\n",
    "    - image is a 2d numpy array\n",
    "    '''\n",
    "    h, w, _ = image.shape\n",
    "    new_h = h // 2\n",
    "    new_w = w // 2\n",
    "\n",
    "    for i in range(new_h):\n",
    "      for j in range(new_w):\n",
    "        im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "        yield im_region, i, j\n",
    "\n",
    "  def forward(self, input):\n",
    "    '''\n",
    "    Performs a forward pass of the maxpool layer using the given input.\n",
    "    Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
    "    - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "    '''\n",
    "    self.last_input = input\n",
    "\n",
    "    h, w, num_filters = input.shape\n",
    "    output = np.zeros((h // 2, w // 2, num_filters))\n",
    "\n",
    "    for im_region, i, j in self.iterate_regions(input):\n",
    "      output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "    return output\n",
    "\n",
    "  def backprop(self, d_L_d_out):\n",
    "    '''\n",
    "    Performs a backward pass of the maxpool layer.\n",
    "    Returns the loss gradient for this layer's inputs.\n",
    "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "    '''\n",
    "    d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "    for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "      h, w, f = im_region.shape\n",
    "      amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "      for i2 in range(h):\n",
    "        for j2 in range(w):\n",
    "          for f2 in range(f):\n",
    "            # If this pixel was the max value, copy the gradient to it.\n",
    "            if im_region[i2, j2, f2] == amax[f2]:\n",
    "              d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "    return d_L_d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39904439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Softmax:\n",
    "  # A standard fully-connected layer with softmax activation.\n",
    "\n",
    "  def __init__(self, input_len, nodes):\n",
    "    # We divide by input_len to reduce the variance of our initial values\n",
    "    self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "    self.biases = np.zeros(nodes)\n",
    "\n",
    "  def forward(self, input):\n",
    "    '''\n",
    "    Performs a forward pass of the softmax layer using the given input.\n",
    "    Returns a 1d numpy array containing the respective probability values.\n",
    "    - input can be any array with any dimensions.\n",
    "    '''\n",
    "    self.last_input_shape = input.shape\n",
    "\n",
    "    input = input.flatten()\n",
    "    self.last_input = input\n",
    "\n",
    "    input_len, nodes = self.weights.shape\n",
    "\n",
    "    totals = np.dot(input, self.weights) + self.biases\n",
    "    self.last_totals = totals\n",
    "\n",
    "    exp = np.exp(totals)\n",
    "    return exp / np.sum(exp, axis=0)\n",
    "\n",
    "  def backprop(self, d_L_d_out, learn_rate):\n",
    "    '''\n",
    "    Performs a backward pass of the softmax layer.\n",
    "    Returns the loss gradient for this layer's inputs.\n",
    "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "    - learn_rate is a float.\n",
    "    '''\n",
    "    # We know only 1 element of d_L_d_out will be nonzero\n",
    "    for i, gradient in enumerate(d_L_d_out):\n",
    "      if gradient == 0:\n",
    "        continue\n",
    "\n",
    "      # e^totals\n",
    "      t_exp = np.exp(self.last_totals)\n",
    "\n",
    "      # Sum of all e^totals\n",
    "      S = np.sum(t_exp)\n",
    "\n",
    "      # Gradients of out[i] against totals\n",
    "      d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
    "      d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
    "\n",
    "      # Gradients of totals against weights/biases/input\n",
    "      d_t_d_w = self.last_input\n",
    "      d_t_d_b = 1\n",
    "      d_t_d_inputs = self.weights\n",
    "\n",
    "      # Gradients of loss against totals\n",
    "      d_L_d_t = gradient * d_out_d_t\n",
    "\n",
    "      # Gradients of loss against weights/biases/input\n",
    "      d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
    "      d_L_d_b = d_L_d_t * d_t_d_b\n",
    "      d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
    "\n",
    "      # Update weights / biases\n",
    "      self.weights -= learn_rate * d_L_d_w\n",
    "      self.biases -= learn_rate * d_L_d_b\n",
    "\n",
    "      return d_L_d_inputs.reshape(self.last_input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f585a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We only use the first 1k examples of each set in the interest of time.\n",
    "# Feel free to change this if you want.\n",
    "# train_images = mnist.train_images()[:1000]\n",
    "# train_labels = mnist.train_labels()[:1000]\n",
    "# test_images = mnist.test_images()[:1000]\n",
    "# test_labels = mnist.test_labels()[:1000]\n",
    "\n",
    "conv1 = Conv3x3(8)                  \n",
    "pool1 = MaxPool2()                  \n",
    "\n",
    "conv2 = Conv3x3(8)                  \n",
    "pool2 = MaxPool2()                  \n",
    "\n",
    "conv = Conv3x3(8)                  \n",
    "pool = MaxPool2()                  \n",
    "softmax = Softmax(13 * 13 * 8, 10) \n",
    "\n",
    "def forward(image, label):\n",
    "  '''\n",
    "  Completes a forward pass of the CNN and calculates the accuracy and\n",
    "  cross-entropy loss.\n",
    "  - image is a 2d numpy array\n",
    "  - label is a digit\n",
    "  '''\n",
    "  # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
    "  # to work with. This is standard practice.\n",
    "  out = conv.forward((image / 255) - 0.5)\n",
    "  out = pool.forward(out)\n",
    "  out = softmax.forward(out)\n",
    "\n",
    "  # Calculate cross-entropy loss and accuracy. np.log() is the natural log.\n",
    "  loss = -np.log(out[label])\n",
    "  acc = 1 if np.argmax(out) == label else 0\n",
    "\n",
    "  return out, loss, acc\n",
    "\n",
    "def train(im, label, lr=.005):\n",
    "  '''\n",
    "  Completes a full training step on the given image and label.\n",
    "  Returns the cross-entropy loss and accuracy.\n",
    "  - image is a 2d numpy array\n",
    "  - label is a digit\n",
    "  - lr is the learning rate\n",
    "  '''\n",
    "  # Forward\n",
    "  out, loss, acc = forward(im, label)\n",
    "\n",
    "  # Calculate initial gradient\n",
    "  gradient = np.zeros(10)\n",
    "  gradient[label] = -1 / out[label]\n",
    "\n",
    "  # Backprop\n",
    "  gradient = softmax.backprop(gradient, lr)\n",
    "  gradient = pool.backprop(gradient)\n",
    "  gradient = conv.backprop(gradient, lr)\n",
    "\n",
    "  return loss, acc\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return gray\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  data = np.load('/home/ece/Piyush/Coursework/PRNN/Assignment 2/bloodmnist.npz')\n",
    "  # data = np.load('/home/ece/Piyush/Coursework/PRNN/Assignment 2/pneumoniamnist.npz')\n",
    "  # print(list(data.keys()))\n",
    "  train_images, train_labels = data['train_images'], data['train_labels']\n",
    "  test_images, test_labels = data['test_images'], data['test_labels']\n",
    "  # x_val, y_val = data['val_images'], data['val_labels']\n",
    "\n",
    "  train_images = [rgb2gray(train_images[i,:,:]) for i in range(train_images.shape[0])]\n",
    "  train_labels = [train_labels[i] for i in range(train_labels.shape[0])]\n",
    "\n",
    "  test_images = [rgb2gray(test_images[i,:,:]) for i in range(test_images.shape[0])]\n",
    "  test_labels = [test_labels[i] for i in range(test_labels.shape[0])]\n",
    "\n",
    "  # print(train_labels[0])\n",
    "  # a = [test_labels[i] for i in range(test_labels.shape[0])]\n",
    "  # print(train_images[0].shape)\n",
    "  # 1/0\n",
    "\n",
    "  loss_list = []\n",
    "  acc_list = []\n",
    "\n",
    "  print('MNIST CNN initialized!')\n",
    "\n",
    "  # Train the CNN for 3 epochs\n",
    "  for epoch in range(1):\n",
    "    print('--- Epoch %d ---' % (epoch + 1))\n",
    "\n",
    "    # Shuffle the training data\n",
    "    permutation = np.random.permutation(len(train_images))\n",
    "    # print(permutation)\n",
    "    # train_images = train_images[permutation.astype(int)]\n",
    "    # train_labels = train_labels[permutation]\n",
    "\n",
    "    # Train!\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "    # _loss = 0\n",
    "    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
    "      if i % 100 == 99:\n",
    "        print(\n",
    "          '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
    "          (i + 1, loss / 100, num_correct)\n",
    "        )\n",
    "        loss = 0\n",
    "        num_correct = 0\n",
    "\n",
    "      l, acc = train(im, label)\n",
    "\n",
    "      loss += l\n",
    "      num_correct += acc\n",
    "\n",
    "      loss_list.append(l)\n",
    "      acc_list.append(acc)\n",
    "\n",
    "\n",
    "  plt.plot(loss_list)\n",
    "  plt.grid()\n",
    "  plt.title('Loss')\n",
    "  plt.xlabel('Iteration')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.savefig('loss_scratch.png')\n",
    "  plt.close()\n",
    "\n",
    "  plt.plot(acc_list)\n",
    "  plt.grid()\n",
    "  plt.title('Accuracy')\n",
    "  plt.xlabel('Iteration')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.savefig('acc_scratch.png')\n",
    "  plt.close()\n",
    "\n",
    "  # Test the CNN\n",
    "  print('\\n--- Testing the CNN ---')\n",
    "  loss = 0\n",
    "  num_correct = 0\n",
    "  for im, label in zip(test_images, test_labels):\n",
    "    _, l, acc = forward(im, label)\n",
    "    loss += l\n",
    "    num_correct += acc\n",
    "\n",
    "  num_tests = len(test_images)\n",
    "  print('Test Loss:', loss / num_tests)\n",
    "  print('Test Accuracy:', num_correct / num_tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efbaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca72b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8f79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc1768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d533c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
