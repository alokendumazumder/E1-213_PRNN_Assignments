{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59377a18",
   "metadata": {},
   "source": [
    "# Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871e8fa",
   "metadata": {},
   "source": [
    "# Library Imports and Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb5ba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11959, 28, 28, 3)\n",
      "(3421, 28, 28, 3)\n",
      "(1712, 28, 28, 3)\n",
      "(11959, 1)\n",
      "(3421, 1)\n",
      "(1712, 1)\n"
     ]
    }
   ],
   "source": [
    "from numpy import load \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "data = load('bloodmnist.npz') #put penmonia instead of bloodmnist, rest remians same\n",
    "lst = data.files \n",
    "Xtrain = data[lst[0]] \n",
    "Xtest = data[lst[1]] \n",
    "Xval = data[lst[2]] \n",
    "ytrain = data[lst[3]] \n",
    "ytest = data[lst[4]] \n",
    "yval = data[lst[5]]\n",
    "########\n",
    "Xtrain = data[lst[0]]\n",
    "ytrain = data[lst[1]]\n",
    "Xval = data[lst[2]]\n",
    "yval = data[lst[3]]\n",
    "Xtest = data[lst[4]]\n",
    "ytest = data[lst[5]]\n",
    "\n",
    "print(Xtrain.shape) \n",
    "print(Xtest.shape) \n",
    "print(Xval.shape) \n",
    "print(ytrain.shape) \n",
    "print(ytest.shape) \n",
    "print(yval.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41194089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784d68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1a365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c77cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11959,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 3, 6, ..., 1, 6, 4], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=[]\n",
    "for i in range(len(ytrain)):\n",
    "    y.append(ytrain[i][0])\n",
    "y_train=np.array(y)\n",
    "print(y_train.shape)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75680f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3421,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 0, 1, ..., 3, 1, 7], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_=[]\n",
    "for i in range(len(ytest)):\n",
    "    y_.append(ytest[i][0])\n",
    "y_test=np.array(y_)\n",
    "print(y_test.shape)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff7cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3421, 2352)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[232, 200, 179, ..., 254, 228, 203],\n",
       "       [239, 219, 184, ..., 178, 151, 158],\n",
       "       [249, 214, 194, ..., 215, 181, 169],\n",
       "       ...,\n",
       "       [176, 133, 160, ..., 181, 142, 145],\n",
       "       [225, 189, 175, ..., 254, 226, 202],\n",
       "       [255, 229, 188, ..., 188, 150, 149]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[]\n",
    "for i in range(len(Xtrain)):\n",
    "    x.append(Xtrain[i].flatten())\n",
    "x_train=np.array(x)\n",
    "\n",
    "x_=[]\n",
    "for i in range(len(Xtest)):\n",
    "    x_.append(Xtest[i].flatten())\n",
    "x_test=np.array(x_)\n",
    "print(x_test.shape)\n",
    "x_test    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad1262",
   "metadata": {},
   "source": [
    "# Backpropagation Without DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a85787d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (11959, 2352)\n",
      "test_x's shape: (3421, 2352)\n",
      "Cost:  0.27723833745142346 Train Accuracy: 19.48323438414583\n",
      "Cost:  0.18992805092818013 Train Accuracy: 46.8015720377958\n",
      "Cost:  0.16337054957673922 Train Accuracy: 55.26381804498703\n",
      "Cost:  0.15113996788033188 Train Accuracy: 58.76745547286563\n",
      "Cost:  0.1434003687007003 Train Accuracy: 60.15553139894641\n",
      "Cost:  0.13614928231715248 Train Accuracy: 62.864788025754656\n",
      "Cost:  0.13715801569561395 Train Accuracy: 59.0099506647713\n",
      "Cost:  0.1273946244940133 Train Accuracy: 65.44861610502551\n",
      "Cost:  0.11755522444528835 Train Accuracy: 66.75307299941467\n",
      "Cost:  0.11563043401783696 Train Accuracy: 68.12442511915712\n",
      "Cost:  0.11321232453659309 Train Accuracy: 65.87507316665273\n",
      "Cost:  0.11125615510939534 Train Accuracy: 67.34676812442511\n",
      "Cost:  0.10756354345302288 Train Accuracy: 66.81160632159879\n",
      "Cost:  0.1054647423490685 Train Accuracy: 68.04916799063467\n",
      "Cost:  0.10423019451266356 Train Accuracy: 69.30345346600888\n",
      "Cost:  0.09698923065816109 Train Accuracy: 70.70825319842797\n",
      "Cost:  0.09848801518718092 Train Accuracy: 70.47411990969145\n",
      "Cost:  0.1036827965683438 Train Accuracy: 69.86370097834266\n",
      "Cost:  0.09327518789990545 Train Accuracy: 71.18488167907016\n",
      "Cost:  0.09031118978368355 Train Accuracy: 74.40421439919726\n",
      "Cost:  0.0933789011879529 Train Accuracy: 73.0328622794548\n",
      "Cost:  0.09044434772975488 Train Accuracy: 74.05301446609248\n",
      "Cost:  0.08714846019033953 Train Accuracy: 73.4425955347437\n",
      "Cost:  0.09247292859716383 Train Accuracy: 73.62655740446526\n",
      "Cost:  0.08446515884340816 Train Accuracy: 75.95952838866125\n",
      "Cost:  0.08827435568186441 Train Accuracy: 73.43423363157454\n",
      "Cost:  0.08436240877520079 Train Accuracy: 75.89263316330796\n",
      "Cost:  0.08288647796146963 Train Accuracy: 77.28070908938875\n",
      "Cost:  0.08312744328006333 Train Accuracy: 76.4027092566268\n",
      "Cost:  0.07929588025223623 Train Accuracy: 77.7656994732001\n",
      "Cost:  0.08453815858979905 Train Accuracy: 76.44451877247262\n",
      "Cost:  0.07812557862799709 Train Accuracy: 78.41792792039467\n",
      "Cost:  0.07335054387933841 Train Accuracy: 79.61368007358475\n",
      "Cost:  0.07584563775681351 Train Accuracy: 78.75240404716112\n",
      "Cost:  0.07447544679254751 Train Accuracy: 79.2541182373108\n",
      "Cost:  0.07520035553192746 Train Accuracy: 78.40956601722552\n",
      "Cost:  0.07311420512643275 Train Accuracy: 79.81436574964462\n",
      "Cost:  0.07229475117894811 Train Accuracy: 80.14884187641107\n",
      "Cost:  0.08052843804604252 Train Accuracy: 78.75240404716112\n",
      "Cost:  0.0705431385806714 Train Accuracy: 80.73417509825236\n",
      "Cost:  0.06967443012183236 Train Accuracy: 80.81779412994398\n",
      "Cost:  0.06686580200709007 Train Accuracy: 81.48674638347688\n",
      "Cost:  0.07207087663554294 Train Accuracy: 80.07358474788862\n",
      "Cost:  0.07103371274306239 Train Accuracy: 79.54678484823145\n",
      "Cost:  0.0675494928930927 Train Accuracy: 81.11046074086462\n",
      "Cost:  0.06469976785040657 Train Accuracy: 80.4665941968392\n",
      "Cost:  0.06527394290272504 Train Accuracy: 81.63726064052177\n",
      "Cost:  0.0660471162240482 Train Accuracy: 81.6790701563676\n",
      "Cost:  0.06524410014476432 Train Accuracy: 81.04356551551133\n",
      "Cost:  0.06640492979004854 Train Accuracy: 80.33280374613263\n",
      "Cost:  0.06338482559284137 Train Accuracy: 81.6957939627059\n",
      "Cost:  0.0654405630277295 Train Accuracy: 80.79270842043648\n",
      "Cost:  0.06284360529473954 Train Accuracy: 83.09223179195585\n",
      "Cost:  0.06334732549340238 Train Accuracy: 82.4316414415921\n",
      "Cost:  0.0630600468870372 Train Accuracy: 82.23931766870139\n",
      "Cost:  0.0597541049033226 Train Accuracy: 83.19257462998578\n",
      "Cost:  0.06217345556685855 Train Accuracy: 82.33129860356217\n",
      "Cost:  0.060696634019760456 Train Accuracy: 82.33129860356217\n",
      "Cost:  0.06175259460371598 Train Accuracy: 82.64068902082113\n",
      "Cost:  0.06640358559579992 Train Accuracy: 80.62547035705326\n",
      "Cost:  0.06028906823705921 Train Accuracy: 83.39326030604566\n",
      "Cost:  0.0618436807942586 Train Accuracy: 81.14390835354126\n",
      "Cost:  0.05797052807159169 Train Accuracy: 83.51868885358307\n",
      "Cost:  0.06310504501536861 Train Accuracy: 82.02190818630321\n",
      "Cost:  0.05728775932322695 Train Accuracy: 83.41834601555314\n",
      "Cost:  0.058147768541593835 Train Accuracy: 83.12567940463249\n",
      "Cost:  0.05767866922697011 Train Accuracy: 83.5772221757672\n",
      "Cost:  0.05585557163908022 Train Accuracy: 83.78626975499624\n",
      "Cost:  0.055964953717707384 Train Accuracy: 84.1625553976085\n",
      "Cost:  0.06484733265556279 Train Accuracy: 82.60724140814449\n",
      "Cost:  0.05548421919037968 Train Accuracy: 84.37160297683752\n",
      "Cost:  0.05559934454974954 Train Accuracy: 84.5722886528974\n",
      "Cost:  0.05699360996561547 Train Accuracy: 84.07057446274771\n",
      "Cost:  0.06682725692393124 Train Accuracy: 76.77899489923907\n",
      "Cost:  0.05499669531194078 Train Accuracy: 84.95693619867882\n",
      "Cost:  0.05660931545958816 Train Accuracy: 84.08729826908605\n",
      "Cost:  0.0756859385121515 Train Accuracy: 78.03328037461327\n",
      "Cost:  0.06318773959286081 Train Accuracy: 81.57872731833766\n",
      "Cost:  0.05450951765101222 Train Accuracy: 84.68099339409649\n",
      "Cost:  0.053159600551613596 Train Accuracy: 84.84823145747971\n",
      "Cost:  0.05499183632520896 Train Accuracy: 84.63918387825069\n",
      "Cost:  0.053484441895513275 Train Accuracy: 85.46701229199766\n",
      "Cost:  0.05371460697102351 Train Accuracy: 84.92348858600216\n",
      "Cost:  0.052052984765027885 Train Accuracy: 85.23287900326115\n",
      "Cost:  0.05217478518188912 Train Accuracy: 85.29141232544526\n",
      "Cost:  0.05219972503081614 Train Accuracy: 84.73116481311146\n",
      "Cost:  0.051823646817458276 Train Accuracy: 85.58407893636591\n",
      "Cost:  0.05269912105949022 Train Accuracy: 85.87674554728656\n",
      "Cost:  0.051002085560563085 Train Accuracy: 85.12417426206204\n",
      "Cost:  0.049584645739100784 Train Accuracy: 85.63425035538089\n",
      "Cost:  0.05050081919640763 Train Accuracy: 85.74295509657999\n",
      "Cost:  0.05019979746525758 Train Accuracy: 85.52554561418178\n",
      "Cost:  0.04849530726077992 Train Accuracy: 85.95200267580901\n",
      "Cost:  0.0519960284726587 Train Accuracy: 85.34158374446024\n",
      "Cost:  0.04808209288076825 Train Accuracy: 86.19449786771469\n",
      "Cost:  0.05076028540750395 Train Accuracy: 84.54720294338992\n",
      "Cost:  0.05903771865471405 Train Accuracy: 83.25110795216992\n",
      "Cost:  0.0488123101510413 Train Accuracy: 86.04398361066978\n",
      "Cost:  0.04837987211747542 Train Accuracy: 86.3032026089138\n",
      "Cost:  0.04953961868771609 Train Accuracy: 86.05234551383894\n",
      "Train Accuracy: 85.95200267580901\n",
      "Test Accuracy: 82.69511838643672\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import datasets.mnist.loader as mnist\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    " \n",
    " \n",
    "class ANN:\n",
    "    def __init__(self, layers_size):\n",
    "        self.layers_size = layers_size\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)\n",
    "        self.n = 0\n",
    "        self.costs = []\n",
    " \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    " \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    " \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
    "                self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    " \n",
    "    def forward(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    " \n",
    "        return A, store\n",
    " \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    " \n",
    "    def backward(self, X, Y, lam,store):\n",
    " \n",
    "        derivatives = {}\n",
    " \n",
    "        store[\"A0\"] = X.T\n",
    " \n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dZ = A - Y.T\n",
    " \n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n +(lam/self.n)*store['W'+str(self.L)]\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    " \n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    " \n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)+(lam/self.n)*store['W'+str(l)]\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    " \n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    " \n",
    "        return derivatives\n",
    " \n",
    "    def fit(self, X, Y, lam,learning_rate=0.01, n_iterations=2500):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        self.n = X.shape[0]\n",
    " \n",
    "        self.layers_size.insert(0, X.shape[1])\n",
    " \n",
    "        self.initialize_parameters()\n",
    "        for loop in range(n_iterations):\n",
    "            A, store = self.forward(X)\n",
    "            cost = -np.mean(Y * np.log(A.T+ 1e-8))\n",
    "            derivatives = self.backward(X, Y, lam,store)\n",
    " \n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"dW\" + str(l)]\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"db\" + str(l)]\n",
    " \n",
    "            if loop % 100 == 0:\n",
    "                print(\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y))\n",
    " \n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    " \n",
    "    def predict(self, X, Y):\n",
    "        A, cache = self.forward(X)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        return accuracy * 100\n",
    " \n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()\n",
    " \n",
    " \n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    # Normalize\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    " \n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
    " \n",
    "    test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
    " \n",
    "    return train_x, train_y, test_x, test_y\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    \n",
    " \n",
    "    train_x, train_y, test_x, test_y = pre_process_data(x_train,y_train,x_test,y_test)\n",
    " \n",
    "    print(\"train_x's shape: \" + str(train_x.shape))\n",
    "    print(\"test_x's shape: \" + str(test_x.shape))\n",
    " \n",
    "    layers_dims = [50, 8]\n",
    " \n",
    "    ann = ANN(layers_dims)\n",
    "    ann.fit(train_x, train_y, lam=0,learning_rate=0.1, n_iterations=10000)\n",
    "    print(\"Train Accuracy:\", ann.predict(train_x, train_y))\n",
    "    print(\"Test Accuracy:\", ann.predict(test_x, test_y))\n",
    "    ann.plot_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3800794",
   "metadata": {},
   "source": [
    "# Backpropagation With DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a1aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e77602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (11959, 2352)\n",
      "test_x's shape: (3421, 2352)\n",
      "[2352, 50, 8]\n",
      "Iteration:  0 Cost:  0.27723833745142346 Train Accuracy: 18.07007274855757\n",
      "Iteration:  100 Cost:  0.21553271325378281 Train Accuracy: 45.74797223848147\n",
      "Iteration:  200 Cost:  0.18669379494076443 Train Accuracy: 50.87381888117736\n",
      "Iteration:  300 Cost:  0.16923069358320197 Train Accuracy: 55.966217911196594\n",
      "Iteration:  400 Cost:  0.15642557589495268 Train Accuracy: 60.36457897817543\n",
      "Iteration:  500 Cost:  0.14589560136135254 Train Accuracy: 63.224349862028596\n",
      "Iteration:  600 Cost:  0.13725963143877845 Train Accuracy: 66.12593026172758\n",
      "Iteration:  700 Cost:  0.1361806074945292 Train Accuracy: 65.04724475290577\n",
      "Iteration:  800 Cost:  0.1314404847076524 Train Accuracy: 65.28137804164228\n",
      "Iteration:  900 Cost:  0.12709784805530522 Train Accuracy: 65.95033029517519\n",
      "Iteration:  1000 Cost:  0.12201491977498233 Train Accuracy: 67.64779663851492\n",
      "Iteration:  1100 Cost:  0.11834856086005799 Train Accuracy: 68.89372021071995\n",
      "Iteration:  1200 Cost:  0.11530915300899976 Train Accuracy: 69.41215820720797\n",
      "Iteration:  1300 Cost:  0.11229262260124681 Train Accuracy: 70.05602475123338\n",
      "Iteration:  1400 Cost:  0.10851467886244491 Train Accuracy: 70.76678652061209\n",
      "Iteration:  1500 Cost:  0.10565561154100522 Train Accuracy: 71.57789112802074\n",
      "Iteration:  1600 Cost:  0.10374207954401714 Train Accuracy: 70.56610084455221\n",
      "Iteration:  1700 Cost:  0.10395365289479727 Train Accuracy: 70.18981520193996\n",
      "Iteration:  1800 Cost:  0.10047469087602796 Train Accuracy: 72.2970148005686\n",
      "Iteration:  1900 Cost:  0.10058523238282945 Train Accuracy: 71.33539593611506\n",
      "Iteration:  2000 Cost:  0.09819187825465922 Train Accuracy: 72.22175767204617\n",
      "Iteration:  2100 Cost:  0.09576730368111717 Train Accuracy: 72.89907182874822\n",
      "Iteration:  2200 Cost:  0.09506581838378703 Train Accuracy: 73.18337653649971\n",
      "Iteration:  2300 Cost:  0.09259050776318635 Train Accuracy: 73.5931097917886\n",
      "Iteration:  2400 Cost:  0.09440849259292369 Train Accuracy: 74.1366334977841\n",
      "Iteration:  2500 Cost:  0.0935206026635228 Train Accuracy: 73.92758591855507\n",
      "Iteration:  2600 Cost:  0.09094284772434759 Train Accuracy: 75.03971904005353\n",
      "Iteration:  2700 Cost:  0.08900433085344948 Train Accuracy: 75.78392842210887\n",
      "Iteration:  2800 Cost:  0.09489181922518156 Train Accuracy: 72.35554812275274\n",
      "Iteration:  2900 Cost:  0.08766715358753248 Train Accuracy: 75.91771887281546\n",
      "Iteration:  3000 Cost:  0.08729799839389289 Train Accuracy: 76.08495693619868\n",
      "Iteration:  3100 Cost:  0.08612037262724909 Train Accuracy: 76.32745212810435\n",
      "Iteration:  3200 Cost:  0.08465032322435909 Train Accuracy: 76.86261393093068\n",
      "Iteration:  3300 Cost:  0.08489442836867836 Train Accuracy: 76.73718538339325\n",
      "Iteration:  3400 Cost:  0.08302992362754856 Train Accuracy: 77.38941383058784\n",
      "Iteration:  3500 Cost:  0.08778514204124511 Train Accuracy: 75.5414332302032\n",
      "Iteration:  3600 Cost:  0.08137461843492375 Train Accuracy: 77.73225186052346\n",
      "Iteration:  3700 Cost:  0.08375408835776048 Train Accuracy: 76.8375282214232\n",
      "Iteration:  3800 Cost:  0.08051166967296791 Train Accuracy: 76.84589012459236\n",
      "Iteration:  3900 Cost:  0.07932427441043861 Train Accuracy: 77.27234718621958\n",
      "Iteration:  4000 Cost:  0.07811317434853782 Train Accuracy: 77.93293753658332\n",
      "Iteration:  4100 Cost:  0.0787548334227787 Train Accuracy: 77.41449954009532\n",
      "Iteration:  4200 Cost:  0.07715378092980521 Train Accuracy: 78.61025169328539\n",
      "Iteration:  4300 Cost:  0.07817276636160753 Train Accuracy: 77.23053767037378\n",
      "Iteration:  4400 Cost:  0.0765267650244219 Train Accuracy: 78.1336232126432\n",
      "Iteration:  4500 Cost:  0.07701729842826874 Train Accuracy: 78.01655656827494\n",
      "Iteration:  4600 Cost:  0.07570387428111588 Train Accuracy: 78.49318504891714\n",
      "Iteration:  4700 Cost:  0.0753607836179725 Train Accuracy: 78.5684421774396\n",
      "Iteration:  4800 Cost:  0.0740228617881924 Train Accuracy: 79.1286896897734\n",
      "Iteration:  4900 Cost:  0.07350456282579973 Train Accuracy: 79.471527719709\n",
      "Iteration:  5000 Cost:  0.07190508796393252 Train Accuracy: 80.19901329542604\n",
      "Iteration:  5100 Cost:  0.07490589905058635 Train Accuracy: 79.92307049084371\n",
      "Iteration:  5200 Cost:  0.07340560996022326 Train Accuracy: 80.22409900493352\n",
      "Iteration:  5300 Cost:  0.07311903387693833 Train Accuracy: 80.40806087465508\n",
      "Iteration:  5400 Cost:  0.07275014439940458 Train Accuracy: 80.44987039050088\n",
      "Iteration:  5500 Cost:  0.07100384481667721 Train Accuracy: 80.95994648381972\n",
      "Iteration:  5600 Cost:  0.07117300632016381 Train Accuracy: 80.63383226022242\n",
      "Iteration:  5700 Cost:  0.06804086793970993 Train Accuracy: 81.3947654486161\n",
      "Iteration:  5800 Cost:  0.07004582502642373 Train Accuracy: 81.12718454720294\n",
      "Iteration:  5900 Cost:  0.06980508521754221 Train Accuracy: 81.22752738523288\n",
      "Iteration:  6000 Cost:  0.06978668172182735 Train Accuracy: 81.12718454720294\n",
      "Iteration:  6100 Cost:  0.06866707245605783 Train Accuracy: 81.58708922150682\n",
      "Iteration:  6200 Cost:  0.06811498659736238 Train Accuracy: 81.73760347855172\n",
      "Iteration:  6300 Cost:  0.06833185127665953 Train Accuracy: 81.45329877080023\n",
      "Iteration:  6400 Cost:  0.06656598450258215 Train Accuracy: 81.42821306129275\n",
      "Iteration:  6500 Cost:  0.06537645325104666 Train Accuracy: 81.81286060707417\n",
      "Iteration:  6600 Cost:  0.07007299722365579 Train Accuracy: 81.06028932184965\n",
      "Iteration:  6700 Cost:  0.06696766126073084 Train Accuracy: 81.83794631658165\n",
      "Iteration:  6800 Cost:  0.06860502766899197 Train Accuracy: 81.33623212643198\n",
      "Iteration:  6900 Cost:  0.06554545271951569 Train Accuracy: 82.31457479722386\n",
      "Iteration:  7000 Cost:  0.06547971481898854 Train Accuracy: 82.21423195919392\n",
      "Iteration:  7100 Cost:  0.0649358510269141 Train Accuracy: 82.18078434651727\n",
      "Iteration:  7200 Cost:  0.06473596767885854 Train Accuracy: 81.6790701563676\n",
      "Iteration:  7300 Cost:  0.06547028600043103 Train Accuracy: 81.25261309474037\n",
      "Iteration:  7400 Cost:  0.06266976314954834 Train Accuracy: 82.4985366669454\n",
      "Iteration:  7500 Cost:  0.06286140331641939 Train Accuracy: 82.40655573208461\n",
      "Iteration:  7600 Cost:  0.06209357444023676 Train Accuracy: 83.0169746634334\n",
      "Iteration:  7700 Cost:  0.06187443865625808 Train Accuracy: 83.03369846977172\n",
      "Iteration:  7800 Cost:  0.061373156780847975 Train Accuracy: 83.00025085709507\n",
      "Iteration:  7900 Cost:  0.0622282260872509 Train Accuracy: 82.56543189229869\n",
      "Iteration:  8000 Cost:  0.061489836678473825 Train Accuracy: 82.84973660005018\n",
      "Iteration:  8100 Cost:  0.061895692746044846 Train Accuracy: 82.59051760180617\n",
      "Iteration:  8200 Cost:  0.059559249078834184 Train Accuracy: 83.69428882013547\n",
      "Iteration:  8300 Cost:  0.06101568696621767 Train Accuracy: 83.22602224266244\n",
      "Iteration:  8400 Cost:  0.061956471481748404 Train Accuracy: 82.87482230955766\n",
      "Iteration:  8500 Cost:  0.06209867126868077 Train Accuracy: 82.74103185885107\n",
      "Iteration:  8600 Cost:  0.059201248456366026 Train Accuracy: 83.66084120745882\n",
      "Iteration:  8700 Cost:  0.05796057194851149 Train Accuracy: 83.91169830253365\n",
      "Iteration:  8800 Cost:  0.05786814427444588 Train Accuracy: 83.9869554310561\n",
      "Iteration:  8900 Cost:  0.062215726463103124 Train Accuracy: 81.80449870390501\n",
      "Iteration:  9000 Cost:  0.06026437671938773 Train Accuracy: 82.841374696881\n",
      "Iteration:  9100 Cost:  0.05935535689670872 Train Accuracy: 83.31800317752321\n",
      "Iteration:  9200 Cost:  0.05862642031010553 Train Accuracy: 83.51868885358307\n",
      "Iteration:  9300 Cost:  0.05709003729520774 Train Accuracy: 84.32979346099172\n",
      "Iteration:  9400 Cost:  0.059952443901080565 Train Accuracy: 82.72430805251275\n",
      "Iteration:  9500 Cost:  0.057607455382025344 Train Accuracy: 83.91169830253365\n",
      "Iteration:  9600 Cost:  0.057107776326229015 Train Accuracy: 84.09566017225521\n",
      "Iteration:  9700 Cost:  0.05722923307526593 Train Accuracy: 83.86152688351869\n",
      "Iteration:  9800 Cost:  0.05729593029744065 Train Accuracy: 84.01204114056358\n",
      "Iteration:  9900 Cost:  0.056077856732378535 Train Accuracy: 84.43013629902165\n",
      "Train Accuracy: 83.39326030604566\n",
      "Test Accuracy: 80.97047646886875\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqh0lEQVR4nO3dd3yV5f3/8dfnnOxFAmQAIewhKltEcSHuamm1Wm1rl9ba1l9bO/22tXZZu4ff2lq/jtYOrXXVKnWPCigQQPYKO0AgIZPscf3+OHfCSXICCeTkZLyfj0ceOec+933OdUXMO9d13dd1mXMOERGRtnyRLoCIiPROCggREQlJASEiIiEpIEREJCQFhIiIhBQV6QJ0p6FDh7rRo0dHuhgiIn3GypUri5xz6aFe61cBMXr0aHJzcyNdDBGRPsPMdnf0mrqYREQkJAWEiIiEpIAQEZGQFBAiIhKSAkJEREJSQIiISEgKCBERCUkBAdz72jbe2loY6WKIiPQqCgjgD29uZ/E2BYSISDAFBOD3GY1NkS6FiEjvooAAfAZN2llPRKQVBQTNLQgFhIhIMAUEXkCoBSEi0ooCAvCZ0aQWhIhIKwoI1MUkIhKKAoJAC0JdTCIirSkgCLQg1MUkItKaAoLmQepIl0JEpHdRQODNg1ALQkSkFQUEGqQWEQlFAYEGqUVEQlFAoEFqEZFQFBBoJrWISCgKCLwuJrUgRERaUUCgQWoRkVAUEIBfLQgRkXYUEIDPp/0gRETaUkCgLiYRkVAUEDTPg4h0KUREepewBoSZXWZmW8wsz8zuCPH6R81srfe11MymBb22y8zWmdl7ZpYbznJqHoSISHtR4XpjM/MD9wEXA/nACjN7zjm3Mei0ncD5zrkSM7sceAA4M+j1+c65onCVsZkGqUVE2gtnC2IOkOec2+GcqwMeBxYGn+CcW+qcK/Gevgtkh7E8HfL5TIPUIiJthDMgRgB7g57ne8c6chPwn6DnDnjZzFaa2S0dXWRmt5hZrpnlFhYWnlBBozRILSLSTti6mAALcSzkb2Ezm08gIM4JOjzPObffzDKAV8xss3Puv+3e0LkHCHRNMXv27BP6Le/TUhsiIu2EswWRD4wMep4N7G97kplNBR4EFjrnDjcfd87t974fAp4h0GUVFsmxUZRXN4Tr7UVE+qRwBsQKYIKZjTGzGOB64LngE8wsB3gauNE5tzXoeKKZJTc/Bi4B1oeroBnJsRyurKWhsSlcHyEi0ueErYvJOddgZrcBLwF+4GHn3AYzu9V7/X7gu8AQ4PdmBtDgnJsNZALPeMeigL87514MV1nTU+JwDg5X1pGZEheujxER6VPCOQaBc24RsKjNsfuDHt8M3Bziuh3AtLbHwyUjORaAQ+W1CggREY9mUkNLKByqqIlwSUREeg8FBEEtiIraCJdERKT3UEAAQ5MCAXGwXC0IEZFmCgggJsrH4MQYtSBERIIoIDwZybEcKldAiIg0U0B40pNjKdQgtYhICwWEJzMlTl1MIiJBFBCejORYCitqtS+EiIhHAeHJSI6loclRUlUX6aKIiPQKCghPenJgslzhEXUziYiAAqJFRsrR5TZEREQB0SLdmyxXqIFqERFAAdEiXcttiIi0ooDwJMZGkRjjVwtCRMSjgAiSkRKnFV1FRDwKiCDpSbFqQYiIeBQQQdJTFBAiIs0UEEHUghAROUoBESQ9OZaK2gaq6xojXRQRkYhTQARp3llOrQgREQVEK81zIQqP6E4mEREFRJAMbz0mLbchIqKAaOVoC0IBISKigAgyODEGv8/UghARQQHRit9nDEmM0SC1iAgKiHbSk2O13IaICAqIdjKSYzUGISKCAqKd9ORYjUGIiKCAaCcjOY7DlXU0NrlIF0VEJKIUEG2kJ8fS2OQorqyLdFFERCJKAdGGltsQEQlQQLRxdOtR3ckkIgObAqKN5uU21IIQkYFOAdHG0OQYQMttiIgoINpIiIkiKTZKt7qKyICngAhBk+VERMIcEGZ2mZltMbM8M7sjxOsfNbO13tdSM5vW2WvDaWhyLIVqQYjIABe2gDAzP3AfcDkwBbjBzKa0OW0ncL5zbirwQ+CBLlwbNsMGxXGgvLqnPk5EpFcKZwtiDpDnnNvhnKsDHgcWBp/gnFvqnCvxnr4LZHf22nDKTovnQGkNDY1NPfWRIiK9TjgDYgSwN+h5vnesIzcB/+nqtWZ2i5nlmlluYWHhSRT3qOy0BBqaHAd1q6uIDGDhDAgLcSzkAkdmNp9AQHyzq9c65x5wzs12zs1OT08/oYK2lZ0WD0B+cVW3vJ+ISF8UzoDIB0YGPc8G9rc9ycymAg8CC51zh7tybbhkpyUAsLdE4xAiMnCFMyBWABPMbIyZxQDXA88Fn2BmOcDTwI3Oua1duTachqfGYQb5JWpBiMjAFRWuN3bONZjZbcBLgB942Dm3wcxu9V6/H/guMAT4vZkBNHjdRSGvDVdZ24qN8pOZHEe+WhAiMoCFLSAAnHOLgEVtjt0f9Phm4ObOXtuTstPi1YIQkQFNM6k7EAgItSBEZOBSQHQgOy2BA2U11GsuhIgMUAqIDowZmkhjk2OvbnUVkQFKAdGBcRlJAOQdOhLhkoiIRIYCogNj0xMB2F5YGeGSiIhEhgKiAylx0WQkx7K9UC0IERmYFBDHMD4jSQEhIgOWAuIYxqUnkXfoCM6FXAZKRKRfU0Acw7j0RCpqGrS7nIgMSAqIYxifkQzAtoPqZhKRgUcBcQynDAsExKYD5REuiYhIz1NAHMOQpFiyUuLYsF8BISIDjwLiOKYMT2HD/rJIF0NEpMcpII7j1OEpbC+spKa+MdJFERHpUQqI4zh1eAqNTY4tBRWRLoqISI9SQBzHlGGDADQOISIDTqcCwsyu7cyx/mjk4HjSEqJ5b29JpIsiItKjOtuC+J9OHut3zIyZOWms3K2AEJGB5ZhbjprZ5cAVwAgzuzfopRSgIZwF601mjkrjtc2HKK2qIzUhJtLFERHpEcdrQewHcoEaYGXQ13PApeEtWu8xa1QaAKv3lEa2ICIiPeiYLQjn3BpgjZn93TlXD2BmacBI59yA6XOZlp2K32es3F3C/MkZkS6OiEiP6OwYxCtmlmJmg4E1wCNm9qswlqtXiY/xM2VYisYhRGRA6WxADHLOlQNXA48452YBF4WvWL3PrFFpvLe3lLqGpkgXRUSkR3Q2IKLMbBhwHfB8GMvTa80dO5jq+kbW5JdGuigiIj2iswHxA+AlYLtzboWZjQW2ha9Yvc/csUMwgyV5RZEuiohIj+hUQDjn/umcm+qc+5z3fIdz7prwFq13SU2I4bThg1iadzjSRRER6RGdnUmdbWbPmNkhMztoZk+ZWXa4C9fbzBs/lNV7S6isHTBTQERkAOtsF9MjBOY+DAdGAP/2jg0o88YPob7RsXxXcaSLIiISdp0NiHTn3CPOuQbv609AehjL1SvNHjWYGL+PpRqHEJEBoLMBUWRmHzMzv/f1MWDAdcbHx/iZNSqNt7cpIESk/+tsQHyawC2uBcAB4EPAp8JVqN7sgknpbC6o4EBZdaSLIiISVp0NiB8Cn3DOpTvnMggExvfCVqpe7EJvqY3XNx+KcElERMKrswExNXjtJedcMTAjPEXq3cZnJDFycDyvb1JAiEj/1tmA8HmL9AHgrcl0zIX++iszY8HkTJZsL9I+1SLSr3U2IH4JLDWzH5rZD4ClwM/CV6ze7cLJGdTUN7F0uwarRaT/6uxM6keBa4CDQCFwtXPuL8e7zswuM7MtZpZnZneEeH2ymb1jZrVm9rU2r+0ys3Vm9p6Z5XauOj3jzLGDSY6N4sX1BZEuiohI2HS6m8g5txHY2NnzzcwP3AdcDOQDK8zsOe99mhUDXwQ+0MHbzHfO9bo/02Oj/Fw0JZOXNx7k7sYmov2dbYiJiPQd4fzNNgfI89ZtqgMeBxYGn+CcO+ScWwHUh7EcYXH5aVmUVtXz7o4BNx1ERAaIcAbECGBv0PN871hnOeBlM1tpZrd0a8m6wXkT00mM8bNonbqZRKR/CmdAWIhjrgvXz3POzQQuB75gZueF/BCzW8ws18xyCwsLT6ScJyQu2s+Fp2Ty8oYCGhq1iZCI9D/hDIh8YGTQ82xgf2cvds7t974fAp4h0GUV6rwHnHOznXOz09N7dnmoK07L4nBlnRbvE5F+KZwBsQKYYGZjzCwGuJ7AirDHZWaJZpbc/Bi4BFgftpKeoAsmZRAf7effazqdeyIifUbYAsI51wDcRmAnuk3AE865DWZ2q5ndCmBmWWaWD3wF+I6Z5ZtZCpAJLDazNcBy4AXn3IvhKuuJio/xc9lpWTy/9oAmzYlIvxPW2dDOuUXAojbH7g96XECg66mtcmBaOMvWXT44YwTPrN7H65sPccXpwyJdHBGRbqMb+E/SvPFDyUiO5elV+wAoqayjXoPWItIPKCBOkt9nfGDGCN7ccoi9xVWc+ePX+OxfVka6WCIiJ00B0Q2unjmChibH717Po66xSUuBi0i/oIDoBpOzUpiRk8o/co/OC3x988EIlkhE5OQpILrJR+bktHr+/X9vxLmuzAsUEeldFBDd5Mqpw1sef+2Siew+XMUrG9WKEJG+a0Bu+hMO8TF+/nrTmZTX1HPJlEyeXJnP9/+9kanZqWQNiot08UREukwtiG50zoShXHH6MKL8Pr571RT2lVbz2b/k0tikriYR6XsUEGFy4eRMfnrN6azJL2NtfmmkiyMi0mUKiDC6ZEoWMX4fT63Kj3RRRES6TAERRmmJMVwzawR/W7aHX7y0JdLFERHpEgVEmH3j0slMHTGI372Rx8OLd0a6OCIinaaACLO0xBj+9pm5APzg+Y1sKaiIcIlERDpHAdEDkmKj+PDswN5Jl/7mvyzdXhThEomIHJ8Cood8Yf545o0fgs/g7hc2aZa1iPR6CogekjMkgb/dPJcffeB0NuwvZ21+WaSLJCJyTAqIHva+04cR7Te+/ew6TaATkV5NAdHDBiVE8533TWH9vnL+vHSXuppEpNdSQETAR8/M4ZRhKfzg+Y189Z9rKKyojXSRRETaUUBEQJTfx19umgPA06v2cdGv3opwiURE2lNARMjQpFje/sZ8AMqq6/nVy5ppLSK9iwIigkYOTuCV289jUmYy976ex+PL99DQ2BTpYomIAAqIiJuQmcw/PhuYaX3H0+v45StbI1wiEZEABUQvkJoQw30fmQnAH97crp3oRKRXUED0Eu+bOozl31oAwGcezeW7/1rPqj0lNGmuhIhEiAKiF8lIiePvnzkTgEff2c3Vv1/K/729I8KlEpGBSgHRy5w9bihvfu2Clufv7DgcucKIyICmgOiFRg9N5PWvnk9yXBRvbink5j+voLK2IdLFEpEBRgHRS41NT+KZz89jyrAUXt10iFPveomXNhREulgiMoAoIHqx8RlJLPrSufxw4akAfPYvK6mua4xwqURkoFBA9AE3njWan10zFYBvPbNOC/yJSI+IinQBpHOuO2Mk7+44zNOr9/HM6n0AvPjlc5mcldJyzvp9ZeQMSSAlLjpSxRSRfkQtiD7k59dO45zxQ1ue/+rlo7Oui47UcuX/LuZLj62ORNFEpB9SC6IP8fuMRz89h8q6Bn68aBOPLd/LI0t2EhvlZ+XuEgDe2FLY7rpH39nFpMxkzhw7pKeLLCJ9mAKij/H5jOS4aG67cAJP5Obz/X9vbHfOkrwiZo9OIzbKD8B3/7UBgF0/eV+PllVE+jZ1MfVRI1LjWfatBUzKTCYptnXOf/TBZdz57Hqcc9TUd/6up92HKzUALiItwhoQZnaZmW0xszwzuyPE65PN7B0zqzWzr3XlWgnsKfHS7eex9q5LuPeGGeR+5yJOGRYYtH4iN597X8uj6EjndqvbUlDB+T9/k/vf0tIeIhIQtoAwMz9wH3A5MAW4wcymtDmtGPgi8IsTuFY8Pp/x/mnDGZoUy19vmsMDN84C4NevbuWOp9a1nLc0r4jNBeUh32N/WTUQ6J4SEYHwtiDmAHnOuR3OuTrgcWBh8AnOuUPOuRVAfVevldCGJMVyyalZPHfbPAAWB/3C/8iDy/jYg8upqmu/bEe0L/BPoV4bFomIJ5wBMQLYG/Q83zvWrdea2S1mlmtmuYWF7e/gGaimZqey854rWDA5g4zkWL4wfxwxUT6KjtQy5bsv8caWQ62WEq9tCIxVNGh5cRHxhDMgLMSxzv726fS1zrkHnHOznXOz09PTO124gcDMeOiTZ7D82xfx9Usns/VHl3PDnJEAfOqRFfz0xc0t5x7xFgOsazh+C2J74REe1DLkIv1eOAMiHxgZ9Dwb2N8D18ox3HP1VNbcdQlzRg/mgbd38Prmg9Q2NFJQVgMcDYpj+cTDy/nRC5soq2rbMygi/Uk4A2IFMMHMxphZDHA98FwPXCvHMSg+moc+OZtov49P/ymXSd95kXv+E2hN7CyqZPQdL/D0qvyW82vqG/nFS1s472dvsLOokoqaQIjkl1Z1+Bk19Y2dChsR6b3CFhDOuQbgNuAlYBPwhHNug5ndama3AphZlpnlA18BvmNm+WaW0tG14SrrQJQcF82fPnkGAIkx/navf+WJNSzzNit64L87+N0beewprmL9vjLiowPn55dUd/j+V/7vYk6766UwlFxEekpY50E45xY55yY658Y55+72jt3vnLvfe1zgnMt2zqU451K9x+UdXSvd6+zxQ9n1k/ex8s6LARg5OJ41d13Cq18JbFZ086O5FFbUsmjdgZZrio7UEu8Fyu9ez6OxybF6TwnXP/AO+0qPBkbeoSM9WxkR6XaaSS3ERft5+xvzeewzcxkUH834jCQev2Uu1XWNnHH3q2wuqOB7V03B7zP2FFdRXh0Ye1i3r4z/rD/A9/69kXd3FPPG5kMdfkZtQyNr9pb2UI1EpDsoIASAkYMTyE5LaHl+6vBBPHrTHACy0+K5fk4OEzKSeGTJLg5X1rWc940n17b84n98xR6cc63GHn7/Zh6lVXU88NYOFt63pGVRQRHp/RQQ0qGzxw1lw/cv5YX/dy5x0X5+c/30ltcev2UuN84dRZW3w92CyRms31fOil0lLN95uOW8n724hWdX72uZqb1KASHSZ2g1VzmmxKCFACdnpfDM58/mpQ0HmT0qjdmj0rjk1Ex2FFZyzaxszrz7Va774zvt3uNAeU3L47sXbeLKacOormvkidx8vn7pJPy+wLSXvcVVbD1YwYJTMsNfMRE5LutPq3fOnj3b5ebmRroYA9Ybmw9x7+vb2HO4iqFJsWw5WBHyvCif4TOjrrGJpz53NrNGpQFw6ndfpLKukTV3XcKg+Pa74q3ZW8rdizbx4Cdma9c8kW5iZiudc7NDvaYWhHSb+ZMzmD85A4BNB8q5/LdvhzwvsJxH4A+Ta/6wlF9cO41rZo6g0uuumvb9l/n8BeMYnBjDzeeObbnuU39aQXFlHdsPHWFGTlp4KyMiCggJj3HpSYzPSOJLCyZw1rghJMdFkV9Szfp9ZdQ3Op5amc+7Ow/jHHztn2vatRh+/+Z2ILBcyIdmZTMoPpqSqsDgeHVd5/e4EJETpy4miZi6hibW7Svjhv97t2UNqJgoX7v1oK6dlc1Pr5nK2G8tAiAlLoonP3c2EzOTW87ZUlDBxMwkzEIt4yUiHTlWF5MCQiJuf2k1P160iRk5adx0zhiu+cNSKmrquWFOTsuWqmOGJrKzqLLVdXNGD+bMsYNZtrOY5TuLmTIshaxBcTzszRAXkeNTQEifUt/YhHOB1sSyHYf58APvdun6ld+5iOgonwayRTrhWAGheRDS60T7fcREBf5pnjl2CG9/Y367c84eN6TV8+bzAWb96FWmfu9lthSEvosq2HNr9vPzlzbjnOvUUucn6uY/53LV/y4O2/uLhIMGqaXXGzk4ga0/upwjtQ385tWtpMRFc/vFE3ln+2Fuf+I9vnrxRB5bsbfdUh6X/ua/nDE6jZFpCXzsrFGcNnwQa/JLqaip58LJmSzJK+KLj60GoLSqnr8t28OqOy9mcGJMt9fh1U0Hu/09RcJNXUzSL9zwwLu8s+MwU4alcNlpWcRF+/jxos0dnj9rVFrIZT8+PW8Mt188geRu7p4afccLAOz6yfu69X1FTpbmQUi/d8/Vp/Pzl7dwz9Wnt4w9fObcsRwoq2HVnhL+s76AF9YeXZW2ORxumz+e372R13L84SU7eXjJTu66agrXn5FDfIwf5xwvbzzInNGDSetC66K+sQmfWctMcYCGxiai/N3Ts+uco6qusdVs9+5WVlVPfVMTQ5Niw/YZ0nupBSEDxtK8IpLiovjNq9vw+4xrZ2Wz4JRMxnm3z3ZkzpjBLN9ZzOSsZG46ZwwXTs4gv6SaaSNTj3nd/F+8SXpSLE/celZLC+LXH57GFacPIzaq/R4cXfXIkp18/98bWfatBWSmxJ30+4Wilk//pxaECIH9L4B2t8Hee8MM9hZXcfXMEazcXcK2g0corqzjL+/uBmD5zmIANhdU8PUn17Zc99nzA7O8Y/0+pueksv1QJZ85L3CssKKWnUWV7CyqJPiPsNv/sYbF2w7zy+umnXR9nlwZ2PXvYHlN2AJCBjYFhAx47582vOXxlVPjWx5/+aIJvLmlkFc2HmTehKHc+ez6Vtf98a0d7d7rH7l7+ex5Y4nyH+1WOlhe2+qcp1bl89SqfEakxnP5aVmcNW4IM3PSiPJbl8Y+auoDM8oDS5eIdD8FhEgHhiTFcs2sbK6ZlQ3AB6YPJ8oXGD8oq67njS2HKKqo5ZGluyitqqPJBXbSC25lADy/dn/I999XWs2Di3fy4OKdQGCv8I/NzaGkqp61+aX8YOFpzAxac6qhsYlG59hacIRThiVTUx+4LfdITQPOOc0il26nMQiRk1RcWUdctI/YKD9/X7abA2U17C2p5rrZ2dz40HJ8Bs1/5M8ZM5ihSTGcOnwQv31tG2kJ0e1aGMHSEqL58QdP57LTsrjxoeUszisC4FfXTePuFza1bN60cPpwPjVvDD6Dqdmp3VY3jUH0fxqDEAmj4HkTN541utVrI1Lj2VdazfunDefGs0YxNXtQywD1584fhxnsLa5m5OB4Vu0pIb+kmvvf2sGuokqq6xspqarnc39bRbTfqG88+sfcloKKli4mgH+9t59/vRdoqXzirFHsK63ml9dOJzkuCp+vcy2LZ1fv4/XNh7j3hhlAYJvYZo1NDp+hVkqQvENHGJEa37JHe3+kFoRIGK3YVcyOwiNcPTOb6C7c3trU5NhXWs2Ookre2HyI/aXVZA2K49F3AgPn509MZ9nOwy3dTB1Jio3iLzfNYen2w+wrrWZmThqlVXXsL63hzitPafULv7m18N+vzydnSAJFR2qZ/aNXAfD7jKumDuM318/o6o+gX6ptaGTSd17k4imZ/N/HQ/7x3WeoBSESIWeMHswZowd3+Tqfzxg5OIGRgxM4f2J6y/Hb5o/n3te38dd397QcO3fCUG6Yk8OCUzL45pNr2Vdazfp95dQ0NHKktoEP/n5py7l/X3b0ugmZSUT7fZRX13PBpKOfkVdYQc6QBMqr61uONTY5nn1vf7uA+NXLWxicGMMn543pch0jpaa+kRi/r9Mtq5DvURcI5qVel19/pYAQ6UMyUuL4zLljeXz5XhqaHM9+YR7Tsge1tASaf4HX1DdSW9/Ea5sPsnhbEenJsRQdqaO4spb1+8sprKjlf55e1/K+P3j+6GcUlAXGRMprGtp9/uW/fZtov7E2v6xVt9f1c3L48aJNXDd7JKeNGBSu6rdYl1/GsNS4Lk/ga2pyTL7zRT559mi+9/5TT/jza7zuN18/73JTQIj0MaOGJPLcbedQXlPP9A4m68VF+4mL9nP1zGyunpnd7vWHFu/kh89vJDMllvEZSaQmxFBUUcuyncUsWneAx1fsYU6Ils+mA+Utj4PHRCbf+SIAOwor+dV109h1uIqJmUkUHakjKTaKrEFdn6fR2ORYm1/K9JGp7cY+rvrdYoYkxrDyzou79J7Ng/p/Wrrr5ALCG//p5/mggBDpi6YMTzmp6286Zww3ndO+W+hDf1jacqfU2vyyluOTs5L5xmWTSImLpqHJMWZoIv96bx/LdhTzzo7DVHm7/C3OK2LOj19r977x0X7+8dm5DBsUz6ubDnLd7JH8aeku/pm7lz99ak7IAHl29T6++s81/OLaaXxo1tGQax48b/5lfyx7i6vYdbiScycEutAOltcc95rOqPYCorymgTPufpXl31rQLwfwFRAi0uLuD57Ov9fs57yJ6SzeVkh2WgLjMhIZn5HcblvYW84bxy3njaOsup74aD9/eHM7v351KwBnjgls5NSsur6R2/6+mj3FVQBU1NS3LKY4957XuGZmNnd/8DTiogNrX1XUNrTMZF+xs7hVQBQHBcOBsmoyk+M6HE/44O+XUHSkjm13X06030dhRce3FHdF8M0BhRW1VNY1khTGNbEipf/VSERO2KSsZCZlTQICczY6ozk4Pj9/HJeelsmEjGT8PmPbwQrW5pdxzaxsvvrEGp5ald9yzY8XbcZncN7EdN7cUshTq/J5enU+k7NSiIv2sXpPacu52wuPtPq8ooqjAXHWPa9zw5yRXHbaMHIGJzBmaGLrc48Ezt1VVMmEzGQOVbRvQQRPMnTO4RzHHcBuuy/62r2lTM9JJSGm536lOuf467u7uWracFITun+JelBAiEg3ifb7mJx1tOtrQmYyE7x9w795+SQGxUfz0bk5PPj2Dh5bvpfPnj+Ob142mZr6Rv7fY6t5a2thqzGOHy48lV2Hq3ho8U7Ouuc1ZuSkUlnb2C64Hlu+l8eW7wXgnPFDOWfCUCZlJbM7aIvaTQUVTMhMbjUp8cmV+byysYA3txTy02umsuCUDD5w3xK2F1aSlRLHf78xv9VGVMFqGloHxEceXMbkrGTGZSRx7/UzWq3g25HGJsdLGwq47NSsYwZSfkkVcdH+dgPyubtLuPNfG1i5uyRstx8rIEQk7DKS4/juVVMAuOuqU/ngjGzOGB1YRiQu2t8yl6CwopbqukZyhiQAgS6kp1flc6CshgPrCgB4a2thy/uOSI1naFIMw1Pjyd1dwuK8opYxFICslDgKymtYtbuE908bzoGy6pbXvvbPNUT5jIYmx5f/8R6DE2Nauq8KymvYUlDB6dmh78iqqm1sd2xzQQWbCyr42iWT2rVkQnlkyU5+9MImfnv9dBZOH9Hheef89A1i/D623n15q+PN3WWVde3L0l0UECLSo+Ki/R12X6Unt/4redigeF744rn4fUZhRS2r9pTw9rYibr9oIrHRPkamJbT8ld/U5Nh26AilVXVsOlDOjJw0pmYP4ouPv8eflu5ie+ERluQVMSMnlctOzSJrUByzRqVRVdfIFb99m+LKOm6cO4riqjpeWHuA+97I4673T2HYoPh25QwOmra+/s81VNQ0MD4jiWtmjaCuoYkpwwa1hF6zzd6WuKVV9aHeBqBlJeC6xvYTIkuqAmEWzr3XFRAi0qsNTw38gs5MieO0EYP4eJvlTJr5fMakrECX1pljj+5Z/tWLJ7K/tJrCilomZaVw55VTWi2CCPDutxYQ7fcxKD4a5xz/3VLIixsKeHFDAeMzkrhy6jBOGZZCU5MjLtrPuzsOd1jeXG8zqi0HK3hh3dFNqr64YAKfv2AccdGBpTnKvImIL6w7QHpyLOdOGEp8tL/VhlKHjjGoXlAWGE/ZdbiSpduLmDtmyElN/gtFS22IiLSxJK+Ix5bvYdOBcrYXVh7/AiAmysdcL5ge+sRsXtt0kLsXbWJv8dHWxoycVGaMTGPLwXKW5LUPmdFDEpg+MpUxQ5P46NwccncVc+tfVwGB7rLxGUncc/XpjBycwFeeeI+nV+0DYEhiDCu+fdEJBcSxltpQQIiIHEN5TT17DldRWdtAYmwURUdq2V9awznjhxIf46egrIb05FgyU2LbzYVobHI0Ocezq/fxzvbDPL16X6vXJ2YmkRQbxXkT09lbXM36fWVU1jWwr7SaGL+P2ob2XUtDEmOI9vs4XFnbMlnxA9OHn/BAtdZiEhE5QSlx0cdcPqTtuEkwv8/wY1w7eyQfmpXNx88ezZghicTH+Fmxq5i5Y4eEvONp/b4yfv3KVoanxnPF6cN4eWMBF0/JZH9pDY8s2cmew1VMzExmZ1ElVXWNnB+0llZ3UgtCRKSP2lxQzqK1B7jtwgkd3pJ7PBFrQZjZZcBvAT/woHPuJ21eN+/1K4Aq4JPOuVXea7uACqARaOioAiIiA9XkrJRWc0+6W9gCwsz8wH3AxUA+sMLMnnPObQw67XJggvd1JvAH73uz+c65/r2erohIL3VibZLOmQPkOed2OOfqgMeBhW3OWQg86gLeBVLNbFgYyyQiIp0UzoAYAewNep7vHevsOQ542cxWmtktYSuliIiEFM4xiFA35LYdET/WOfOcc/vNLAN4xcw2O+f+2+5DAuFxC0BOTs7JlFdERIKEswWRD4wMep4N7O/sOc655u+HgGcIdFm145x7wDk32zk3Oz09PLd6iYgMROEMiBXABDMbY2YxwPXAc23OeQ74uAXMBcqccwfMLNHMkgHMLBG4BFgfxrKKiEgbYetics41mNltwEsEbnN92Dm3wcxu9V6/H1hE4BbXPAK3uX7KuzwTeMablRgF/N0592K4yioiIu1popyIyAA2YNZiMrNCYPcJXj4UGGhzLlTngUF17v9Opr6jnHMhB3D7VUCcDDPLHWiztVXngUF17v/CVd9wDlKLiEgfpoAQEZGQFBBHPRDpAkSA6jwwqM79X1jqqzEIEREJSS0IEREJSQEhIiIhDfiAMLPLzGyLmeWZ2R2RLk93MbORZvaGmW0ysw1m9iXv+GAze8XMtnnf04Ku+R/v57DFzC6NXOlPjpn5zWy1mT3vPe/XdTazVDN70sw2e/+9zxoAdb7d+3e93sweM7O4/lZnM3vYzA6Z2fqgY12uo5nNMrN13mv3WtuNs4/FOTdgvwgsAbIdGAvEAGuAKZEuVzfVbRgw03ucDGwFpgA/A+7wjt8B/NR7PMWrfywwxvu5+CNdjxOs+1eAvwPPe8/7dZ2BPwM3e49jgNT+XGcCWwLsBOK9508An+xvdQbOA2YC64OOdbmOwHLgLAKrZ/8HuLyzZRjoLYjObGrUJznnDjhv+1bnXAWwicD/WAsJ/ELB+/4B7/FC4HHnXK1zbieB9bFCrqDbm5lZNvA+4MGgw/22zmaWQuAXyUMAzrk651wp/bjOnigg3syigAQCq0D3qzq7wPYGxW0Od6mO3gZsKc65d1wgLR4Nuua4BnpAdGZToz7PzEYDM4BlQKZz7gAEQgTI8E7rLz+L3wDfAJqCjvXnOo8FCoFHvG61B70VkPttnZ1z+4BfAHuAAwRWgX6ZflznIF2t4wjvcdvjnTLQA6Izmxr1aWaWBDwFfNk5V36sU0Mc61M/CzO7EjjknFvZ2UtCHOtTdSbwl/RM4A/OuRlAJYGuh470+Tp7/e4LCXSlDAcSzexjx7okxLE+VedO6KiOJ1X3gR4QndnUqM8ys2gC4fA359zT3uGDzft+e98Pecf7w89iHvB+M9tFoLvwQjP7K/27zvlAvnNumff8SQKB0Z/rfBGw0zlX6JyrB54GzqZ/17lZV+uY7z1ue7xTBnpAdGZToz7Ju1PhIWCTc+5XQS89B3zCe/wJ4F9Bx683s1gzGwNMIDC41Wc45/7HOZftnBtN4L/l6865j9G/61wA7DWzSd6hBcBG+nGdCXQtzTWzBO/f+QICY2z9uc7NulRHrxuqwszmej+rjwddc3yRHqmP9BeBDYu2Ehj1/3aky9ON9TqHQFNyLfCe93UFMAR4DdjmfR8cdM23vZ/DFrpwp0Nv/AIu4OhdTP26zsB0INf7b/0skDYA6vx9YDOBnSb/QuDunX5VZ+AxAmMs9QRaAjedSB2B2d7PaTvwO7wVNDrzpaU2REQkpIHexSQiIh1QQIiISEgKCBERCUkBISIiISkgREQkJAWESASZ2QXNq86K9DYKCBERCUkBIdIJZvYxM1tuZu+Z2R+9PSeOmNkvzWyVmb1mZuneudPN7F0zW2tmzzSv2W9m483sVTNb410zznv7pKD9HP7WvF6/mf3EzDZ67/OLCFVdBjAFhMhxmNkpwIeBec656UAj8FEgEVjlnJsJvAXc5V3yKPBN59xUYF3Q8b8B9znnphFYO+iAd3wG8GUCa/qPBeaZ2WDgg8Cp3vv8KJx1FAlFASFyfAuAWcAKM3vPez6WwJLi//DO+StwjpkNAlKdc295x/8MnGdmycAI59wzAM65GudclXfOcudcvnOuicCSKKOBcqAGeNDMrgaazxXpMQoIkeMz4M/Ouene1yTn3PdCnHesdWuOtc1jbdDjRiDKOddAYFObpwhs8PJi14oscvIUECLH9xrwITPLgJZ9gUcR+P/nQ945HwEWO+fKgBIzO9c7fiPwlgvsxZFvZh/w3iPWzBI6+kBvH49BzrlFBLqfpnd7rUSOIyrSBRDp7ZxzG83sO8DLZuYjsLrmFwhsznOqma0EygiMU0BgGeb7vQDYAXzKO34j8Ecz+4H3Htce42OTgX+ZWRyB1sft3VwtkePSaq4iJ8jMjjjnkiJdDpFwUReTiIiEpBaEiIiEpBaEiIiEpIAQEZGQFBAiIhKSAkJEREJSQIiISEj/H02l9oLAOuuqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import datasets.mnist.loader as mnist\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn .preprocessing import OneHotEncoder\n",
    " \n",
    " \n",
    "class ANN:\n",
    "    def __init__(self, layers_size,p):\n",
    "        self.layers_size = layers_size\n",
    "        #print(layers_size)\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)\n",
    "        self.n = 0\n",
    "        self.p=p\n",
    "        self.costs = []\n",
    " \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    " \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    " \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    "        print(self.layers_size)\n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
    "                self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    "            #print(\"W\" + str(l)+\" shape \")\n",
    "            #print(self.parameters[\"W\" + str(l)].shape)\n",
    "            #print(\"b\" + str(l)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l)].shape)\n",
    "            \n",
    "            \n",
    " \n",
    "    def forward(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            #print(\"W\" + str(l + 1)+\"shape is\")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].shape)\n",
    "            #print(\"A \"+\"shape is\")\n",
    "            #print(A.shape)\n",
    "            #print(\"W\" + str(l+1)+\" dot A shape \")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].dot(A).shape)\n",
    "            #print(\"b\" + str(l+1)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l+1)].shape)\n",
    "            \n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            #print(\"Z shape \")\n",
    "            #print(Z.shape)\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    "            #print(\"Stored A,W,Z for index \"+str(l + 1))\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    "        #print(\"A shape is \")\n",
    "        #print(A.shape)\n",
    "        #print(\"Stored A,W,Z for index \"+str(self.L))\n",
    "        return A, store\n",
    "    \n",
    "    def forward2(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            #print(\"W\" + str(l + 1)+\"shape is\")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].shape)\n",
    "            #print(\"A \"+\"shape is\")\n",
    "            #print(A.shape)\n",
    "            #print(\"W\" + str(l+1)+\" dot A shape \")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].dot(A).shape)\n",
    "            #print(\"b\" + str(l+1)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l+1)].shape)\n",
    "            \n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            #print(\"Z shape \")\n",
    "            #print(Z.shape)\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]*np.ones(self.parameters[\"W\" + str(l + 1)].shape)*self.p\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    "            #print(\"Stored A,W,Z for index \"+str(l + 1))\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    "        #print(\"A shape is \")\n",
    "        #print(A.shape)\n",
    "        #print(\"Stored A,W,Z for index \"+str(self.L))\n",
    "        return A, store\n",
    " \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    "    \n",
    " \n",
    "    def backward(self, X, Y, lam,store):\n",
    " \n",
    "        derivatives = {}\n",
    " \n",
    "        store[\"A0\"] = X.T\n",
    " \n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dZ = A - Y.T\n",
    "        #print(\"self.n \",self.n)\n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n +(lam/self.n)*store['W'+str(self.L)]\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    "        \n",
    "        \n",
    "        #print(\"Shape of dZ is\",dZ.shape)\n",
    "        #print(\"Shape of dW is\",dW.shape)\n",
    "        #print(\"Shape of db is\",db.shape)\n",
    "        #print(\"Shape of dAPrev is\",dAPrev.shape)\n",
    " \n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    "        #print(\"shape of derivative of dW \"+str(self.L)+\" is \",derivatives[\"dW\" + str(self.L)].shape)\n",
    "        #print(\"shape of derivative of db \"+str(self.L)+\" is \",derivatives[\"db\" + str(self.L)].shape)\n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)+(lam/self.n)*store['W'+str(l)]\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    " \n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    "            #print(\"shape of derivative of dW \"+str(l)+\" is \",derivatives[\"dW\" + str(l)].shape)\n",
    "            #print(\"shape of derivative of db \"+str(l)+\" is \",derivatives[\"db\" + str(l)].shape)\n",
    "        \n",
    "        #print(\"derivatives shape is \")\n",
    "        #print(derivatives)\n",
    "        return derivatives\n",
    " \n",
    "    def fit(self, X, Y, lam,learning_rate=0.01, n_iterations=2500):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        self.n = X.shape[0]\n",
    " \n",
    "        self.layers_size.insert(0, X.shape[1])\n",
    " \n",
    "        self.initialize_parameters()\n",
    "        for loop in range(n_iterations):\n",
    "            A, store = self.forward(X)\n",
    "            cost = -np.mean(Y * np.log(A.T+ 1e-8))\n",
    "            #print(\"Y shape is \",Y.shape)\n",
    "            #print(\"np.log(A.T+ 1e-8) shape is \",np.log(A.T+ 1e-8).shape )\n",
    "            #print(\"(Y * np.log(A.T+ 1e-8)).shape is \",(Y * np.log(A.T+ 1e-8)).shape)\n",
    "            #print(\"cost\",cost)\n",
    "            derivatives = self.backward(X, Y, lam,store)\n",
    " \n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"dW\" + str(l)]*self.droputMatrix(self.p,derivatives[\n",
    "                    \"dW\" + str(l)])\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"db\" + str(l)]*self.droputMatrix(self.p,derivatives[\n",
    "                    \"db\" + str(l)])\n",
    " \n",
    "            if loop % 100 == 0:\n",
    "                print(\"Iteration: \", loop,\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y))\n",
    " \n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    " \n",
    "    def predict(self, X, Y):\n",
    "        A, cache = self.forward2(X)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        return accuracy * 100\n",
    " \n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()\n",
    "        \n",
    "    def droputMatrix(self,p,Mat):\n",
    "        noOfOnes=int(Mat.shape[0]*Mat.shape[1]*p)\n",
    "        noOfZeros=Mat.shape[0]*Mat.shape[1]-noOfOnes\n",
    "        ones=np.ones(noOfOnes)\n",
    "        zeroes=np.zeros(noOfZeros)\n",
    "        total=np.concatenate((ones,zeroes))\n",
    "        np.random.shuffle(total)\n",
    "        total=total.reshape((Mat.shape[0],Mat.shape[1]))\n",
    "        return total\n",
    "    \n",
    "        \n",
    "        \n",
    " \n",
    " \n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    # Normalize\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    " \n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
    " \n",
    "    test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
    " \n",
    "    return train_x, train_y, test_x, test_y\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    " \n",
    "    train_x, train_y, test_x, test_y = pre_process_data(x_train,y_train,x_test,y_test)\n",
    " \n",
    "    print(\"train_x's shape: \" + str(train_x.shape))\n",
    "    print(\"test_x's shape: \" + str(test_x.shape))\n",
    " \n",
    "    layers_dims = [50, 8]\n",
    " \n",
    "    ann = ANN(layers_dims,0.5)\n",
    "    ann.fit(train_x, train_y, lam=0,learning_rate=0.1, n_iterations=10000)\n",
    "    print(\"Train Accuracy:\", ann.predict(train_x, train_y))\n",
    "    print(\"Test Accuracy:\", ann.predict(test_x, test_y))\n",
    "    ann.plot_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4cacd",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36070427",
   "metadata": {},
   "source": [
    "# Library Imports And Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d35712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4708, 28, 28)\n",
      "(524, 28, 28)\n",
      "(624, 28, 28)\n",
      "(4708, 1)\n",
      "(524, 1)\n",
      "(624, 1)\n"
     ]
    }
   ],
   "source": [
    "from numpy import load \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "data = load('pneumoniamnist.npz') #put penmonia instead of bloodmnist, rest remians same\n",
    "lst = data.files \n",
    "Xtrain = data[lst[0]] \n",
    "Xtest = data[lst[1]] \n",
    "Xval = data[lst[2]] \n",
    "ytrain = data[lst[3]] \n",
    "ytest = data[lst[4]] \n",
    "yval = data[lst[5]] \n",
    "print(Xtrain.shape) \n",
    "print(Xtest.shape) \n",
    "print(Xval.shape) \n",
    "print(ytrain.shape) \n",
    "print(ytest.shape) \n",
    "print(yval.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529f7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in range(len(ytrain)):\n",
    "    y.append(ytrain[i][0])\n",
    "y_train=np.array(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43e44851",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_=[]\n",
    "for i in range(len(ytest)):\n",
    "    y_.append(ytest[i][0])\n",
    "y_test=np.array(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eea74a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for i in range(len(Xtrain)):\n",
    "    x.append(Xtrain[i].flatten())\n",
    "x_train=np.array(x)\n",
    "\n",
    "x_=[]\n",
    "for i in range(len(Xtest)):\n",
    "    x_.append(Xtest[i].flatten())\n",
    "x_test=np.array(x_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fbf4f3",
   "metadata": {},
   "source": [
    "# Backpropagation Without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c38f52bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (4708, 784)\n",
      "test_x's shape: (524, 784)\n",
      "Cost:  0.7267737801447295 Train Accuracy: 74.21410365335599\n",
      "Cost:  0.18507880309109098 Train Accuracy: 84.17587085811384\n",
      "Cost:  0.12016057086050604 Train Accuracy: 91.22769753610875\n",
      "Cost:  0.09838218230466217 Train Accuracy: 92.43840271877654\n",
      "Cost:  0.08751569030081337 Train Accuracy: 93.35174171622769\n",
      "Cost:  0.216187834668708 Train Accuracy: 87.2982158028887\n",
      "Cost:  0.07648781822877246 Train Accuracy: 94.11639762107052\n",
      "Cost:  0.07332604281482379 Train Accuracy: 94.24384027187766\n",
      "Cost:  0.07060994589905728 Train Accuracy: 94.41376380628716\n",
      "Cost:  0.06822926732414426 Train Accuracy: 94.79609175870858\n",
      "Cost:  0.06636932062495728 Train Accuracy: 94.9660152931181\n",
      "Cost:  0.06483078603654166 Train Accuracy: 95.11469838572643\n",
      "Cost:  0.06449982825025341 Train Accuracy: 95.05097706032285\n",
      "Cost:  0.06318273758570098 Train Accuracy: 95.09345794392523\n",
      "Cost:  0.0619352197387869 Train Accuracy: 95.28462192013593\n",
      "Cost:  0.06099133322849291 Train Accuracy: 95.30586236193713\n",
      "Cost:  0.060175432408988504 Train Accuracy: 95.3483432455395\n",
      "Cost:  0.0594437392812742 Train Accuracy: 95.49702633814783\n",
      "Cost:  0.05877640804508698 Train Accuracy: 95.56074766355141\n",
      "Cost:  0.058161146823275965 Train Accuracy: 95.66694987255735\n",
      "Cost:  0.05758923493178383 Train Accuracy: 95.73067119796092\n",
      "Cost:  0.057054308040014705 Train Accuracy: 95.7943925233645\n",
      "Cost:  0.060791778849644075 Train Accuracy: 95.3695836873407\n",
      "Cost:  0.05731004301476333 Train Accuracy: 95.62446898895497\n",
      "Cost:  0.056318124224828084 Train Accuracy: 95.81563296516568\n",
      "Cost:  0.05576355767764648 Train Accuracy: 95.81563296516568\n",
      "Cost:  0.05531132751746171 Train Accuracy: 95.85811384876806\n",
      "Cost:  0.054901123147426345 Train Accuracy: 95.94307561597282\n",
      "Cost:  0.05451570630258647 Train Accuracy: 95.964316057774\n",
      "Cost:  0.05414865131184125 Train Accuracy: 96.00679694137638\n",
      "Cost:  0.05379681818112559 Train Accuracy: 96.00679694137638\n",
      "Cost:  0.053458292949481054 Train Accuracy: 96.09175870858114\n",
      "Cost:  0.053131720552240996 Train Accuracy: 96.1554800339847\n",
      "Cost:  0.2257823508908384 Train Accuracy: 88.74256584536958\n",
      "Cost:  0.05321649480687008 Train Accuracy: 96.13423959218352\n",
      "Cost:  0.05268925318861342 Train Accuracy: 96.1554800339847\n",
      "Cost:  0.05233329727782095 Train Accuracy: 96.1554800339847\n",
      "Cost:  0.05202776999916289 Train Accuracy: 96.1767204757859\n",
      "Cost:  0.051743184170544215 Train Accuracy: 96.30416312659304\n",
      "Cost:  0.05147086516054546 Train Accuracy: 96.28292268479184\n",
      "Cost:  0.051207647416395705 Train Accuracy: 96.28292268479184\n",
      "Cost:  0.05095204898785014 Train Accuracy: 96.32540356839422\n",
      "Cost:  0.05354856883551997 Train Accuracy: 95.56074766355141\n",
      "Cost:  0.06739046021086369 Train Accuracy: 94.11639762107052\n",
      "Cost:  0.05088829785590917 Train Accuracy: 96.3891248937978\n",
      "Cost:  0.05042536705175165 Train Accuracy: 96.34664401019542\n",
      "Cost:  0.05014723629325466 Train Accuracy: 96.41036533559898\n",
      "Cost:  0.04990457971696656 Train Accuracy: 96.41036533559898\n",
      "Cost:  0.0496761162323911 Train Accuracy: 96.45284621920136\n",
      "Cost:  0.04945586983518612 Train Accuracy: 96.45284621920136\n",
      "Cost:  0.09593803265190898 Train Accuracy: 91.61002548853017\n",
      "Cost:  0.049431135019923564 Train Accuracy: 96.51656754460492\n",
      "Cost:  0.049115754694659806 Train Accuracy: 96.49532710280374\n",
      "Cost:  0.04888103586534603 Train Accuracy: 96.47408666100254\n",
      "Cost:  0.04866903855186401 Train Accuracy: 96.49532710280374\n",
      "Cost:  0.048524395155472536 Train Accuracy: 96.3891248937978\n",
      "Cost:  0.04898656925390241 Train Accuracy: 96.51656754460492\n",
      "Cost:  0.04844793466205624 Train Accuracy: 96.53780798640612\n",
      "Cost:  0.04818746005286959 Train Accuracy: 96.5590484282073\n",
      "Cost:  0.047976663720278666 Train Accuracy: 96.53780798640612\n",
      "Cost:  0.04778256985735527 Train Accuracy: 96.5590484282073\n",
      "Cost:  0.047596455939715045 Train Accuracy: 96.5590484282073\n",
      "Cost:  0.04741673898213969 Train Accuracy: 96.5590484282073\n",
      "Cost:  0.04841466620760921 Train Accuracy: 96.60152931180968\n",
      "Cost:  0.04755411816512998 Train Accuracy: 96.64401019541206\n",
      "Cost:  0.04725283886526798 Train Accuracy: 96.70773152081563\n",
      "Cost:  0.04703835980226542 Train Accuracy: 96.5802888700085\n",
      "Cost:  0.04685111833982768 Train Accuracy: 96.62276975361087\n",
      "Cost:  0.046675201001855335 Train Accuracy: 96.64401019541206\n",
      "Cost:  0.04650550938322306 Train Accuracy: 96.66525063721325\n",
      "Cost:  0.04634003538378232 Train Accuracy: 96.62276975361087\n",
      "Cost:  0.04618006308060142 Train Accuracy: 96.62276975361087\n",
      "Cost:  0.047552455230479976 Train Accuracy: 96.64401019541206\n",
      "Cost:  0.046375435941278395 Train Accuracy: 96.70773152081563\n",
      "Cost:  0.046092862970279626 Train Accuracy: 96.68649107901444\n",
      "Cost:  0.045895848231313326 Train Accuracy: 96.68649107901444\n",
      "Cost:  0.045724297948260655 Train Accuracy: 96.72897196261682\n",
      "Cost:  0.0455629782576598 Train Accuracy: 96.68649107901444\n",
      "Cost:  0.04540709035512354 Train Accuracy: 96.68649107901444\n",
      "Cost:  0.04525478033874526 Train Accuracy: 96.70773152081563\n",
      "Cost:  0.27290053389532404 Train Accuracy: 75.10620220900594\n",
      "Cost:  0.04544238675845791 Train Accuracy: 96.66525063721325\n",
      "Cost:  0.045124276458600635 Train Accuracy: 96.68649107901444\n",
      "Cost:  0.04492999011042582 Train Accuracy: 96.70773152081563\n",
      "Cost:  0.04476690382326444 Train Accuracy: 96.72897196261682\n",
      "Cost:  0.044614873004144046 Train Accuracy: 96.79269328802039\n",
      "Cost:  0.254122890615297 Train Accuracy: 75.78589634664401\n",
      "Cost:  0.04480986479548892 Train Accuracy: 96.68649107901444\n",
      "Cost:  0.04449547523118446 Train Accuracy: 96.75021240441801\n",
      "Cost:  0.04430264954333714 Train Accuracy: 96.81393372982157\n",
      "Cost:  0.04414183906452776 Train Accuracy: 96.83517417162277\n",
      "Cost:  0.04399294778013865 Train Accuracy: 96.83517417162277\n",
      "Cost:  0.043849951988925356 Train Accuracy: 96.83517417162277\n",
      "Cost:  0.045977728552172734 Train Accuracy: 96.70773152081563\n",
      "Cost:  0.04407579979066396 Train Accuracy: 96.81393372982157\n",
      "Cost:  0.043595194391891065 Train Accuracy: 96.83517417162277\n",
      "Cost:  0.04347262699239234 Train Accuracy: 96.85641461342396\n",
      "Cost:  0.04541480685298571 Train Accuracy: 96.72897196261682\n",
      "Cost:  0.04469034458916233 Train Accuracy: 96.77145284621919\n",
      "Cost:  0.043570712531715884 Train Accuracy: 96.87765505522515\n",
      "Train Accuracy: 96.92013593882753\n",
      "Test Accuracy: 95.22900763358778\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import datasets.mnist.loader as mnist\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    " \n",
    " \n",
    "class ANN:\n",
    "    def __init__(self, layers_size):\n",
    "        self.layers_size = layers_size\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)\n",
    "        self.n = 0\n",
    "        self.costs = []\n",
    " \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    " \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    " \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
    "                self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    " \n",
    "    def forward(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    " \n",
    "        return A, store\n",
    " \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    " \n",
    "    def backward(self, X, Y, lam,store):\n",
    " \n",
    "        derivatives = {}\n",
    " \n",
    "        store[\"A0\"] = X.T\n",
    " \n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dZ = A - Y.T\n",
    " \n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n +(lam/self.n)*store['W'+str(self.L)]\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    " \n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    " \n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)+(lam/self.n)*store['W'+str(l)]\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    " \n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    " \n",
    "        return derivatives\n",
    " \n",
    "    def fit(self, X, Y, lam,learning_rate=0.01, n_iterations=2500):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        self.n = X.shape[0]\n",
    " \n",
    "        self.layers_size.insert(0, X.shape[1])\n",
    " \n",
    "        self.initialize_parameters()\n",
    "        for loop in range(n_iterations):\n",
    "            A, store = self.forward(X)\n",
    "            cost = -np.mean(Y * np.log(A.T+ 1e-8))\n",
    "            derivatives = self.backward(X, Y, lam,store)\n",
    " \n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"dW\" + str(l)]\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"db\" + str(l)]\n",
    " \n",
    "            if loop % 100 == 0:\n",
    "                print(\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y))\n",
    " \n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    " \n",
    "    def predict(self, X, Y):\n",
    "        A, cache = self.forward(X)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        return accuracy * 100\n",
    " \n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()\n",
    " \n",
    " \n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    # Normalize\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    " \n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
    " \n",
    "    test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
    " \n",
    "    return train_x, train_y, test_x, test_y\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    \n",
    " \n",
    "    train_x, train_y, test_x, test_y = pre_process_data(x_train,y_train,x_test,y_test)\n",
    " \n",
    "    print(\"train_x's shape: \" + str(train_x.shape))\n",
    "    print(\"test_x's shape: \" + str(test_x.shape))\n",
    " \n",
    "    layers_dims = [50, 2]\n",
    " \n",
    "    ann = ANN(layers_dims)\n",
    "    ann.fit(train_x, train_y, lam=0,learning_rate=0.1, n_iterations=10000)\n",
    "    print(\"Train Accuracy:\", ann.predict(train_x, train_y))\n",
    "    print(\"Test Accuracy:\", ann.predict(test_x, test_y))\n",
    "    ann.plot_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861bcbc0",
   "metadata": {},
   "source": [
    "# Backpropagation with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cb854ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 50, 25, 2]\n",
      "Cost:  0.34126977551708537 Train Accuracy: 74.21410365335599\n",
      "Cost:  0.2836894573641692 Train Accuracy: 74.21410365335599\n",
      "Cost:  0.28192018843509103 Train Accuracy: 74.21410365335599\n",
      "Cost:  0.2797041067136716 Train Accuracy: 74.21410365335599\n",
      "Cost:  0.27653722824302446 Train Accuracy: 74.21410365335599\n",
      "Cost:  0.2718094443851519 Train Accuracy: 74.21410365335599\n",
      "Cost:  0.2643619186953038 Train Accuracy: 74.21410365335599\n",
      "Cost:  0.25272762352918854 Train Accuracy: 74.21410365335599\n",
      "Cost:  0.23463237254776506 Train Accuracy: 74.21410365335599\n",
      "Cost:  0.20960764154694872 Train Accuracy: 77.86745964316057\n",
      "Cost:  0.18023340809549923 Train Accuracy: 84.60067969413764\n",
      "Cost:  0.1536454501219401 Train Accuracy: 88.57264231096008\n",
      "Cost:  0.13378564513613408 Train Accuracy: 90.29311809685642\n",
      "Cost:  0.11976502246948136 Train Accuracy: 91.39762107051827\n",
      "Cost:  0.11002888429328805 Train Accuracy: 91.65250637213254\n",
      "Cost:  0.10277639437973153 Train Accuracy: 92.05607476635514\n",
      "Cost:  0.09719636282838548 Train Accuracy: 92.58708581138487\n",
      "Cost:  0.09267956028049698 Train Accuracy: 92.82073067119796\n",
      "Cost:  0.08894296656263502 Train Accuracy: 93.24553950722175\n",
      "Cost:  0.08574650514904197 Train Accuracy: 93.56414613423959\n",
      "Cost:  0.08296320054326363 Train Accuracy: 93.73406966864911\n",
      "Cost:  0.0805283795263578 Train Accuracy: 93.84027187765506\n",
      "Cost:  0.07837830706059905 Train Accuracy: 94.07391673746814\n",
      "Cost:  0.07647385272911786 Train Accuracy: 94.11639762107052\n",
      "Cost:  0.07476197912397481 Train Accuracy: 94.20135938827528\n",
      "Cost:  0.07323932375191695 Train Accuracy: 94.30756159728122\n",
      "Cost:  0.07186531109855385 Train Accuracy: 94.41376380628716\n",
      "Cost:  0.07061971780945078 Train Accuracy: 94.60492778249787\n",
      "Cost:  0.06948891785506332 Train Accuracy: 94.73237043330501\n",
      "Cost:  0.06845842202086401 Train Accuracy: 94.77485131690739\n",
      "Cost:  0.06751825880203974 Train Accuracy: 94.85981308411215\n",
      "Cost:  0.06665513147524416 Train Accuracy: 94.98725573491929\n",
      "Cost:  0.06585881888913772 Train Accuracy: 95.02973661852167\n",
      "Cost:  0.06512978239083295 Train Accuracy: 95.07221750212405\n",
      "Cost:  0.06445145277483397 Train Accuracy: 95.09345794392523\n",
      "Cost:  0.06381925858176912 Train Accuracy: 95.15717926932881\n",
      "Cost:  0.06323137444038758 Train Accuracy: 95.22090059473237\n",
      "Cost:  0.06268131806245086 Train Accuracy: 95.30586236193713\n",
      "Cost:  0.062158971101825056 Train Accuracy: 95.32710280373831\n",
      "Cost:  0.06166898573832346 Train Accuracy: 95.32710280373831\n",
      "Cost:  0.061207603896098535 Train Accuracy: 95.3695836873407\n",
      "Cost:  0.06077129195165987 Train Accuracy: 95.43330501274426\n",
      "Cost:  0.06035203048283253 Train Accuracy: 95.41206457094307\n",
      "Cost:  0.05995426397735983 Train Accuracy: 95.45454545454545\n",
      "Cost:  0.059572931542247125 Train Accuracy: 95.47578589634665\n",
      "Cost:  0.05920897778252455 Train Accuracy: 95.56074766355141\n",
      "Cost:  0.058859142632809044 Train Accuracy: 95.62446898895497\n",
      "Cost:  0.058522107541525696 Train Accuracy: 95.66694987255735\n",
      "Cost:  0.05819766581232793 Train Accuracy: 95.68819031435854\n",
      "Cost:  0.05788402361143146 Train Accuracy: 95.68819031435854\n",
      "Cost:  0.057579919659228686 Train Accuracy: 95.70943075615973\n",
      "Cost:  0.057286717607870026 Train Accuracy: 95.73067119796092\n",
      "Cost:  0.05700111257667199 Train Accuracy: 95.7731520815633\n",
      "Cost:  0.05672391887487748 Train Accuracy: 95.81563296516568\n",
      "Cost:  0.056454487348498096 Train Accuracy: 95.81563296516568\n",
      "Cost:  0.05619105424494103 Train Accuracy: 95.83687340696686\n",
      "Cost:  0.055935156521802476 Train Accuracy: 95.83687340696686\n",
      "Cost:  0.05568514328797326 Train Accuracy: 95.85811384876806\n",
      "Cost:  0.05544193545169474 Train Accuracy: 95.87935429056924\n",
      "Cost:  0.05520504159401644 Train Accuracy: 95.90059473237044\n",
      "Cost:  0.05497421540186631 Train Accuracy: 95.94307561597282\n",
      "Cost:  0.05474683601178732 Train Accuracy: 95.98555649957518\n",
      "Cost:  0.05452482246438784 Train Accuracy: 95.98555649957518\n",
      "Cost:  0.054306954485870314 Train Accuracy: 96.07051826677994\n",
      "Cost:  0.054094599807704535 Train Accuracy: 96.04927782497876\n",
      "Cost:  0.0538869742738602 Train Accuracy: 96.11299915038232\n",
      "Cost:  0.05368178505151312 Train Accuracy: 96.11299915038232\n",
      "Cost:  0.05348098771129737 Train Accuracy: 96.11299915038232\n",
      "Cost:  0.053283910009578266 Train Accuracy: 96.1554800339847\n",
      "Cost:  0.05309014289446811 Train Accuracy: 96.1767204757859\n",
      "Cost:  0.052899491973449654 Train Accuracy: 96.1767204757859\n",
      "Cost:  0.05271248577853094 Train Accuracy: 96.1767204757859\n",
      "Cost:  0.05252854918665924 Train Accuracy: 96.19796091758708\n",
      "Cost:  0.05234753557230021 Train Accuracy: 96.1767204757859\n",
      "Cost:  0.05217002316911892 Train Accuracy: 96.19796091758708\n",
      "Cost:  0.05199521168937892 Train Accuracy: 96.24044180118946\n",
      "Cost:  0.051822956606074146 Train Accuracy: 96.24044180118946\n",
      "Cost:  0.05165283617213484 Train Accuracy: 96.26168224299066\n",
      "Cost:  0.051485830335008566 Train Accuracy: 96.3678844519966\n",
      "Cost:  0.0513215016205875 Train Accuracy: 96.3678844519966\n",
      "Cost:  0.0511594478780655 Train Accuracy: 96.34664401019542\n",
      "Cost:  0.050999230956250016 Train Accuracy: 96.3678844519966\n",
      "Cost:  0.050840969128615056 Train Accuracy: 96.3678844519966\n",
      "Cost:  0.050685355482515286 Train Accuracy: 96.41036533559898\n",
      "Cost:  0.05053164896393224 Train Accuracy: 96.43160577740016\n",
      "Cost:  0.05037924586390943 Train Accuracy: 96.41036533559898\n",
      "Cost:  0.050229487376907016 Train Accuracy: 96.43160577740016\n",
      "Cost:  0.050081810250206825 Train Accuracy: 96.43160577740016\n",
      "Cost:  0.04993549669325237 Train Accuracy: 96.43160577740016\n",
      "Cost:  0.04979137171318521 Train Accuracy: 96.43160577740016\n",
      "Cost:  0.049648897732559136 Train Accuracy: 96.43160577740016\n",
      "Cost:  0.04950793515946413 Train Accuracy: 96.43160577740016\n",
      "Cost:  0.049368250011335095 Train Accuracy: 96.43160577740016\n",
      "Cost:  0.04923047915526815 Train Accuracy: 96.45284621920136\n",
      "Cost:  0.049094045344464626 Train Accuracy: 96.47408666100254\n",
      "Cost:  0.04895913783197454 Train Accuracy: 96.47408666100254\n",
      "Cost:  0.048825789556673306 Train Accuracy: 96.49532710280374\n",
      "Cost:  0.04869387215415628 Train Accuracy: 96.47408666100254\n",
      "Cost:  0.04856227769624566 Train Accuracy: 96.49532710280374\n",
      "Cost:  0.048432785950206905 Train Accuracy: 96.53780798640612\n",
      "Train Accuracy: 96.53780798640612\n",
      "Test Accuracy: 95.61068702290076\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import datasets.mnist.loader as mnist\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn .preprocessing import OneHotEncoder\n",
    " \n",
    " \n",
    "class ANN:\n",
    "    def __init__(self, layers_size,p):\n",
    "        self.layers_size = layers_size\n",
    "        #print(layers_size)\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)\n",
    "        self.n = 0\n",
    "        self.p=p\n",
    "        self.costs = []\n",
    " \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    " \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    " \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    "        print(self.layers_size)\n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
    "                self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    "            #print(\"W\" + str(l)+\" shape \")\n",
    "            #print(self.parameters[\"W\" + str(l)].shape)\n",
    "            #print(\"b\" + str(l)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l)].shape)\n",
    "            \n",
    "            \n",
    " \n",
    "    def forward(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            #print(\"W\" + str(l + 1)+\"shape is\")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].shape)\n",
    "            #print(\"A \"+\"shape is\")\n",
    "            #print(A.shape)\n",
    "            #print(\"W\" + str(l+1)+\" dot A shape \")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].dot(A).shape)\n",
    "            #print(\"b\" + str(l+1)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l+1)].shape)\n",
    "            \n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            #print(\"Z shape \")\n",
    "            #print(Z.shape)\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    "            #print(\"Stored A,W,Z for index \"+str(l + 1))\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    "        #print(\"A shape is \")\n",
    "        #print(A.shape)\n",
    "        #print(\"Stored A,W,Z for index \"+str(self.L))\n",
    "        return A, store\n",
    "    \n",
    "    def forward2(self, X):\n",
    "        store = {}\n",
    " \n",
    "        A = X.T\n",
    "        for l in range(self.L - 1):\n",
    "            #print(\"W\" + str(l + 1)+\"shape is\")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].shape)\n",
    "            #print(\"A \"+\"shape is\")\n",
    "            #print(A.shape)\n",
    "            #print(\"W\" + str(l+1)+\" dot A shape \")\n",
    "            #print(self.parameters[\"W\" + str(l + 1)].dot(A).shape)\n",
    "            #print(\"b\" + str(l+1)+\" shape \")\n",
    "            #print(self.parameters[\"b\" + str(l+1)].shape)\n",
    "            \n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            #print(\"Z shape \")\n",
    "            #print(Z.shape)\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]*np.ones(self.parameters[\"W\" + str(l + 1)].shape)*self.p\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    "            #print(\"Stored A,W,Z for index \"+str(l + 1))\n",
    " \n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    "        #print(\"A shape is \")\n",
    "        #print(A.shape)\n",
    "        #print(\"Stored A,W,Z for index \"+str(self.L))\n",
    "        return A, store\n",
    " \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    "    \n",
    " \n",
    "    def backward(self, X, Y, lam,store):\n",
    " \n",
    "        derivatives = {}\n",
    " \n",
    "        store[\"A0\"] = X.T\n",
    " \n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dZ = A - Y.T\n",
    "        #print(\"self.n \",self.n)\n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n +(lam/self.n)*store['W'+str(self.L)]\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    "        \n",
    "        \n",
    "        #print(\"Shape of dZ is\",dZ.shape)\n",
    "        #print(\"Shape of dW is\",dW.shape)\n",
    "        #print(\"Shape of db is\",db.shape)\n",
    "        #print(\"Shape of dAPrev is\",dAPrev.shape)\n",
    " \n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    "        #print(\"shape of derivative of dW \"+str(self.L)+\" is \",derivatives[\"dW\" + str(self.L)].shape)\n",
    "        #print(\"shape of derivative of db \"+str(self.L)+\" is \",derivatives[\"db\" + str(self.L)].shape)\n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)+(lam/self.n)*store['W'+str(l)]\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    " \n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    "            #print(\"shape of derivative of dW \"+str(l)+\" is \",derivatives[\"dW\" + str(l)].shape)\n",
    "            #print(\"shape of derivative of db \"+str(l)+\" is \",derivatives[\"db\" + str(l)].shape)\n",
    "        \n",
    "        #print(\"derivatives shape is \")\n",
    "        #print(derivatives)\n",
    "        return derivatives\n",
    " \n",
    "    def fit(self, X, Y, lam,learning_rate=0.01, n_iterations=2500):\n",
    "        np.random.seed(1)\n",
    " \n",
    "        self.n = X.shape[0]\n",
    " \n",
    "        self.layers_size.insert(0, X.shape[1])\n",
    " \n",
    "        self.initialize_parameters()\n",
    "        for loop in range(n_iterations):\n",
    "            A, store = self.forward(X)\n",
    "            cost = -np.mean(Y * np.log(A.T+ 1e-8))\n",
    "            #print(\"Y shape is \",Y.shape)\n",
    "            #print(\"np.log(A.T+ 1e-8) shape is \",np.log(A.T+ 1e-8).shape )\n",
    "            #print(\"(Y * np.log(A.T+ 1e-8)).shape is \",(Y * np.log(A.T+ 1e-8)).shape)\n",
    "            #print(\"cost\",cost)\n",
    "            derivatives = self.backward(X, Y, lam,store)\n",
    " \n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"dW\" + str(l)]*self.droputMatrix(self.p,derivatives[\n",
    "                    \"dW\" + str(l)])\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"db\" + str(l)]*self.droputMatrix(self.p,derivatives[\n",
    "                    \"db\" + str(l)])\n",
    " \n",
    "            if loop % 100 == 0:\n",
    "                print(\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y))\n",
    " \n",
    "            if loop % 10 == 0:\n",
    "                self.costs.append(cost)\n",
    " \n",
    "    def predict(self, X, Y):\n",
    "        A, cache = self.forward2(X)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        accuracy = (y_hat == Y).mean()\n",
    "        return accuracy * 100\n",
    " \n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()\n",
    "        \n",
    "    def droputMatrix(self,p,Mat):\n",
    "        noOfOnes=int(Mat.shape[0]*Mat.shape[1]*p)\n",
    "        noOfZeros=Mat.shape[0]*Mat.shape[1]-noOfOnes\n",
    "        ones=np.ones(noOfOnes)\n",
    "        zeroes=np.zeros(noOfZeros)\n",
    "        total=np.concatenate((ones,zeroes))\n",
    "        np.random.shuffle(total)\n",
    "        total=total.reshape((Mat.shape[0],Mat.shape[1]))\n",
    "        return total\n",
    "    \n",
    "        \n",
    "        \n",
    " \n",
    " \n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    # Normalize\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    " \n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
    " \n",
    "    test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
    " \n",
    "    return train_x.reshape(len(train_x),28*28), train_y, test_x.reshape(len(test_x),28*28), test_y\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    \n",
    " \n",
    "    train_x, train_y, test_x, test_y = pre_process_data(Xtrain,ytrain,Xtest,ytest)\n",
    "    #print(train_y)\n",
    "    #print(\"train_x's shape: \" + str(train_x.shape))\n",
    "    #print(\"test_x's shape: \" + str(test_x.shape))\n",
    "    #print(\"train_y's shape: \" + str(train_y.shape))\n",
    "    #print(\"test_y's shape: \" + str(test_y.shape))\n",
    " \n",
    "    layers_dims = [50, 25, 2]\n",
    " \n",
    "    ann = ANN(layers_dims,0.4)\n",
    "    ann.fit(train_x, train_y, lam=0,learning_rate=0.1, n_iterations=10000)\n",
    "    print(\"Train Accuracy:\", ann.predict(train_x, train_y))\n",
    "    print(\"Test Accuracy:\", ann.predict(test_x, test_y))\n",
    "    ann.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed45660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
